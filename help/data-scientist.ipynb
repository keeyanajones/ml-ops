{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLOps Data Scientist\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **A Notebook for MLOps Data Scientist**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Data Scientist pre-configured Vertex AI Workbench Jupyter Notebook Offers a powerful, streamlined environment designed to showcase typical data science tasks within Vertex AI Workbench.  It provides a structured template that a GCP developer can customize for specific projects and data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tasks Data Scientists Perform in a Vertex Workbench Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data scientists typically engage in a cycle of data understanding, model development, evaluation, and preparation for deployment. Here's how that translates to actions in a Vertex Workbench notebook:\n",
        "\n",
        "#### 1. Environment Setup & Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sets up the development environment, authenticates to Google Cloud, and defines essential project variables for subsequent machine learning tasks on Vertex AI.\n",
        "\n",
        "**Goals of this section:**\n",
        "1.  Verify the Python environment and installed libraries.\n",
        "2.  Understand how authentication to Google Cloud services works in Vertex Workbench.\n",
        "3.  Define key project-specific variables like Project ID, Region, and Google Cloud Storage bucket URIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task:** Ensure the notebook environment is correctly set up, authenticated, and has access to necessary libraries and GCP services.\n",
        "\n",
        "Notebook Configuration:\n",
        "- **Kernel Selection:** Default to a TensorFlow or PyTorch kernel with GPU support if relevant.\n",
        "- **Pre-installed Libraries:** Ensure google-cloud-aiplatform, google-cloud-bigquery, google-cloud-storage, pandas, numpy, scikit-learn, matplotlib, seaborn, tensorflow, pytorch, torchvision, tqdm, etc., are pre-installed via your custom container image.\n",
        "- **Authentication:** Explain that the notebook's service account handles most authentication automatically. Provide boilerplate code for explicit authentication if needed for specific scenarios (gcloud auth application-default login or programmatic authentication for specific APIs).\n",
        "- **Project Variables:** Include initial cells to define PROJECT_ID, REGION, BUCKET_URI, BIGQUERY_DATASET, etc. This makes it easy for data scientists to configure their project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel and Pre-installed Libraries\n",
        "\n",
        "Vertex AI Workbench instances come pre-configured with popular machine learning frameworks (TensorFlow, PyTorch) and common data science libraries. This notebook is designed to run with a **TensorFlow (GPU)** or **PyTorch (GPU)** kernel, leveraging the power of GPUs for accelerated computation.\n",
        "\n",
        "**Expected pre-installed libraries:**\n",
        "* `google-cloud-aiplatform` (Vertex AI SDK)\n",
        "* `google-cloud-bigquery`\n",
        "* `google-cloud-storage`\n",
        "* `pandas`\n",
        "* `numpy`\n",
        "* `scikit-learn`\n",
        "* `matplotlib`\n",
        "* `seaborn`\n",
        "* `tensorflow` (if using TensorFlow kernel)\n",
        "* `torch`, `torchvision` (if using PyTorch kernel)\n",
        "* `tqdm`\n",
        "* And many others commonly used in data science.\n",
        "\n",
        "Let's verify some key installations and their versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import common libraries to verify installation\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import seaborn\n",
        "import tqdm\n",
        "import logging\n",
        "\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")\n",
        "print(f\"Scikit-learn Version: {sklearn.__version__}\")\n",
        "print(f\"Matplotlib Version: {matplotlib.__version__}\")\n",
        "print(f\"Seaborn Version: {seaborn.__version__}\")\n",
        "print(f\"tqdm Version: {tqdm.__version__}\")\n",
        "\n",
        "# Optional: Check for TensorFlow or PyTorch if relevant to your default kernel\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "    if tf.test.is_built_with_cuda():\n",
        "        print(\"TensorFlow is built with CUDA (GPU support).\")\n",
        "    else:\n",
        "        print(\"TensorFlow is NOT built with CUDA (GPU support).\")\n",
        "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "        print(f\"Found {len(tf.config.list_physical_devices('GPU'))} GPU(s) available for TensorFlow.\")\n",
        "    else:\n",
        "        print(\"No GPU found for TensorFlow.\")\n",
        "except ImportError:\n",
        "    print(\"TensorFlow not installed or accessible.\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Found {torch.cuda.device_count()} GPU(s) available for PyTorch.\")\n",
        "        print(f\"Current PyTorch GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"No GPU found for PyTorch.\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch not installed or accessible.\")\n",
        "\n",
        "# Set up basic logging for notebooks\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.info(\"Environment setup verification complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade pip \\\n",
        "google-cloud-aiplatform \\\n",
        "google-cloud-storage \\\n",
        "google-generativeai \\\n",
        "google-cloud-bigquery \\\n",
        "google-cloud-logging \\\n",
        "google-cloud-monitoring "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade pip \\ \n",
        "# tensorflow or python \\\n",
        "\n",
        "# Data Science Core \n",
        "pandas numpy scipy \\\n",
        "# ML Frameworks, TensorFlow from base, pytorch from specific wheels\n",
        "torch torchvision torchaudio \\\n",
        "# Tranditional ML\n",
        "scikit-learn statsmodels xgboost lightgbm catboost \\\n",
        "# Visualization \n",
        "matplotlib seaborn plotly bokeh dash nltk  spacy  textblob gensim transformers \\\n",
        "# Computer Vision \n",
        "Pillow opencv-python \\\n",
        "# MLOPs & Experiment Tracking\n",
        "mlflow  Kubeflow-pipelines \\\n",
        "# Model Explaination\n",
        "shap Lime \\\n",
        "# Data Ingestion & Connectivity \n",
        "openpyxl xlrd SQLAlchemy psycopg2-binary pymysql db-dtypes fsspec gcsfs \\\n",
        "# Other utilities & jupyter Extensions \n",
        "tqdm jupyterLab jupyterLab-git ipywidgets jupyter-resource-usage jupyter-widgets jupyterlab-manager nbconvert \\\n",
        "# Exporting Models \n",
        "onnx onnxruntime "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Google Cloud Authentication\n",
        "\n",
        "Vertex AI Workbench instances are typically associated with a **Service Account**. This service account is automatically used by Google Cloud client libraries (like `google-cloud-aiplatform`, `google-cloud-storage`, `google-cloud-bigquery`) for authentication to most Google Cloud services.\n",
        "\n",
        "This means you usually **do not need to explicitly log in** or provide credentials within the notebook.\n",
        "\n",
        "However, for some specific scenarios (e.g., interacting with `gcloud CLI` commands directly for certain administrative tasks, or if your service account lacks necessary permissions and you need to impersonate another), you might use the following:\n",
        "\n",
        "* **`gcloud auth application-default login`**: Authenticates the `gcloud CLI` using your user account's credentials. Useful for `gcloud` commands that rely on user authentication.\n",
        "* **Programmatic Authentication**: For highly specific cases, you might manually load credentials, though this is rare in Vertex Workbench for typical ML workflows.\n",
        "\n",
        "**For the vast majority of ML development in Vertex Workbench, no action is required in this section.** The default service account authentication is sufficient and recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following lines ONLY if you need explicit user authentication\n",
        "# (e.g., for specific gcloud CLI commands or when the service account is insufficient).\n",
        "# For most ML workflows, the default service account authentication is enough.\n",
        "\n",
        "# from google.colab import auth # If using Colab Enterprise\n",
        "# auth.authenticate_user()     # If using Colab Enterprise\n",
        "\n",
        "# If you were outside a managed environment and needed to authenticate with a service account key file:\n",
        "# from google.oauth2 import service_account\n",
        "# key_path = 'path/to/your/service-account-key.json'\n",
        "# credentials = service_account.Credentials.from_service_account_file(key_path)\n",
        "# client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
        "\n",
        "print(\"Authentication check: Typically, Vertex AI Workbench handles authentication automatically via the instance's service account.\")\n",
        "print(\"No explicit authentication step is usually required for client library operations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Project Variables\n",
        "\n",
        "To ensure your notebook interacts with the correct Google Cloud resources, define the following essential project-specific variables.\n",
        "\n",
        "**Important:**\n",
        "* Replace the placeholder values with your actual GCP Project ID, desired Region, and the name of your primary Google Cloud Storage bucket.\n",
        "* Ensure the service account associated with this notebook has the necessary permissions to access these resources (e.g., `Vertex AI User`, `Storage Object Admin`, `BigQuery Data Editor`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- IMPORTANT: CONFIGURE THESE VARIABLES ---\n",
        "# Replace with your Google Cloud Project ID\n",
        "PROJECT_ID = \"your-gcp-project-id\" # e.g., \"my-ml-project-12345\"\n",
        "\n",
        "# Replace with your desired Google Cloud Region (e.g., \"us-central1\", \"europe-west4\")\n",
        "# Choose a region close to your data and users for lower latency.\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "# Replace with the name of your primary Google Cloud Storage bucket for ML artifacts.\n",
        "# This bucket will be used to store datasets, model checkpoints, outputs, etc.\n",
        "GCS_BUCKET_NAME = \"your-ml-artifacts-bucket\" # e.g., \"gs://my-ml-artifacts-bucket-123\"\n",
        "\n",
        "# Optional: Replace with the name of your BigQuery Dataset, if applicable\n",
        "BIGQUERY_DATASET = \"your_bigquery_dataset\" # e.g., \"my_ml_data\"\n",
        "\n",
        "# --- DO NOT MODIFY BELOW THIS LINE (unless you know what you're doing) ---\n",
        "# Construct full GCS bucket URI\n",
        "BUCKET_URI = f\"gs://{GCS_BUCKET_NAME}\"\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "try:\n",
        "    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "    logging.info(f\"Vertex AI SDK initialized successfully for project: {PROJECT_ID}, region: {REGION}, staging bucket: {BUCKET_URI}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to initialize Vertex AI SDK: {e}\")\n",
        "    logging.error(\"Please ensure your PROJECT_ID, REGION, and GCS_BUCKET_NAME are correct and accessible.\")\n",
        "\n",
        "\n",
        "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
        "print(f\"REGION: {REGION}\")\n",
        "print(f\"GCS_BUCKET_NAME: {GCS_BUCKET_NAME}\")\n",
        "print(f\"BUCKET_URI: {BUCKET_URI}\")\n",
        "print(f\"BIGQUERY_DATASET: {BIGQUERY_DATASET}\")\n",
        "\n",
        "logging.info(\"All project variables defined and Vertex AI SDK initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "You've successfully set up your environment! You are now ready to proceed with data exploration, model development, and other machine learning tasks within this project context.\n",
        "\n",
        "* Proceed to `Data Exploration and Preparation` to start working with your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration & Preparation (EDA & Feature Engineering)\n",
        "\n",
        "**Task:** Understand the data, clean it, transform it, and create new features.\n",
        "\n",
        "Notebook Content/Capabilities:\n",
        "- Data Loading:\n",
        "   - **BigQuery:** Example code to query data from BigQuery tables using google-cloud-bigquery or pandas-gbq.\n",
        "   - **Cloud Storage:** Example code to load files (CSV, JSON, Parquet, images) from GCS using google-cloud-storage or pandas.read_csv().\n",
        "   - **Vertex AI Feature Store:** Boilerplate to connect to and retrieve features from the Feature Store.\n",
        "   - **Data Cleaning & Transformation:** Demonstrate common pandas operations for handling missing values (df.fillna(), df.dropna()), outliers, data type conversions.\n",
        "   - **Feature Engineering:** Examples of creating new columns, one-hot encoding, scaling, or using more advanced techniques.\n",
        "   - **Visualization:** Example plots using matplotlib, seaborn, or plotly for distribution, correlation, and relationships.\n",
        "   - **Saving Prepared Data:** Code to save processed data back to GCS or BigQuery for subsequent training stages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Model Development (Training & Experimentation)\n",
        "\n",
        "**Task:** Select, train, and fine-tune machine learning models.\n",
        "\n",
        "- Notebook Content/Capabilities:\n",
        "    - **Model Definition:** Boilerplate for common model architectures (e.g., a simple scikit-learn model, a basic TensorFlow Keras model, or a PyTorch model).\n",
        "    - **Training Loop:** A clear training loop structure, especially for deep learning.\n",
        "    \n",
        "    - Hyperparameter Tuning:\n",
        "        - Guidance on manual tuning.\n",
        "        - Example code to kick off a Vertex AI Vizier hyperparameter tuning job (often externalized to a Python script called from the notebook).\n",
        "\n",
        "    - Experiment Tracking with Vertex AI Experiments:\n",
        "        - **Crucial:** Include code to initialize an experiment run (aiplatform.init(), aiplatform.start_run()).\n",
        "        - Log metrics (run.log_metric()), parameters (run.log_params()), and artifacts (run.log_artifact()) like plots or model checkpoints. This is vital for comparing different model iterations.\n",
        "    \n",
        "    - **Model Checkpointing:** Code to save model weights or the entire model periodically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Model Evaluation\n",
        "\n",
        "**Task:** Assess model performance, understand its strengths and weaknesses, and identify biases.\n",
        "- Notebook Content/Capabilities:\n",
        "   - **Prediction:** Code to make predictions on a held-out test set.\n",
        "   - **Metric Calculation:** Examples of calculating standard metrics (e.g., accuracy_score, precision_score, recall_score, f1_score for classification; mean_squared_error, r2_score for regression).\n",
        "   - **Visualization of Results:** Confusion matrices, ROC curves, precision-recall curves, residual plots, etc.\n",
        "   - **Vertex Explainable AI:** Boilerplate to generate feature importances or saliency maps for predictions (e.g., integrated gradients, XAI API calls). This helps data scientists understand \"why\" a model made a certain prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Model Deployment Preparation & Local Testing\n",
        "\n",
        "**Task:** Prepare the trained model for deployment, often by saving it in a deployable format and testing predictions locally.\n",
        "- Notebook Content/Capabilities:\n",
        "    - **Model Serialization:** Code to save the trained model in a format suitable for Vertex AI (e.g., TensorFlow SavedModel, PyTorch state_dict, scikit-learn joblib).\n",
        "    - **Local Prediction Test:** A small function or script to load the saved model and make local predictions to ensure it's working as expected before full deployment.\n",
        "    - **Vertex AI Model Registry Integration:** Boilerplate to upload the trained model to the Model Registry, including model versions and metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. MLOps Integration & Orchestration (Initiated from Notebook)\n",
        "\n",
        "**Task:** Transition from local notebook experimentation to automated, scalable ML workflows.\n",
        "- Notebook Content/Capabilities:\n",
        "    - **Custom Training Job Submission:** Example code to take the training script developed in the notebook and submit it as a custom training job on Vertex AI (e.g., specifying machine type, accelerators, custom container). This allows training on larger datasets and more powerful infrastructure.\n",
        "    - **Vertex AI Pipeline Triggering:** While pipeline definitions are usually in separate Python files, the notebook can include cells to:\n",
        "       - Compile a pre-defined pipeline.\n",
        "       - Submit/run a Vertex AI Pipeline, passing parameters like model version or data path. This enables automation of the full ML workflow (data prep -> training -> evaluation -> deployment).\n",
        "   - Model Deployment to Endpoint:\n",
        "       - Example code to take a model from the Model Registry and deploy it to a Vertex AI Endpoint for online predictions, including specifying machine types and autoscaling settings.\n",
        "       - Initial calls to test the deployed endpoint.\n",
        "   - **Model Monitoring Configuration:** Code snippets to configure Vertex AI Model Monitoring for the deployed endpoint, setting up alerts for data drift or prediction drift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. Collaboration & Version Control\n",
        "\n",
        "**Task:** Work collaboratively and manage different versions of code and notebooks.\n",
        "- Notebook Configuration/Guidance:\n",
        "   - **Git Integration:** Pre-configure the notebook with Git extensions or provide instructions on how to clone repositories, commit changes, and push to remote (GitHub, GitLab, Cloud Source Repositories).\n",
        "   - **Shared Drives:** If using shared GCS buckets for data or models, ensure appropriate permissions and clear paths.\n",
        "   - **Markdown for Documentation:** Encourage data scientists to use Markdown cells extensively for documenting their thought process, findings, and code explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_template.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
