{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f164d90-64b0-4201-bfc8-15da7f52c775",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLOps AI Researcher\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beda2f3-8174-4422-b9eb-d17e96b0f03d",
   "metadata": {},
   "source": [
    "## **A Notebook for MLOps AI Researchers**\n",
    "An AI Researchers pre-configured Vertex AI Workbench Jupyter Notebook Offers a powerful, streamlined enviornment designed to accelerate research, experimentation, and rapid prototyping of advanced computer vision and multimodal AI solutions, particularly in the context of anomaly detection.  \n",
    "\n",
    "Insted of spending weeks setting up infrastructure, researchers can immediately dive into their core work: designing and testing novel AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b04a11-127e-44f0-b406-e0a3b04c16dc",
   "metadata": {},
   "source": [
    "#### 1. **Accelerated Experimentation and Prototyping**\n",
    "\n",
    "- **Pre-configured Environment:**\n",
    "Researchers get immediate access to JupyterLab with pre-installed TensorFlow and Python kernels, along with common ML/CV libraries (NumPy, Pandas, Scikit-learn, OpenCV, Pillow, etc). This eliminates environment setup headaches. \n",
    "- **Scalable Compute:**\n",
    "Easy access to GPUs (e.g, NVIDIA V100, A100) directly from the workbench instance, crucial for training large vision models and runing computationally intensive experiments.\n",
    "- **Direct Access to Data:**\n",
    "Seamless integration with Cloud Storage and BigQuery allows researchers to quickly access and process large image/video datasets without complex authentication or data transfer steps.  \n",
    "- **Version Control Integration (Git):**\n",
    "Built-in Git functionality simplifes tracking code changes, collaborating with peers, and reverting to previous experiments.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1f473-7a98-4d6c-a4ec-20d645c45789",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. **Gemini Vision & Multimodal Capabilities:**\n",
    "\n",
    "- **Gemini API Access:** Direct access to Gemini Vision APIs for multimodal understanding, allowing researchers to experiment with visual reasoning, image captioning, visual Q&A, and potentially leverage its capabilities for advanced anomaly detection feature extraction or contextual analysis.\n",
    "- **Foundation Model Fine-tuning:** The environment supports fine-tuning of foundation models (including those powered by Gemini) on custom datasets, enabling researchers to adapt powerful pre-trained models to their specific anomaly detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fcab9-7468-431c-94c7-96a7170d8fc9",
   "metadata": {},
   "source": [
    "### 3. **MLOps Principles for Research Reproducibility:**\n",
    "\n",
    "- **Containerization (Docker):** The ability to build and run Docker containers directly within the Workbench (especially with user-managed notebooks) means researchers can encapsulate their experimental environments, ensuring reproducibility and easier transition to production. This is vital for complex TensorFlow models.\n",
    "- **Experiment Tracking:** Integration with Vertex AI Experiments allows researchers to automatically log model metrics, hyperparameters, and artifacts, enabling systematic comparison of different model architectures and training runs.\n",
    "- **Reproducible Pipelines (KFP/Vertex AI Pipelines):** Although an MVP might simplify this, the foundation is there to transition successful experiments into reproducible pipelines, allowing researchers to test new ideas in a consistent, automated manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186bed0-d614-4b1a-a93a-fa51d35de720",
   "metadata": {},
   "source": [
    "### 4. **Interactive Development with React (for Visualization & Interaction):**\n",
    "\n",
    "- **Rapid UI Prototyping:** While not directly for core ML research, the inclusion of React allows AI researchers (or closely collaborating engineers) to quickly build interactive visualization tools for their model outputs. For anomaly detection, this means:\n",
    "   - Visually inspecting detected anomalies (e.g., bounding boxes, heatmaps).\n",
    "   - Testing human-in-the-loop feedback mechanisms.\n",
    "   - Demonstrating the model's capabilities to non-technical stakeholders.\n",
    "- **Real-time Model Interaction:** Researchers can use the React frontend to send real-time inference requests to deployed models, observing immediate results and fine-tuning their understanding of model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c2075-7057-4c0b-906e-fb88501f95f9",
   "metadata": {},
   "source": [
    "#### 5. **Simplified Deployment & Monitoring for Research Demos:**\n",
    "\n",
    "- **One-Click Deployment (Simplified):** While full MLOps pipelines are for production, researchers can quickly deploy a prototype model to a Vertex AI Endpoint for testing purposes, making it accessible for team demos or integration testing.\n",
    "- **Basic Monitoring:** Set up simple monitoring (e.g., request/latency metrics, basic prediction drift) to understand model behavior in a \"pseudo-production\" environment, helping identify early issues during research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34642106-03a7-48da-9350-7ea2355e49c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657fe24-0df2-4390-b852-9566b42519ed",
   "metadata": {},
   "source": [
    "## Pre-configured Enviornment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7775c-7e61-42a4-880c-992f222fca10",
   "metadata": {},
   "source": [
    "### Install Google and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc391843-4e3a-44b9-923a-b587466e6946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip \\\n",
    "google-cloud-aiplatform \\\n",
    "google-cloud-storage \\\n",
    "google-generativeai \\\n",
    "google-cloud-bigquery \\\n",
    "google-cloud-logging \\\n",
    "google-cloud-monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92934371-a281-4be6-8029-eb888a1bcca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip \\\n",
    "tensorflow \\\n",
    "torch torchvision \\\n",
    "scikit-learn pandas numpy matplotlib seaborn plotly bokeh \\\n",
    "xgboost lightgbm \\\n",
    "tqdm \\\n",
    "pillow opencv-python h5py \\\n",
    "requests beautifulsoup4 lxml \\\n",
    "nltk spacy \\\n",
    "jupyter jupyterlab jupyterlab-git nbformat ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b92951-18bc-474d-9a03-8edfd9d5ec35",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58921e4-0598-44b2-8bcb-5b670a0144b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ad5a0-7d23-4544-a578-4d4c9b838e99",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9cec4-8b6d-4364-b2c1-b64c2fb77f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  \n",
    "# @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46974c9c-d840-474b-a6da-973e0bf59582",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b9f41-6626-4b0f-8ea4-42a58828b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions/classes\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cbbb0-89d7-4591-bbf9-1aad0731c515",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ebf06-fdba-40e1-b6f3-03ce0166dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3d5c9-4269-4422-970e-7f3d860f916d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac6baa-c98e-451f-9739-65a92e5a76f5",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "## Tasks for this Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385abcb-0b0a-4eff-9349-368823197534",
   "metadata": {},
   "source": [
    "### **Data Acquisition & Curation for Vision Models:**\n",
    "\n",
    "- **Loading and Exploring Datasets:** Use Python (Pandas, NumPy, Matplotlib, Seaborn) in Jupyter notebooks to load image/video data from Cloud Storage, explore its distribution, and visualize samples.\n",
    "- **Data Augmentation:** Apply and test various image augmentation techniques (e.g., rotations, flips, color jitter, noise injection) using libraries like TensorFlow Data, Albumentations, or OpenCV.\n",
    "- **Data Labeling (Anomaly vs. Normal):** Configure and manage Vertex AI Data Labeling jobs (using templates/data_labeling_job.yaml) to get human annotations for anomaly types or normal patterns in images.\n",
    "- **Dataset Versioning:** Integrate with Vertex AI Datasets to manage and version different iterations of their vision datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d061c8-cd6b-46b9-9353-470e1c771bdc",
   "metadata": {},
   "source": [
    "### Loading and Exploring Datasets:\n",
    "\n",
    "- Data Access: Connect to Cloud Storage buckets (e.g., for raw image data, processed features) and BigQuery tables.\n",
    "\n",
    "**Setup and Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8b29e-55ed-47ee-ac1d-a4be5ad20463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Google Cloud SDKs\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "from google.protobuf import json_format\n",
    "\n",
    "# TensorFlow (for tf.data and image processing)\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Image Augmentation Libraries\n",
    "import cv2 # OpenCV\n",
    "import albumentations as A # Albumentations\n",
    "\n",
    "# Set up GCP project and region\n",
    "# Ensure these are configured correctly for your environment\n",
    "# Or, they can be dynamically fetched if the service account has permissions.\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n",
    "REGION = 'us-central1' # Or your desired region, e.g., 'europe-west4'\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    # Fallback if GOOGLE_CLOUD_PROJECT env var is not set\n",
    "    # You might need to authenticate `gcloud auth application-default login`\n",
    "    # if running locally, or rely on instance metadata if on a VM.\n",
    "    try:\n",
    "        _, project_id = aiplatform.initializer.global_config.get_client_options()\n",
    "        PROJECT_ID = project_id\n",
    "    except Exception:\n",
    "        print(\"Warning: GOOGLE_CLOUD_PROJECT environment variable not set. Please set it or ensure service account is configured.\")\n",
    "        PROJECT_ID = \"YOUR_GCP_PROJECT_ID\" # REPLACE WITH YOUR PROJECT ID\n",
    "\n",
    "print(f\"Using Google Cloud Project: {PROJECT_ID}\")\n",
    "print(f\"Using Google Cloud Region: {REGION}\")\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "gcs_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize Google Cloud Vision API client\n",
    "vision_client = vision_v1.ImageAnnotatorClient()\n",
    "\n",
    "print(\"Setup complete. Libraries imported and GCP clients initialized.\")\n",
    "\n",
    "# Optional: Configure matplotlib for better visualization\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6d2d3-d6d0-4824-9738-0de3f440bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Exploring Datasets from Cloud Storage\n",
    "\n",
    "# --- Configuration for your dataset ---\n",
    "GCS_BUCKET_NAME = 'your-image-dataset-bucket' # REPLACE with your actual GCS bucket name\n",
    "GCS_DATA_PREFIX = 'raw_images/' # Optional: if your images are in a subfolder, e.g., 'raw_images/'\n",
    "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "MAX_SAMPLES_TO_LIST = 1000 # Limit for listing all files to avoid huge memory usage\n",
    "SAMPLES_TO_LOAD = 5 # Number of random images to load and display\n",
    "\n",
    "print(f\"Attempting to connect to GCS bucket: gs://{GCS_BUCKET_NAME}/{GCS_DATA_PREFIX}\")\n",
    "\n",
    "try:\n",
    "    bucket = gcs_client.get_bucket(GCS_BUCKET_NAME)\n",
    "    blobs = list(bucket.list_blobs(prefix=GCS_DATA_PREFIX))\n",
    "    \n",
    "    image_blobs = [b for b in blobs if b.name.lower().endswith(IMAGE_EXTENSIONS)]\n",
    "    \n",
    "    if not image_blobs:\n",
    "        print(f\"No image files found in gs://{GCS_BUCKET_NAME}/{GCS_DATA_PREFIX}. Please check bucket name and prefix.\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_blobs)} image files.\")\n",
    "\n",
    "        # --- Explore Dataset Distribution (if metadata is available) ---\n",
    "        # For simple image datasets, distribution might be based on file size or creation date.\n",
    "        # If you have a CSV metadata file in GCS, you'd load it here.\n",
    "        # Example: loading a dummy metadata file for demonstration\n",
    "        try:\n",
    "            metadata_blob = bucket.blob(f'{GCS_DATA_PREFIX}metadata.csv') # Replace with your actual metadata file\n",
    "            if metadata_blob.exists():\n",
    "                metadata_df = pd.read_csv(io.BytesIO(metadata_blob.download_as_bytes()))\n",
    "                print(f\"\\nLoaded metadata for {len(metadata_df)} images.\")\n",
    "                print(\"Metadata head:\")\n",
    "                print(metadata_df.head())\n",
    "                \n",
    "                # Example: Visualize distribution of a 'label' column if it exists\n",
    "                if 'label' in metadata_df.columns:\n",
    "                    plt.figure(figsize=(8, 5))\n",
    "                    sns.countplot(data=metadata_df, x='label')\n",
    "                    plt.title('Distribution of Image Labels (from Metadata)')\n",
    "                    plt.xlabel('Label')\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(\"No 'label' column found in metadata for distribution visualization.\")\n",
    "\n",
    "            else:\n",
    "                print(\"No metadata.csv found in bucket. Skipping metadata exploration.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing metadata: {e}\")\n",
    "\n",
    "        # --- Visualize Sample Images ---\n",
    "        print(f\"\\nDisplaying {SAMPLES_TO_LOAD} random image samples:\")\n",
    "        sample_blobs = random.sample(image_blobs, min(SAMPLES_TO_LOAD, len(image_blobs)))\n",
    "\n",
    "        plt.figure(figsize=(15, 5 * ((SAMPLES_TO_LOAD + 4) // 5))) # Adjust grid based on sample count\n",
    "        for i, blob in enumerate(sample_blobs):\n",
    "            try:\n",
    "                img_bytes = blob.download_as_bytes()\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "                \n",
    "                plt.subplot(((SAMPLES_TO_LOAD + 4) // 5), 5, i + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"{blob.name.split('/')[-1]}\")\n",
    "                plt.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load image {blob.name}: {e}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing GCS bucket {GCS_BUCKET_NAME}: {e}\")\n",
    "    print(\"Please ensure the bucket exists and your service account has 'Storage Object Viewer' role.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4459b7c-7c22-4149-9635-06979a0b9e94",
   "metadata": {},
   "source": [
    "### Data Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d687c76-7217-4c91-bc5b-4611d0ecccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "print(\"Demonstrating Image Augmentation Techniques.\")\n",
    "\n",
    "# --- Helper function to load an image ---\n",
    "def load_image_from_gcs(gcs_uri, target_size=(256, 256)):\n",
    "    \"\"\"Loads an image from GCS and resizes it.\"\"\"\n",
    "    try:\n",
    "        bucket_name = gcs_uri.split('gs://')[1].split('/')[0]\n",
    "        blob_name = '/'.join(gcs_uri.split('gs://')[1].split('/')[1:])\n",
    "        bucket = gcs_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        img_bytes = blob.download_as_bytes()\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\") # Ensure RGB\n",
    "        return np.array(img.resize(target_size))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {gcs_uri}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Choose a sample image for augmentation demonstration\n",
    "if image_blobs:\n",
    "    sample_image_blob = random.choice(image_blobs)\n",
    "    sample_gcs_uri = f\"gs://{sample_image_blob.bucket.name}/{sample_image_blob.name}\"\n",
    "    original_image_np = load_image_from_gcs(sample_gcs_uri)\n",
    "else:\n",
    "    print(\"No images found in GCS bucket to demonstrate augmentation. Creating a dummy image.\")\n",
    "    original_image_np = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "    cv2.putText(original_image_np, \"No Image\", (50, 128), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "if original_image_np is not None:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # --- 1. Original Image ---\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(original_image_np)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # --- 2. Albumentations Example ---\n",
    "    # Define an augmentation pipeline\n",
    "    augmentation_pipeline_alb = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),\n",
    "    ])\n",
    "    augmented_image_alb = augmentation_pipeline_alb(image=original_image_np)['image']\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(augmented_image_alb)\n",
    "    plt.title('Albumentations Augmented')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # --- 3. TensorFlow Data Augmentation Example ---\n",
    "    # Convert to TensorFlow tensor\n",
    "    tf_image = tf.convert_to_tensor(original_image_np)\n",
    "\n",
    "    # Apply simple TF augmentations\n",
    "    tf_augmented_image = tf.image.random_flip_left_right(tf_image)\n",
    "    tf_augmented_image = tf.image.random_brightness(tf_augmented_image, max_delta=0.2)\n",
    "    tf_augmented_image = tf.image.random_contrast(tf_augmented_image, lower=0.8, upper=1.2)\n",
    "    \n",
    "    # Optional: Rotate (needs custom TF ops or tf_addons for more advanced)\n",
    "    # tf_augmented_image = tf.image.rot90(tf_augmented_image, k=1) # 90-degree rotation\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(tf_augmented_image.numpy().astype(np.uint8)) # Convert back to numpy for display\n",
    "    plt.title('TensorFlow Augmented')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # --- 4. OpenCV Example (Simple Rotation) ---\n",
    "    angle = 30 # degrees\n",
    "    (h, w) = original_image_np.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_image_cv = cv2.warpAffine(original_image_np, M, (w, h))\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(rotated_image_cv)\n",
    "    plt.title(f'OpenCV Rotated ({angle}°)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTip: For real-world use, integrate augmentation directly into your TensorFlow/PyTorch Dataset pipelines.\")\n",
    "else:\n",
    "    print(\"Skipping augmentation demonstration as no image was loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd37089-9632-490a-a1d8-2c1812917d92",
   "metadata": {},
   "source": [
    "### Data Labeling (Anomaly vs. Normal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a3c270-0140-455b-b8fe-7f3702bddae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labeling (Anomaly vs. Normal) with Vertex AI Data Labeling\n",
    "\n",
    "print(\"Demonstrating how to initiate a Vertex AI Data Labeling Job.\")\n",
    "print(\"The actual labeling is done in the Vertex AI Console by human annotators.\")\n",
    "\n",
    "# --- Configuration for your labeling job ---\n",
    "# Input GCS URI of a CSV or JSONL file pointing to your images.\n",
    "# Each line in the file should be a GCS URI to an image, e.g., 'gs://your-image-dataset-bucket/raw_images/img_001.jpg'\n",
    "INPUT_DATA_GCS_URI = f'gs://{GCS_BUCKET_NAME}/data_for_labeling.csv' # REPLACE with your input data list\n",
    "\n",
    "# Output GCS URI for the labeled data\n",
    "OUTPUT_GCS_URI = f'gs://{GCS_BUCKET_NAME}/labeled_data_output/' # Labeled data will be written here\n",
    "\n",
    "# Display name for your labeling job\n",
    "LABELING_JOB_DISPLAY_NAME = 'Anomaly_Detection_Labeling_Job_V1'\n",
    "\n",
    "# Instruction URI for labelers (a PDF or web page with clear labeling guidelines)\n",
    "INSTRUCTION_URI = 'gs://cloud-samples-data/ai-platform-unified/instructions/instructions.pdf' # REPLACE or create your own\n",
    "\n",
    "# Annotation spec schema URI (JSON schema for your labels)\n",
    "# For 'Anomaly vs. Normal' you might have a simple classification schema.\n",
    "# Example: a schema for image classification where 'anomaly' and 'normal' are labels.\n",
    "# Example content for `classification_schema.yaml` in GCS:\n",
    "# data_item_schema_uri: \"gs://google-cloud-aiplatform/schema/data_item/image_1.0.0.yaml\"\n",
    "# annotation_types:\n",
    "#   - display_name: \"Anomaly\"\n",
    "#     description: \"The image shows an anomaly.\"\n",
    "#     label_type: \"classification\"\n",
    "#   - display_name: \"Normal\"\n",
    "#     description: \"The image shows a normal pattern.\"\n",
    "#     label_type: \"classification\"\n",
    "ANNOTATION_SPEC_SCHEMA_URI = f'gs://{GCS_BUCKET_NAME}/labeling_schemas/anomaly_normal_classification_schema.yaml' # REPLACE with your actual schema\n",
    "\n",
    "# Worker pool configuration (optional, typically used for custom workforce)\n",
    "# For Google-managed workforce, leave this as default.\n",
    "# See: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.dataLabelingJobs\n",
    "\n",
    "# Duration for which the labeling job is available for annotators\n",
    "# 7 days (604800 seconds)\n",
    "LABELING_JOB_DURATION_SECONDS = 604800\n",
    "\n",
    "# Number of human annotators per data item (e.g., 3 for higher confidence)\n",
    "ANNOTATOR_COUNT = 3\n",
    "\n",
    "print(f\"\\nLabeling Job Configuration:\")\n",
    "print(f\"  Input Data: {INPUT_DATA_GCS_URI}\")\n",
    "print(f\"  Output URI: {OUTPUT_GCS_URI}\")\n",
    "print(f\"  Job Name: {LABELING_JOB_DISPLAY_NAME}\")\n",
    "print(f\"  Instructions: {INSTRUCTION_URI}\")\n",
    "print(f\"  Schema: {ANNOTATION_SPEC_SCHEMA_URI}\")\n",
    "print(f\"  Annotators per item: {ANNOTATOR_COUNT}\")\n",
    "\n",
    "\n",
    "# --- Step 1: Prepare your input data file (if not already done) ---\n",
    "# Create a dummy CSV with GCS URIs for demonstration\n",
    "# In a real scenario, this file would list all images you want to label.\n",
    "if not gcs_client.get_bucket(GCS_BUCKET_NAME).blob(INPUT_DATA_GCS_URI.split(f'{GCS_BUCKET_NAME}/')[1]).exists():\n",
    "    print(f\"\\nCreating a dummy input data file: {INPUT_DATA_GCS_URI}\")\n",
    "    # Get a few sample image URIs to put into the CSV\n",
    "    sample_uris = [f\"gs://{b.bucket.name}/{b.name}\" for b in random.sample(image_blobs, min(10, len(image_blobs)))]\n",
    "    dummy_csv_content = \"\\n\".join(sample_uris)\n",
    "    \n",
    "    input_blob = gcs_client.get_bucket(GCS_BUCKET_NAME).blob(INPUT_DATA_GCS_URI.split(f'{GCS_BUCKET_NAME}/')[1])\n",
    "    input_blob.upload_from_string(dummy_csv_content, content_type='text/csv')\n",
    "    print(f\"Dummy input data created at {INPUT_DATA_GCS_URI}\")\n",
    "else:\n",
    "    print(f\"Input data file already exists at {INPUT_DATA_GCS_URI}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Ensure your annotation schema is in GCS ---\n",
    "# You would upload your actual `anomaly_normal_classification_schema.yaml` here.\n",
    "# For demonstration, let's create a very basic placeholder if it doesn't exist.\n",
    "if not gcs_client.get_bucket(GCS_BUCKET_NAME).blob(ANNOTATION_SPEC_SCHEMA_URI.split(f'{GCS_BUCKET_NAME}/')[1]).exists():\n",
    "    print(f\"\\nCreating a dummy annotation schema at {ANNOTATION_SPEC_SCHEMA_URI}\")\n",
    "    dummy_schema_content = \"\"\"\n",
    "data_item_schema_uri: \"gs://google-cloud-aiplatform/schema/data_item/image_1.0.0.yaml\"\n",
    "annotation_types:\n",
    "  - display_name: \"Anomaly\"\n",
    "    description: \"The image shows an anomaly.\"\n",
    "    label_type: \"classification\"\n",
    "  - display_name: \"Normal\"\n",
    "    description: \"The image shows a normal pattern.\"\n",
    "    label_type: \"classification\"\n",
    "\"\"\"\n",
    "    schema_blob = gcs_client.get_bucket(GCS_BUCKET_NAME).blob(ANNOTATION_SPEC_SCHEMA_URI.split(f'{GCS_BUCKET_NAME}/')[1])\n",
    "    schema_blob.upload_from_string(dummy_schema_content, content_type='text/yaml')\n",
    "    print(f\"Dummy schema created at {ANNOTATION_SPEC_SCHEMA_URI}\")\n",
    "else:\n",
    "    print(f\"Annotation schema already exists at {ANNOTATION_SPEC_SCHEMA_URI}\")\n",
    "\n",
    "# --- Step 3: Create and Run the Data Labeling Job ---\n",
    "try:\n",
    "    # Use the `aiplatform.ImageDataset.create_from_gcs()` method\n",
    "    # It will implicitly create a dataset and then start the labeling job.\n",
    "    # Note: For `DataLabelingJob` directly, you'd typically define a `model_evaluation_job` and then create the job.\n",
    "    # The SDK provides a simpler way to start an image labeling job directly.\n",
    "    \n",
    "    # We first create a dummy Vertex AI Dataset. Data Labeling Job needs a Dataset ID.\n",
    "    # In a real scenario, you'd create this Dataset and then pass its ID.\n",
    "    # Or, the labeling job can create a temporary dataset.\n",
    "    \n",
    "    # The `aiplatform.DataLabelingJob.create()` method is the most direct way\n",
    "    # to initiate from Python.\n",
    "    \n",
    "    print(\"\\nAttempting to create Vertex AI Data Labeling Job...\")\n",
    "\n",
    "    # The actual job creation payload\n",
    "    data_labeling_job_payload = {\n",
    "        \"display_name\": LABELING_JOB_DISPLAY_NAME,\n",
    "        \"specialist_pools\": [], # Use Google-managed workforce\n",
    "        \"input_config\": {\n",
    "            \"gcs_source\": {\n",
    "                \"input_uris\": [INPUT_DATA_GCS_URI]\n",
    "            },\n",
    "            \"data_type\": \"IMAGE\" # For image labeling\n",
    "        },\n",
    "        \"output_config\": {\n",
    "            \"gcs_destination\": {\n",
    "                \"output_uri_prefix\": OUTPUT_GCS_URI\n",
    "            }\n",
    "        },\n",
    "        \"instruction_uri\": INSTRUCTION_URI,\n",
    "        \"annotation_spec_set_config\": {\n",
    "            \"annotation_spec_schema_uri\": ANNOTATION_SPEC_SCHEMA_URI,\n",
    "            \"replica_count\": ANNOTATOR_COUNT,\n",
    "            \"question_type\": \"IMAGE_CLASSIFICATION\", # Or IMAGE_BOUNDING_BOX, IMAGE_SEGMENTATION etc.\n",
    "            \"human_annotation_management_config\": {\n",
    "                \"human_annotation_request_config\": {\n",
    "                    \"question_type\": \"IMAGE_CLASSIFICATION_ANNOTATION\",\n",
    "                    \"instruction_uri\": INSTRUCTION_URI,\n",
    "                    \"manual_batch_tuning_parameters\": {},\n",
    "                    \"schema_configs\": [\n",
    "                        # This part specifies how your schema maps to the UI\n",
    "                        # For simple classification, this is often handled implicitly\n",
    "                    ],\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"duration\": {\"seconds\": LABELING_JOB_DURATION_SECONDS}\n",
    "    }\n",
    "\n",
    "    # Use the low-level client for full flexibility if needed, or the SDK\n",
    "    # For now, let's use the SDK's higher-level wrapper if available for this specific task\n",
    "    # Or, show the REST API approach for clarity if SDK doesn't abstract it well.\n",
    "\n",
    "    # Option 1: Use SDK's `create_image_data_labeling_job` (more direct for common cases)\n",
    "    # This might require some adjustments to parameters based on the latest SDK version.\n",
    "    \n",
    "    # Option 2: Use the lower-level `aiplatform.gapic.JobServiceClient` for full control\n",
    "    from google.cloud.aiplatform_v1.services import job_service\n",
    "    from google.cloud.aiplatform_v1.types import DataLabelingJob, GcsSource, GcsDestination\n",
    "\n",
    "    job_client = job_service.JobServiceClient(client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"})\n",
    "\n",
    "    # Prepare the DataLabelingJob object\n",
    "    dl_job = DataLabelingJob(\n",
    "        display_name=LABELING_JOB_DISPLAY_NAME,\n",
    "        inputs=GcsSource(input_uris=[INPUT_DATA_GCS_URI]),\n",
    "        outputs=GcsDestination(output_uri_prefix=OUTPUT_GCS_URI),\n",
    "        # You'll need to define the instructions and annotation spec set directly in the proto format\n",
    "        # This is where the complexity comes in for `DataLabelingJob` via client.\n",
    "        # For simplicity, if SDK offers a direct method for Image Classification Labeling, use that.\n",
    "        \n",
    "        # Example using the higher-level SDK for image classification\n",
    "        # Note: This is an example, please refer to latest SDK docs for exact method signature\n",
    "        # as it can vary.\n",
    "        # aiplatform.ImageDataset.create_from_gcs requires an import path not provided directly\n",
    "        # by the basic aiplatform client.\n",
    "    )\n",
    "\n",
    "    # Simplified call via `aiplatform` SDK if it supports it cleanly:\n",
    "    # This is a conceptual call; the actual method signature might differ.\n",
    "    # Refer to `aiplatform.DataLabelingJob` documentation.\n",
    "    \n",
    "    # As of current SDK, creating directly is complex. It's usually done via console\n",
    "    # or by wrapping it in a custom Python function that builds the proto.\n",
    "    print(\"\\nNote: Directly creating complex Data Labeling Jobs via Python SDK can be verbose.\")\n",
    "    print(\"For a full functional example, refer to Google Cloud's official documentation for Data Labeling API.\")\n",
    "    print(f\"You would typically navigate to Vertex AI -> Data Labeling in the console to create/monitor the job, or use a more detailed Python script.\")\n",
    "    print(f\"Your input data list is: {INPUT_DATA_GCS_URI}\")\n",
    "    print(f\"Your schema is at: {ANNOTATION_SPEC_SCHEMA_URI}\")\n",
    "    print(f\"Once created, monitor the job at: https://console.cloud.google.com/vertex-ai/data-labeling-jobs/list?project={PROJECT_ID}&region={REGION}\")\n",
    "\n",
    "    # Dummy placeholder for actually starting the job (replace with real code if needed)\n",
    "    # data_labeling_job = aiplatform.DataLabelingJob.create(\n",
    "    #     display_name=LABELING_JOB_DISPLAY_NAME,\n",
    "    #     # ... other parameters from data_labeling_job_payload\n",
    "    # )\n",
    "    # print(f\"Data Labeling Job '{data_labeling_job.display_name}' created. ID: {data_labeling_job.name}\")\n",
    "    # print(f\"Monitor job in console: {data_labeling_job.resource_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Data Labeling Job: {e}\")\n",
    "    print(\"Please ensure your service account has 'Vertex AI User' and 'Data Labeler' roles, and necessary files exist in GCS.\")\n",
    "    print(\"Also ensure the Vertex AI Data Labeling API is enabled in your project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e740ad3-ce61-4c4b-b66c-37629735c1ed",
   "metadata": {},
   "source": [
    "### Dataset Versioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63809ffb-c904-4376-898b-19b30d653178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Versioning with Vertex AI Datasets\n",
    "\n",
    "print(\"Demonstrating Dataset Versioning with Vertex AI Datasets.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_DISPLAY_NAME = 'Anomaly_Vision_Dataset'\n",
    "DATASET_GCS_INPUT_URI = f'gs://{GCS_BUCKET_NAME}/{GCS_DATA_PREFIX}' # Your source image directory\n",
    "# If you have a specific CSV/JSONL file that lists all images in the dataset, use that:\n",
    "# DATASET_GCS_INPUT_URI = f'gs://{GCS_BUCKET_NAME}/all_image_uris.csv'\n",
    "\n",
    "# --- 1. Create a new Vertex AI Dataset (if it doesn't exist) ---\n",
    "print(f\"\\nChecking for existing dataset: {DATASET_DISPLAY_NAME}\")\n",
    "datasets = aiplatform.ImageDataset.list(filter=f'display_name=\"{DATASET_DISPLAY_NAME}\"')\n",
    "\n",
    "if datasets:\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Found existing dataset: {dataset.display_name} (ID: {dataset.resource_name})\")\n",
    "    print(\"You can typically import new data to update a dataset, which implicitly creates a new version.\")\n",
    "else:\n",
    "    print(f\"Creating new dataset: {DATASET_DISPLAY_NAME}\")\n",
    "    try:\n",
    "        # Create an Image Dataset\n",
    "        # The import process can take time.\n",
    "        # For large datasets, consider creating an empty dataset first, then importing in chunks.\n",
    "        dataset = aiplatform.ImageDataset.create(\n",
    "            display_name=DATASET_DISPLAY_NAME,\n",
    "            gcs_source=DATASET_GCS_INPUT_URI, # Path to your images (can be folder or URI list)\n",
    "            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification, # Or image.bounding_box etc.\n",
    "            sync=True # Wait for the dataset creation to complete\n",
    "        )\n",
    "        print(f\"Dataset '{dataset.display_name}' created successfully. ID: {dataset.resource_name}\")\n",
    "        print(f\"Dataset in console: https://console.cloud.google.com/vertex-ai/datasets/images/{dataset.name.split('/')[-1]}?project={PROJECT_ID}&region={REGION}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        print(\"Ensure the GCS_DATA_PREFIX contains valid image files and your service account has necessary permissions.\")\n",
    "        print(\"You may need to provide a CSV/JSONL file with image URIs if direct folder import doesn't work for your setup.\")\n",
    "        # Attempt to list datasets even if creation failed, to prevent further errors\n",
    "        datasets = aiplatform.ImageDataset.list(filter=f'display_name=\"{DATASET_DISPLAY_NAME}\"')\n",
    "        if datasets:\n",
    "            dataset = datasets[0]\n",
    "            print(f\"Found existing dataset despite creation error: {dataset.display_name}\")\n",
    "        else:\n",
    "            print(\"Could not find or create dataset. Skipping further steps.\")\n",
    "            dataset = None # Ensure dataset is None if not found/created\n",
    "\n",
    "\n",
    "if dataset:\n",
    "    # --- 2. Check Dataset Versioning / Import New Data ---\n",
    "    print(f\"\\nDataset ID: {dataset.resource_name}\")\n",
    "    print(f\"Dataset labels: {dataset.labels}\") # Labels can be used for versioning/metadata\n",
    "\n",
    "    # To create a new \"version\" or update the dataset, you import new data.\n",
    "    # Vertex AI Datasets manages versions implicitly by tracking the import history.\n",
    "    # Each import job creates a new \"snapshot\" or iteration of the dataset.\n",
    "\n",
    "    # Example: If you have a new set of labeled data (e.g., from your Data Labeling Job)\n",
    "    # You would import it into this dataset.\n",
    "    LABELED_DATA_IMPORT_URI = f'gs://{GCS_BUCKET_NAME}/labeled_data_output/annotations.jsonl' # Example from Data Labeling Job output\n",
    "\n",
    "    print(f\"\\nAttempting to import new data (simulating a new version/update).\")\n",
    "    print(f\"Importing from: {LABELED_DATA_IMPORT_URI}\")\n",
    "    \n",
    "    try:\n",
    "        # This will create a new import job for the dataset.\n",
    "        # It's important to have a valid import schema for your data (e.g., for labeled images).\n",
    "        # For labeled data, the schema will be different (e.g., `single_label_classification`).\n",
    "        # This process implicitly versions the dataset.\n",
    "        \n",
    "        # NOTE: A real import requires `LABELED_DATA_IMPORT_URI` to be a valid JSONL\n",
    "        # file in the Vertex AI Dataset import format, e.g., with image URIs and annotations.\n",
    "        # Example structure for image classification JSONL:\n",
    "        # {\"imageGcsUri\": \"gs://bucket/img1.jpg\", \"classificationAnnotation\": {\"displayName\": \"Normal\"}}\n",
    "        # {\"imageGcsUri\": \"gs://bucket/img2.jpg\", \"classificationAnnotation\": {\"displayName\": \"Anomaly\"}}\n",
    "\n",
    "        # Check if dummy labeled data exists (from Cell 4)\n",
    "        if not gcs_client.get_bucket(GCS_BUCKET_NAME).blob(LABELED_DATA_IMPORT_URI.split(f'{GCS_BUCKET_NAME}/')[1]).exists():\n",
    "             print(f\"Warning: Dummy labeled data file not found at {LABELED_DATA_IMPORT_URI}. Skipping import.\")\n",
    "             print(\"Please run Cell 4 to create a labeling job first, then generate some dummy output.\")\n",
    "             print(\"Alternatively, create a dummy annotations.jsonl manually with valid format.\")\n",
    "        else:\n",
    "            # This is a blocking call: `sync=True`\n",
    "            dataset.import_data(\n",
    "                gcs_source=[LABELED_DATA_IMPORT_URI],\n",
    "                import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification, # Adjust schema as needed\n",
    "                sync=True\n",
    "            )\n",
    "            print(\"New data successfully imported into the dataset. This acts as a new 'version'.\")\n",
    "            print(\"The dataset now includes the newly imported data, implicitly versioned by the import job.\")\n",
    "            print(f\"You can see import history in the Vertex AI console under the dataset's 'Imports' tab.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing new data into dataset: {e}\")\n",
    "        print(\"Ensure the import URI points to a valid file formatted according to the schema.\")\n",
    "        print(\"Common issues: incorrect JSONL format, invalid image URIs, schema mismatch.\")\n",
    "\n",
    "    # --- 3. Accessing Dataset Information (for MLOps Tracking) ---\n",
    "    print(f\"\\nDataset resource name: {dataset.resource_name}\")\n",
    "    print(f\"Dataset create time: {dataset.create_time.isoformat()}\")\n",
    "    print(f\"Dataset update time: {dataset.update_time.isoformat()}\")\n",
    "\n",
    "    # You can get the import history, which serves as version tracking\n",
    "    print(\"\\nRecent Dataset Import History (implicit versioning):\")\n",
    "    # This requires using the lower-level GAPIC client for detailed history or parsing logs.\n",
    "    # The `aiplatform` SDK often provides simpler ways to get core info.\n",
    "    # For a full history, you'd typically look at `dataset.metadata_artifact.lineage_subgraph` or monitor operations.\n",
    "    print(\"Please view the full import history in the Vertex AI console under the dataset's 'Imports' tab.\")\n",
    "\n",
    "\n",
    "    # --- 4. Using the Dataset for Training ---\n",
    "    print(\"\\nThis dataset can now be used as input for Vertex AI Training jobs.\")\n",
    "    print(\"Example (conceptual):\")\n",
    "    print(\"training_job = aiplatform.CustomTrainingJob(...)\")\n",
    "    print(\"model = training_job.run(dataset=dataset, ...)\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping dataset versioning steps as no dataset could be created or found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364afd3-3bf8-4918-bb26-be87595f6d17",
   "metadata": {},
   "source": [
    "### **Anomaly Detection Model Development & Experimentation:**\n",
    "\n",
    "- **Architectural Exploration:** Experiment with various TensorFlow model architectures for anomaly detection (e.g., autoencoders, GANs for anomaly generation, one-class classification models, pre-trained backbone + anomaly head).\n",
    "- **Gemini Vision Integration:** Experiment with using Gemini Vision for generating image embeddings, classifying image content, or generating descriptions that could aid anomaly detection (e.g., \"damaged part of machine\").\n",
    "- **Hyperparameter Tuning:** Use Vertex AI Vizier to systematically tune hyperparameters for their anomaly detection models, leveraging Workbench's compute power.\n",
    "- **Custom Training Logic:** Write and test custom TensorFlow training loops and loss functions specifically tailored for anomaly detection (e.g., reconstruction loss, novelty detection metrics).\n",
    "- **Experiment Tracking:** Log model performance metrics (e.g., AUC, precision, recall for anomaly detection), training curves, and model checkpoints to Vertex AI Experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89806e-1a9c-4536-966c-f19cac7b78f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading and Exploring Datasets:\n",
    "\n",
    "- Data Access: Connect to Cloud Storage buckets (e.g., for raw image data, processed features) and BigQuery tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f9b25-1427-46b7-b512-c3ea533650fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Basic Data Loading for Model Development\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Google Cloud SDKs\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.vision_v1 import types\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part, Image as GImage\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, models, optimizers\n",
    "\n",
    "# Scikit-learn for evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up GCP project and region (ensure these match your environment)\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n",
    "REGION = 'us-central1' # Or your desired region\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    try:\n",
    "        _, project_id = aiplatform.initializer.global_config.get_client_options()\n",
    "        PROJECT_ID = project_id\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"YOUR_GCP_PROJECT_ID\" # REPLACE WITH YOUR PROJECT ID\n",
    "\n",
    "print(f\"Using Google Cloud Project: {PROJECT_ID}\")\n",
    "print(f\"Using Google Cloud Region: {REGION}\")\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "gcs_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize Vertex AI for Generative AI (Gemini)\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "gemini_model_pro_vision = GenerativeModel(\"gemini-pro-vision\")\n",
    "gemini_model_pro = GenerativeModel(\"gemini-pro\") # For text-only generation\n",
    "\n",
    "# --- Configuration for your dataset (use your \"normal\" data for training anomaly models) ---\n",
    "# Assuming your 'normal' images are in a specific GCS subfolder\n",
    "GCS_NORMAL_DATA_PREFIX = 'raw_images/normal/' # REPLACE with your actual normal data path\n",
    "GCS_ANOMALY_DATA_PREFIX = 'raw_images/anomaly/' # REPLACE with your actual anomaly data path (for testing)\n",
    "GCS_BUCKET_NAME = 'your-image-dataset-bucket' # REPLACE with your actual GCS bucket name\n",
    "\n",
    "IMAGE_SIZE = (128, 128) # Standardize image size for models\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"\\nLoading sample 'normal' images from: gs://{GCS_BUCKET_NAME}/{GCS_NORMAL_DATA_PREFIX}\")\n",
    "\n",
    "# --- Helper function to load and preprocess images for TensorFlow ---\n",
    "def load_and_preprocess_image(gcs_uri, label=None, image_size=IMAGE_SIZE):\n",
    "    \"\"\"Loads image from GCS URI, decodes, resizes, and normalizes.\"\"\"\n",
    "    img_bytes = tf.io.read_file(gcs_uri)\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3) # Adjust for PNG/BMP as needed\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = img / 255.0 # Normalize to [0, 1]\n",
    "    if label is not None:\n",
    "        return img, label\n",
    "    return img\n",
    "\n",
    "# --- Load a small sample of normal image URIs for in-notebook experimentation ---\n",
    "normal_image_uris = []\n",
    "try:\n",
    "    bucket = gcs_client.get_bucket(GCS_BUCKET_NAME)\n",
    "    normal_blobs = list(bucket.list_blobs(prefix=GCS_NORMAL_DATA_PREFIX))\n",
    "    normal_image_uris = [f\"gs://{b.bucket.name}/{b.name}\" for b in normal_blobs if b.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Found {len(normal_image_uris)} 'normal' images.\")\n",
    "\n",
    "    # Create a TensorFlow Dataset for normal data\n",
    "    # Use a small subset for in-notebook quick tests\n",
    "    num_normal_samples = min(500, len(normal_image_uris)) # Limit for quick testing\n",
    "    train_uris, val_uris = train_test_split(normal_image_uris[:num_normal_samples], test_size=0.2, random_state=42)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Prepared {len(train_uris)} training normal samples and {len(val_uris)} validation normal samples.\")\n",
    "\n",
    "    # Load some anomaly image URIs for testing\n",
    "    anomaly_image_uris = []\n",
    "    anomaly_blobs = list(bucket.list_blobs(prefix=GCS_ANOMALY_DATA_PREFIX))\n",
    "    anomaly_image_uris = [f\"gs://{b.bucket.name}/{b.name}\" for b in anomaly_blobs if b.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Found {len(anomaly_image_uris)} 'anomaly' images for testing.\")\n",
    "\n",
    "    # Create a TensorFlow Dataset for anomaly data (for evaluation later)\n",
    "    anomaly_ds = tf.data.Dataset.from_tensor_slices(anomaly_image_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading initial image data: {e}\")\n",
    "    print(\"Ensure GCS_BUCKET_NAME, GCS_NORMAL_DATA_PREFIX, and GCS_ANOMALY_DATA_PREFIX are correct.\")\n",
    "    train_ds, val_ds, anomaly_ds = None, None, None # Prevent further errors if data not loaded\n",
    "\n",
    "print(\"Setup complete. Basic data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbebb04-6de0-4edc-b3ab-0329dc99b573",
   "metadata": {},
   "source": [
    "#### Architectural Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d3c71-184f-4991-bfd5-7289e30d9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectural Exploration (Autoencoders for Anomaly Detection)\n",
    "\n",
    "print(\"--- Architectural Exploration: Autoencoder Model ---\")\n",
    "print(\"An autoencoder learns to reconstruct normal data. Anomalies result in high reconstruction error.\")\n",
    "\n",
    "def build_autoencoder(input_shape):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_inputs)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # Latent space representation\n",
    "    latent = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, latent, name=\"encoder\")\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=latent.shape[1:]) # Input to decoder is the latent space shape\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(decoder_inputs)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(input_shape[-1], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    decoder = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    autoencoder_outputs = decoder(encoder(encoder_inputs))\n",
    "    autoencoder = keras.Model(encoder_inputs, autoencoder_outputs, name=\"autoencoder\")\n",
    "    return autoencoder\n",
    "\n",
    "if train_ds:\n",
    "    autoencoder_model = build_autoencoder(IMAGE_SIZE + (3,))\n",
    "    autoencoder_model.compile(optimizer='adam', loss='mse') # Mean Squared Error for reconstruction loss\n",
    "\n",
    "    print(\"\\nAutoencoder Model Summary:\")\n",
    "    autoencoder_model.summary()\n",
    "\n",
    "    print(\"\\nTraining Autoencoder on 'Normal' Data (a small subset for quick test)...\")\n",
    "    history = autoencoder_model.fit(\n",
    "        train_ds,\n",
    "        epochs=5, # Keep epochs low for quick notebook demo\n",
    "        validation_data=val_ds,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Autoencoder Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Visualize Reconstruction ---\n",
    "    print(\"\\nVisualizing Original vs. Reconstructed Images:\")\n",
    "    sample_batch = next(iter(val_ds)) # Get a batch of validation images\n",
    "    original_images = sample_batch[:min(5, BATCH_SIZE)] # Take first few images\n",
    "    reconstructed_images = autoencoder_model.predict(original_images)\n",
    "\n",
    "    n = len(original_images)\n",
    "    plt.figure(figsize=(2 * n, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original_images[i])\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed_images[i])\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Anomaly Scoring using Reconstruction Error ---\n",
    "    print(\"\\nCalculating Reconstruction Error for Normal vs. Anomaly Images:\")\n",
    "    \n",
    "    # Calculate MSE for normal images\n",
    "    normal_reconstruction_errors = []\n",
    "    for batch in val_ds:\n",
    "        reconstructed_batch = autoencoder_model.predict(batch)\n",
    "        mse = tf.reduce_mean(tf.square(batch - reconstructed_batch), axis=(1, 2, 3))\n",
    "        normal_reconstruction_errors.extend(mse.numpy())\n",
    "\n",
    "    # Calculate MSE for anomaly images\n",
    "    anomaly_reconstruction_errors = []\n",
    "    if anomaly_ds:\n",
    "        for batch in anomaly_ds:\n",
    "            reconstructed_batch = autoencoder_model.predict(batch)\n",
    "            mse = tf.reduce_mean(tf.square(batch - reconstructed_batch), axis=(1, 2, 3))\n",
    "            anomaly_reconstruction_errors.extend(mse.numpy())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(normal_reconstruction_errors, bins=50, kde=True, color='blue', label='Normal')\n",
    "    if anomaly_ds and anomaly_reconstruction_errors:\n",
    "        sns.histplot(anomaly_reconstruction_errors, bins=50, kde=True, color='red', label='Anomaly')\n",
    "    plt.title('Distribution of Reconstruction Errors')\n",
    "    plt.xlabel('Reconstruction Error (MSE)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Expected: Anomaly images should have higher reconstruction errors than normal images.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Autoencoder exploration as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21996c94-76fa-4c10-9075-41ed8d02c853",
   "metadata": {},
   "source": [
    "#### Gemini Version Integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e092c-27d1-432f-b4d5-1f369f005291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini Vision Integration for Anomaly Detection\n",
    "\n",
    "print(\"--- Gemini Vision Integration ---\")\n",
    "print(\"Leveraging Gemini Vision for image understanding to aid anomaly detection.\")\n",
    "\n",
    "# --- Helper function to load image as GImage for Gemini ---\n",
    "def load_gimage_from_gcs(gcs_uri):\n",
    "    \"\"\"Loads an image from GCS and converts to vertexai.preview.generative_models.Image.\"\"\"\n",
    "    try:\n",
    "        bucket_name = gcs_uri.split('gs://')[1].split('/')[0]\n",
    "        blob_name = '/'.join(gcs_uri.split('gs://')[1].split('/')[1:])\n",
    "        bucket = gcs_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        img_bytes = blob.download_as_bytes()\n",
    "        return GImage.from_bytes(img_bytes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {gcs_uri} for Gemini: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get a few sample image URIs (mix of normal and anomaly if possible)\n",
    "sample_image_uris = []\n",
    "if normal_image_uris:\n",
    "    sample_image_uris.extend(random.sample(normal_image_uris, min(3, len(normal_image_uris))))\n",
    "if anomaly_image_uris:\n",
    "    sample_image_uris.extend(random.sample(anomaly_image_uris, min(2, len(anomaly_image_uris))))\n",
    "\n",
    "if sample_image_uris:\n",
    "    print(f\"\\nAnalyzing {len(sample_image_uris)} sample images using Gemini Vision:\")\n",
    "    for i, uri in enumerate(sample_image_uris):\n",
    "        print(f\"\\n--- Processing Image {i+1}: {uri.split('/')[-1]} ---\")\n",
    "        g_image = load_gimage_from_gcs(uri)\n",
    "\n",
    "        if g_image:\n",
    "            # Display image in notebook\n",
    "            img_bytes = gcs_client.get_bucket(uri.split('gs://')[1].split('/')[0]).blob('/'.join(uri.split('gs://')[1].split('/')[1:])).download_as_bytes()\n",
    "            display(Image.open(io.BytesIO(img_bytes)).resize((200, 200))) # display original image\n",
    "\n",
    "            # --- 1. Generate Image Descriptions / Captions ---\n",
    "            try:\n",
    "                prompt_description = \"Describe this image in detail, focusing on any objects, patterns, or unusual features.\"\n",
    "                response_description = gemini_model_pro_vision.generate_content([g_image, prompt_description])\n",
    "                print(f\"Gemini Description:\\n{response_description.text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating description for {uri}: {e}\")\n",
    "\n",
    "            # --- 2. Anomaly-Specific Questioning ---\n",
    "            # Ask targeted questions to identify anomalies based on expected 'normal' features\n",
    "            prompt_anomaly_q = \"Does this image show any signs of damage, wear, or unusual patterns for a machine part? Explain why or why not.\"\n",
    "            try:\n",
    "                response_anomaly_q = gemini_model_pro_vision.generate_content([g_image, prompt_anomaly_q])\n",
    "                print(f\"\\nGemini Anomaly Analysis:\\n{response_anomaly_q.text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with anomaly analysis for {uri}: {e}\")\n",
    "\n",
    "            # --- 3. (Conceptual) Generating Embeddings for Classification ---\n",
    "            # As of current Gemini API, direct programmatic access to raw embeddings\n",
    "            # for images/multimodal content isn't directly exposed for easy extraction\n",
    "            # like text embeddings via `textembedding-gecko`. However, the model uses them\n",
    "            # internally. You would typically use the generated text descriptions\n",
    "            # and then get text embeddings from `textembedding-gecko` for downstream tasks,\n",
    "            # or use the image itself directly in a multimodal classification model.\n",
    "            \n",
    "            # If `textembedding-gecko` is what you'd use for text features from Gemini's output:\n",
    "            try:\n",
    "                text_embeddings_model = aiplatform.TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "                # Use a part of Gemini's description to get an embedding\n",
    "                if response_description and response_description.text:\n",
    "                    description_embedding = text_embeddings_model.predict([response_description.text])\n",
    "                    print(f\"\\nExample Text Embedding (from description): Shape {description_embedding.embeddings[0].values.shape}\")\n",
    "                    # These embeddings can be used as features for a separate anomaly classifier\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting text embedding (ensure textembedding-gecko is enabled): {e}\")\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"Skipping Gemini analysis for {uri} due to image loading error.\")\n",
    "\n",
    "    print(\"\\nGemini Vision can be powerful for zero-shot anomaly detection by describing anomalies or for generating features (textual descriptions) to train other models.\")\n",
    "else:\n",
    "    print(\"Skipping Gemini Vision integration as no sample images were available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ef21b-702c-4fbd-b45b-528cd3c5dc67",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec607209-22cb-41d9-af28-3a0b43049dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with Vertex AI Vizier\n",
    "\n",
    "print(\"--- Hyperparameter Tuning with Vertex AI Vizier ---\")\n",
    "print(\"Vertex AI Vizier systematically tunes hyperparameters to find optimal model performance.\")\n",
    "\n",
    "# --- 1. Define the Training Script ---\n",
    "# This script will be executed by the Vizier study.\n",
    "# It should accept hyperparameters as command-line arguments and report the objective metric.\n",
    "\n",
    "TRAINING_SCRIPT_NAME = 'anomaly_trainer.py'\n",
    "TRAINING_SCRIPT_CONTENT = f\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from google.cloud import storage\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "# Helper function to load image from GCS and preprocess\n",
    "def load_and_preprocess_image(gcs_uri, image_size=({IMAGE_SIZE[0]}, {IMAGE_SIZE[1]})):\n",
    "    img_bytes = tf.io.read_file(gcs_uri)\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Autoencoder Model Definition\n",
    "def build_autoencoder(input_shape, latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_inputs)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # Adjust latent dimension using Conv2D or Dense layers\n",
    "    x = layers.Conv2D(latent_dim, (3, 3), activation='relu', padding='same')(x) # Latent layer\n",
    "\n",
    "    decoder_inputs_shape = x.shape[1:] # Capture shape after latent layer\n",
    "    \n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(input_shape[-1], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = keras.Model(encoder_inputs, decoder_outputs, name=\"autoencoder\")\n",
    "    return autoencoder\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--latent_dimension', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=10) # Vizier will control this\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--normal_data_uri', type=str, required=True)\n",
    "    parser.add_argument('--anomaly_data_uri', type=str, required=True)\n",
    "    parser.add_argument('--model_dir', type=str, default=os.getenv('AIP_MODEL_DIR')) # Vertex AI managed output dir\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Starting training with LR={args.learning_rate}, LatentDim={args.latent_dimension}, Epochs={args.epochs}\")\n",
    "\n",
    "    # Load data from GCS\n",
    "    gcs_client = storage.Client()\n",
    "    normal_uris = [f\"gs://{{args.normal_data_uri.split('gs://')[1]}}{blob.name}\" for blob in gcs_client.list_blobs(args.normal_data_uri.split('gs://')[1].split('/')[0], prefix='/'.join(args.normal_data_uri.split('gs://')[1].split('/')[1:])) if blob.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    anomaly_uris = [f\"gs://{{args.anomaly_data_uri.split('gs://')[1]}}{blob.name}\" for blob in gcs_client.list_blobs(args.anomaly_data_uri.split('gs://')[1].split('/')[0], prefix='/'.join(args.anomaly_data_uri.split('gs://')[1].split('/')[1:])) if blob.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # For simplicity, use first X normal images for training/validation\n",
    "    num_train_samples = min(2000, len(normal_uris)) # Use more data for actual training\n",
    "    train_uris, val_uris = train_test_split(normal_uris[:num_train_samples], test_size=0.2, random_state=42)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # For evaluation, load anomaly data and some normal data\n",
    "    test_normal_uris = normal_uris[num_train_samples:min(num_train_samples + 500, len(normal_uris))]\n",
    "    test_anomaly_uris = anomaly_uris[:min(500, len(anomaly_uris))] # Use up to 500 anomalies for test\n",
    "\n",
    "    test_normal_ds = tf.data.Dataset.from_tensor_slices(test_normal_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_anomaly_ds = tf.data.Dataset.from_tensor_slices(test_anomaly_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(args.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    autoencoder_model = build_autoencoder(IMAGE_SIZE + (3,), args.latent_dimension)\n",
    "    autoencoder_model.compile(optimizer=optimizers.Adam(learning_rate=args.learning_rate), loss='mse')\n",
    "\n",
    "    history = autoencoder_model.fit(\n",
    "        train_ds,\n",
    "        epochs=args.epochs,\n",
    "        validation_data=val_ds,\n",
    "        verbose=0 # Run silently for Vizier\n",
    "    )\n",
    "\n",
    "    # --- Evaluate model and report metric to Vizier ---\n",
    "    # Combine test datasets and get reconstruction errors\n",
    "    all_test_images = []\n",
    "    all_test_labels = [] # 0 for normal, 1 for anomaly\n",
    "\n",
    "    for batch in test_normal_ds:\n",
    "        all_test_images.append(batch)\n",
    "        all_test_labels.extend([0] * len(batch))\n",
    "    for batch in test_anomaly_ds:\n",
    "        all_test_images.append(batch)\n",
    "        all_test_labels.extend([1] * len(batch))\n",
    "    \n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"No test data available for evaluation. Skipping metric reporting.\")\n",
    "        exit() # Exit if no data to prevent errors\n",
    "\n",
    "    all_test_images_tensor = tf.concat(all_test_images, axis=0)\n",
    "    reconstructed_test_images = autoencoder_model.predict(all_test_images_tensor)\n",
    "    reconstruction_errors = tf.reduce_mean(tf.square(all_test_images_tensor - reconstructed_test_images), axis=(1, 2, 3)).numpy()\n",
    "\n",
    "    # Calculate AUC for anomaly detection (higher is better)\n",
    "    # Labels: 0 (normal), 1 (anomaly)\n",
    "    # Scores: reconstruction_errors (higher error = more anomalous)\n",
    "    if len(np.unique(all_test_labels)) > 1: # Ensure both classes are present for AUC\n",
    "        auc_score = roc_auc_score(all_test_labels, reconstruction_errors)\n",
    "        print(f\"Validation AUC: {{auc_score:.4f}}\")\n",
    "        # Report the metric to Vizier\n",
    "        # Important: The metric name must match the one defined in the Vizier study config\n",
    "        tf.summary.scalar('val_auc', auc_score, step=args.epochs) # For TensorBoard\n",
    "        # Vertex AI Vizier automatically captures logs for `HP_METRIC_TAG` output or `tf.summary`\n",
    "        # Or you can explicitly print as: print(f'hp_metric: {auc_score}')\n",
    "    else:\n",
    "        print(\"Not enough classes in test data to calculate AUC. Skipping metric reporting.\")\n",
    "    \n",
    "    # Save the best model (optional, for later deployment)\n",
    "    # autoencoder_model.save(args.model_dir) # Vertex AI handles this for experiments.\n",
    "\"\"\"\n",
    "\n",
    "# Write the training script to a file\n",
    "with open(TRAINING_SCRIPT_NAME, 'w') as f:\n",
    "    f.write(TRAINING_SCRIPT_CONTENT)\n",
    "\n",
    "# --- 2. Upload the Training Script to GCS ---\n",
    "TRAINING_PACKAGE_GCS_URI = f'gs://{GCS_BUCKET_NAME}/training_packages/{TRAINING_SCRIPT_NAME}'\n",
    "blob = gcs_client.get_bucket(GCS_BUCKET_NAME).blob(f'training_packages/{TRAINING_SCRIPT_NAME}')\n",
    "blob.upload_from_filename(TRAINING_SCRIPT_NAME)\n",
    "print(f\"\\nTraining script uploaded to: {TRAINING_PACKAGE_GCS_URI}\")\n",
    "\n",
    "# --- 3. Define the Vizier Study Configuration ---\n",
    "DISPLAY_NAME = f'anomaly-detection-vizier-study-{pd.Timestamp.now().strftime(\"%Y%m%d%H%M\")}'\n",
    "TRIAL_COUNT = 10 # Number of hyperparameter trials to run\n",
    "PARALLEL_TRIAL_COUNT = 3 # Number of trials to run in parallel\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "parameter_spec = [\n",
    "    {\"parameter_id\": \"learning_rate\", \"double_value_spec\": {\"min_value\": 1e-4, \"max_value\": 1e-2}, \"scale_type\": \"UNIT_LINEAR_SCALE\"},\n",
    "    {\"parameter_id\": \"latent_dimension\", \"integer_value_spec\": {\"min_value\": 64, \"max_value\": 256}, \"scale_type\": \"UNIT_LINEAR_SCALE\"},\n",
    "    {\"parameter_id\": \"epochs\", \"integer_value_spec\": {\"min_value\": 5, \"max_value\": 20}, \"scale_type\": \"UNIT_LINEAR_SCALE\"}\n",
    "]\n",
    "\n",
    "# Define the objective metric (must match what your training script reports)\n",
    "metric_spec = [{\"metric_id\": \"val_auc\", \"goal\": \"MAXIMIZE\"}]\n",
    "\n",
    "# --- 4. Create and Run the Hyperparameter Tuning Job ---\n",
    "if train_ds:\n",
    "    try:\n",
    "        hp_job = aiplatform.CustomContainerTrainingJob(\n",
    "            display_name=DISPLAY_NAME,\n",
    "            container_uri='gcr.io/deeplearning-platform-release/tf2-gpu.2-13', # Use a suitable TensorFlow container\n",
    "            # Alternatively, if your custom image has everything, use it:\n",
    "            # container_uri=f'us-central1-docker.pkg.dev/{PROJECT_ID}/your-repo/vertex-ai-research-env:latest',\n",
    "            \n",
    "            # The script and command to run it\n",
    "            command=['python', TRAINING_SCRIPT_NAME],\n",
    "            \n",
    "            # Machine configuration for each trial\n",
    "            machine_type='n1-standard-4', # Or more powerful for larger models/data\n",
    "            accelerator_type='NVIDIA_TESLA_V100', # Use GPU\n",
    "            accelerator_count=1,\n",
    "            \n",
    "            # The GCS paths for the training data\n",
    "            args=[\n",
    "                f'--normal_data_uri=gs://{GCS_BUCKET_NAME}/{GCS_NORMAL_DATA_PREFIX}',\n",
    "                f'--anomaly_data_uri=gs://{GCS_BUCKET_NAME}/{GCS_ANOMALY_DATA_PREFIX}'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"\\nStarting Hyperparameter Tuning Job (Vizier Study)... This will take time.\")\n",
    "        hpt_job = hp_job.run_tuning(\n",
    "            service_account=None, # Uses default compute service account\n",
    "            parameter_spec=parameter_spec,\n",
    "            metric_spec=metric_spec,\n",
    "            max_trial_count=TRIAL_COUNT,\n",
    "            parallel_trial_count=PARALLEL_TRIAL_COUNT\n",
    "        )\n",
    "\n",
    "        print(f\"Hyperparameter Tuning Job '{hpt_job.display_name}' started.\")\n",
    "        print(f\"Monitor job in console: https://console.cloud.google.com/vertex-ai/hyperparameter-tuning/jobs/{hpt_job.name.split('/')[-1]}?project={PROJECT_ID}&region={REGION}\")\n",
    "\n",
    "        # Wait for the job to complete and print results\n",
    "        # hpt_job.wait_for_resource_creation() # Wait for job to be created\n",
    "        # hpt_job.wait() # Wait for job to complete (can be long)\n",
    "        # print(\"\\nHyperparameter Tuning Job completed.\")\n",
    "        # print(f\"Best Trial: {hpt_job.trials[0].parameters} with {hpt_job.trials[0].final_measurement.metrics[0].value}\")\n",
    "        print(\"\\n(Note: The cell will return before the Vizier job completes. Check console for results.)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Hyperparameter Tuning Job: {e}\")\n",
    "        print(\"Ensure your service account has 'Vertex AI User' role and necessary APIs are enabled.\")\n",
    "else:\n",
    "    print(\"Skipping Hyperparameter Tuning as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203d47f-681a-4239-886c-d733a8e64227",
   "metadata": {},
   "source": [
    "#### Custom Training Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98b425-cd65-496e-948f-bd760d681304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Logic and Loss Functions\n",
    "\n",
    "print(\"--- Custom Training Logic and Loss Functions ---\")\n",
    "print(\"Implementing a custom training loop for anomaly detection with specialized loss.\")\n",
    "\n",
    "if train_ds and anomaly_ds:\n",
    "    # --- 1. Define a Custom Autoencoder Model ---\n",
    "    # We'll use a slightly different autoencoder or a more complex one\n",
    "    def build_custom_autoencoder(input_shape, filter_base=32):\n",
    "        encoder_inputs = keras.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(filter_base, (3, 3), activation='relu', padding='same')(encoder_inputs)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(filter_base * 2, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(filter_base * 4, (3, 3), activation='relu', padding='same')(x)\n",
    "        latent = layers.MaxPooling2D((2, 2), padding='same')(x) # Example latent representation\n",
    "\n",
    "        decoder_inputs = keras.Input(shape=latent.shape[1:])\n",
    "        x = layers.Conv2DTranspose(filter_base * 4, (3, 3), activation='relu', padding='same')(decoder_inputs)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        x = layers.Conv2DTranspose(filter_base * 2, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        x = layers.Conv2DTranspose(filter_base, (3, 3), activation='relu', padding='same')(x)\n",
    "        decoder_outputs = layers.UpSampling2D((2, 2))(x)\n",
    "        decoder_outputs = layers.Conv2D(input_shape[-1], (3, 3), activation='sigmoid', padding='same')(decoder_outputs)\n",
    "        \n",
    "        autoencoder = keras.Model(encoder_inputs, decoder_outputs, name=\"custom_autoencoder\")\n",
    "        return autoencoder\n",
    "\n",
    "    custom_autoencoder = build_custom_autoencoder(IMAGE_SIZE + (3,))\n",
    "\n",
    "    # --- 2. Define a Custom Loss Function (e.g., Weighted MSE) ---\n",
    "    # In anomaly detection, you might want to penalize reconstruction errors differently\n",
    "    # or add a regularization term based on latent space properties.\n",
    "    def weighted_reconstruction_loss(y_true, y_pred, weight_factor=1.5):\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=[1, 2, 3])\n",
    "        # Example: if you had labels (0=normal, 1=anomaly), you could weight anomaly loss more\n",
    "        # Here, we just use a constant weight factor for demonstration.\n",
    "        return tf.reduce_mean(mse * weight_factor) # Simple weighting for demonstration\n",
    "\n",
    "    # You can also use `losses.MeanSquaredError()` if no custom weighting is needed.\n",
    "    # We'll use a standard MSE for the custom loop to keep focus on the loop itself.\n",
    "    reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # --- 3. Implement a Custom Training Loop ---\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x_batch_normal):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed_images = custom_autoencoder(x_batch_normal, training=True)\n",
    "            loss = reconstruction_loss_fn(x_batch_normal, reconstructed_images)\n",
    "        gradients = tape.gradient(loss, custom_autoencoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, custom_autoencoder.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(x_batch_normal):\n",
    "        reconstructed_images = custom_autoencoder(x_batch_normal, training=False)\n",
    "        loss = reconstruction_loss_fn(x_batch_normal, reconstructed_images)\n",
    "        return loss\n",
    "\n",
    "    EPOCHS = 10 # More epochs for a better demonstration\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(\"\\nStarting Custom Training Loop...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        for batch_idx, x_batch_normal in enumerate(train_ds):\n",
    "            loss = train_step(x_batch_normal)\n",
    "            epoch_train_loss += loss\n",
    "            num_train_batches += 1\n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss.numpy())\n",
    "\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        for batch_idx, x_batch_normal in enumerate(val_ds):\n",
    "            loss = val_step(x_batch_normal)\n",
    "            epoch_val_loss += loss\n",
    "            num_val_batches += 1\n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # Plot custom training losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Custom Train Loss')\n",
    "    plt.plot(val_losses, label='Custom Val Loss')\n",
    "    plt.title('Custom Autoencoder Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCustom training loop complete.\")\n",
    "\n",
    "    # --- 4. Evaluate with Custom Metrics (AUC for Anomaly Detection) ---\n",
    "    print(\"\\nEvaluating Custom Autoencoder for Anomaly Detection (AUC)...\")\n",
    "    \n",
    "    all_test_uris = []\n",
    "    all_test_labels = [] # 0 for normal, 1 for anomaly\n",
    "\n",
    "    # Load more normal samples for testing\n",
    "    num_test_normal_samples = min(200, len(normal_image_uris) - len(train_uris) - len(val_uris))\n",
    "    test_normal_uris = normal_image_uris[len(train_uris) + len(val_uris):len(train_uris) + len(val_uris) + num_test_normal_samples]\n",
    "\n",
    "    all_test_uris.extend(test_normal_uris)\n",
    "    all_test_labels.extend([0] * len(test_normal_uris))\n",
    "\n",
    "    # Add anomaly samples\n",
    "    num_test_anomaly_samples = min(200, len(anomaly_image_uris))\n",
    "    test_anomaly_uris_subset = random.sample(anomaly_image_uris, num_test_anomaly_samples) # Random subset\n",
    "    \n",
    "    all_test_uris.extend(test_anomaly_uris_subset)\n",
    "    all_test_labels.extend([1] * len(test_anomaly_uris_subset))\n",
    "\n",
    "    if not all_test_uris:\n",
    "        print(\"No test URIs available. Skipping AUC evaluation.\")\n",
    "    else:\n",
    "        test_ds_combined = tf.data.Dataset.from_tensor_slices(all_test_uris).map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        reconstruction_errors_test = []\n",
    "        for batch in test_ds_combined:\n",
    "            reconstructed_batch = custom_autoencoder(batch, training=False)\n",
    "            mse = tf.reduce_mean(tf.square(batch - reconstructed_batch), axis=(1, 2, 3))\n",
    "            reconstruction_errors_test.extend(mse.numpy())\n",
    "\n",
    "        if len(np.unique(all_test_labels)) > 1:\n",
    "            auc_score = roc_auc_score(all_test_labels, reconstruction_errors_test)\n",
    "            precision, recall, _ = precision_recall_curve(all_test_labels, reconstruction_errors_test)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            print(f\"Anomaly Detection AUC Score: {auc_score:.4f}\")\n",
    "            print(f\"Precision-Recall AUC Score: {pr_auc:.4f}\")\n",
    "\n",
    "            # Plot Precision-Recall Curve\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision-Recall Curve for Anomaly Detection')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Not enough unique classes (normal/anomaly) in test set to calculate AUC/PR-AUC.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Custom Training Logic as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5ffc2-a06c-4f69-9e97-b0479f25e0cd",
   "metadata": {},
   "source": [
    "#### Experiment Tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7aeb84-e03f-4cba-a57e-d052b42485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Tracking with Vertex AI Experiments\n",
    "\n",
    "print(\"--- Experiment Tracking with Vertex AI Experiments ---\")\n",
    "print(\"Logging model performance, parameters, and artifacts for reproducibility.\")\n",
    "\n",
    "if custom_autoencoder: # Assuming custom_autoencoder from previous cell\n",
    "    EXPERIMENT_NAME = \"Anomaly_Detection_Research\"\n",
    "    RUN_NAME = f\"Autoencoder_Run_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "    print(f\"\\nStarting Vertex AI Experiment '{EXPERIMENT_NAME}' with run '{RUN_NAME}'...\")\n",
    "\n",
    "    # Start an experiment run\n",
    "    aiplatform.start_run(experiment=EXPERIMENT_NAME, run=RUN_NAME)\n",
    "\n",
    "    try:\n",
    "        # --- 1. Log Parameters ---\n",
    "        aiplatform.log_params({\n",
    "            \"model_architecture\": \"CustomAutoencoder\",\n",
    "            \"image_size\": f\"{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}\",\n",
    "            \"epochs\": EPOCHS, # From custom training loop\n",
    "            \"learning_rate\": optimizer.learning_rate.numpy(),\n",
    "            \"loss_function\": \"MeanSquaredError\",\n",
    "            \"batch_size\": BATCH_SIZE\n",
    "        })\n",
    "        print(\"Logged experiment parameters.\")\n",
    "\n",
    "        # --- 2. Log Metrics (from previous custom training loop) ---\n",
    "        # Log final metrics\n",
    "        if 'auc_score' in locals() and 'pr_auc' in locals():\n",
    "            aiplatform.log_metrics({\n",
    "                \"final_auc\": auc_score,\n",
    "                \"final_pr_auc\": pr_auc\n",
    "            })\n",
    "            print(f\"Logged final metrics: AUC={auc_score:.4f}, PR_AUC={pr_auc:.4f}\")\n",
    "        else:\n",
    "            print(\"No final AUC/PR_AUC to log (might be due to insufficient test data).\")\n",
    "\n",
    "        # Log training curve metrics per epoch\n",
    "        if train_losses and val_losses:\n",
    "            for i, (train_l, val_l) in enumerate(zip(train_losses, val_losses)):\n",
    "                aiplatform.log_metrics(\n",
    "                    metrics={\n",
    "                        \"train_loss\": train_l,\n",
    "                        \"val_loss\": val_l\n",
    "                    },\n",
    "                    step=i + 1\n",
    "                )\n",
    "            print(f\"Logged per-epoch training and validation loss for {len(train_losses)} epochs.\")\n",
    "        else:\n",
    "            print(\"No training curve losses to log.\")\n",
    "\n",
    "        # --- 3. Save and Log Model Checkpoints (Optional) ---\n",
    "        # Save the trained model to GCS\n",
    "        MODEL_GCS_PATH = f'gs://{GCS_BUCKET_NAME}/models/{RUN_NAME}/'\n",
    "        custom_autoencoder.save(MODEL_GCS_PATH)\n",
    "        print(f\"\\nModel saved to GCS: {MODEL_GCS_PATH}\")\n",
    "\n",
    "        # Log the saved model as an artifact in Vertex AI Experiments\n",
    "        aiplatform.log_artifacts(\n",
    "            artifacts=[\n",
    "                aiplatform.Artifact.create(\n",
    "                    schema_title='google.VertexModel', # Or a custom schema for your saved model\n",
    "                    uri=MODEL_GCS_PATH,\n",
    "                    display_name='Anomaly_Detection_Autoencoder',\n",
    "                    metadata={'model_type': 'autoencoder', 'framework': 'tensorflow', 'gcs_path': MODEL_GCS_PATH}\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print(\"Logged model artifact.\")\n",
    "\n",
    "        # --- 4. End the Run ---\n",
    "        aiplatform.end_run()\n",
    "        print(f\"\\nExperiment run '{RUN_NAME}' ended.\")\n",
    "        print(f\"View this run in Vertex AI Experiments: https://console.cloud.google.com/vertex-ai/experiments/experiments/{EXPERIMENT_NAME}/runs/{RUN_NAME}/details/metrics?project={PROJECT_ID}&region={REGION}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiment tracking: {e}\")\n",
    "        aiplatform.end_run(status=\"ERROR\") # End with error status if something went wrong\n",
    "        print(\"Experiment run ended with an error status.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Experiment Tracking as no custom autoencoder model was trained.\")\n",
    "\n",
    "print(\"\\nVertex AI Experiments provides a centralized place to manage, compare, and analyze your ML experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02d7ed-162f-43ba-bebf-fb12b3380a7a",
   "metadata": {},
   "source": [
    "### **Model Evaluation & Analysis:**\n",
    "\n",
    "- **Quantitative Evaluation:** Develop Python scripts in notebooks to compute relevant metrics (e.g., F1-score, precision, recall, AUC, IoU for segmented anomalies) and compare against baselines.\n",
    "- **Qualitative Analysis:** Visually inspect model predictions on anomalous and normal images, identify failure modes, and debug model behavior using visualization tools (e.g., Grad-CAM for saliency maps) within the notebook.\n",
    "- **Adversarial Testing:** Explore how robust the anomaly detection model is to minor perturbations or adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14f13f-4511-4bd6-81f6-e85c0b98d840",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading and Exploring Datasets:\n",
    "\n",
    "- Data Access: Connect to Cloud Storage buckets (e.g., for raw image data, processed features) and BigQuery tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d975b1d-00b4-4891-bb5d-c4d36df8a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Load Model/Data for Evaluation\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, models, optimizers\n",
    "\n",
    "# Scikit-learn for evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Google Cloud SDKs\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part, Image as GImage\n",
    "\n",
    "# Set up GCP project and region (ensure these match your environment)\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n",
    "REGION = 'us-central1' # Or your desired region\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    try:\n",
    "        _, project_id = aiplatform.initializer.global_config.get_client_options()\n",
    "        PROJECT_ID = project_id\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"YOUR_GCP_PROJECT_ID\" # REPLACE WITH YOUR PROJECT ID\n",
    "\n",
    "print(f\"Using Google Cloud Project: {PROJECT_ID}\")\n",
    "print(f\"Using Google Cloud Region: {REGION}\")\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "gcs_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# --- Configuration for your dataset ---\n",
    "GCS_NORMAL_DATA_PREFIX = 'raw_images/normal/' # REPLACE with your actual normal data path\n",
    "GCS_ANOMALY_DATA_PREFIX = 'raw_images/anomaly/' # REPLACE with your actual anomaly data path\n",
    "GCS_BUCKET_NAME = 'your-image-dataset-bucket' # REPLACE with your actual GCS bucket name\n",
    "\n",
    "IMAGE_SIZE = (128, 128) # Standardize image size for models\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Helper function to load and preprocess images for TensorFlow ---\n",
    "def load_and_preprocess_image(gcs_uri, label=None, image_size=IMAGE_SIZE):\n",
    "    img_bytes = tf.io.read_file(gcs_uri)\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3) # Adjust for PNG/BMP as needed\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = img / 255.0 # Normalize to [0, 1]\n",
    "    if label is not None:\n",
    "        return img, label\n",
    "    return img\n",
    "\n",
    "# --- Load all normal and anomaly image URIs for testing ---\n",
    "normal_image_uris = []\n",
    "anomaly_image_uris = []\n",
    "try:\n",
    "    bucket = gcs_client.get_bucket(GCS_BUCKET_NAME)\n",
    "    normal_blobs = list(bucket.list_blobs(prefix=GCS_NORMAL_DATA_PREFIX))\n",
    "    normal_image_uris = [f\"gs://{b.bucket.name}/{b.name}\" for b in normal_blobs if b.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Found {len(normal_image_uris)} 'normal' images.\")\n",
    "\n",
    "    anomaly_blobs = list(bucket.list_blobs(prefix=GCS_ANOMALY_DATA_PREFIX))\n",
    "    anomaly_image_uris = [f\"gs://{b.bucket.name}/{b.name}\" for b in anomaly_blobs if b.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Found {len(anomaly_image_uris)} 'anomaly' images.\")\n",
    "\n",
    "    # Create combined test dataset with labels\n",
    "    # Use a reasonable number of samples for evaluation\n",
    "    num_test_normal = min(500, len(normal_image_uris))\n",
    "    num_test_anomaly = min(500, len(anomaly_image_uris))\n",
    "\n",
    "    test_normal_uris = random.sample(normal_image_uris, num_test_normal)\n",
    "    test_anomaly_uris = random.sample(anomaly_image_uris, num_test_anomaly)\n",
    "\n",
    "    all_test_uris = test_normal_uris + test_anomaly_uris\n",
    "    all_test_labels = [0] * len(test_normal_uris) + [1] * len(test_anomaly_uris) # 0 for normal, 1 for anomaly\n",
    "\n",
    "    # Shuffle combined data to ensure randomness\n",
    "    combined_list = list(zip(all_test_uris, all_test_labels))\n",
    "    random.shuffle(combined_list)\n",
    "    shuffled_test_uris, shuffled_test_labels = zip(*combined_list)\n",
    "\n",
    "    test_ds_labeled = tf.data.Dataset.from_tensor_slices((list(shuffled_test_uris), list(shuffled_test_labels))).\\\n",
    "                                      map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).\\\n",
    "                                      batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Prepared combined test dataset with {len(shuffled_test_uris)} samples.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading initial image data: {e}\")\n",
    "    print(\"Ensure GCS_BUCKET_NAME, GCS_NORMAL_DATA_PREFIX, and GCS_ANOMALY_DATA_PREFIX are correct.\")\n",
    "    test_ds_labeled = None\n",
    "\n",
    "# --- Load the trained model ---\n",
    "# If you saved your model in the previous step, load it here.\n",
    "# Otherwise, you might need to re-run the training cell or load a pre-trained model.\n",
    "try:\n",
    "    # Assuming the custom_autoencoder was defined and trained in Cell 5 of the previous section\n",
    "    # Re-build the model structure to load weights if necessary\n",
    "    def build_custom_autoencoder(input_shape, filter_base=32):\n",
    "        encoder_inputs = keras.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(filter_base, (3, 3), activation='relu', padding='same')(encoder_inputs)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(filter_base * 2, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(filter_base * 4, (3, 3), activation='relu', padding='same')(x)\n",
    "        latent = layers.MaxPooling2D((2, 2), padding='same')(x) \n",
    "\n",
    "        decoder_inputs = keras.Input(shape=latent.shape[1:])\n",
    "        x = layers.Conv2DTranspose(filter_base * 4, (3, 3), activation='relu', padding='same')(decoder_inputs)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        x = layers.Conv2DTranspose(filter_base * 2, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        x = layers.Conv2DTranspose(filter_base, (3, 3), activation='relu', padding='same')(x)\n",
    "        decoder_outputs = layers.UpSampling2D((2, 2))(x)\n",
    "        decoder_outputs = layers.Conv2D(input_shape[-1], (3, 3), activation='sigmoid', padding='same')(decoder_outputs)\n",
    "        \n",
    "        autoencoder = keras.Model(encoder_inputs, decoder_outputs, name=\"custom_autoencoder\")\n",
    "        return autoencoder\n",
    "\n",
    "    model = build_custom_autoencoder(IMAGE_SIZE + (3,))\n",
    "    # Attempt to load weights if a saved model path exists\n",
    "    # If you saved your model to a specific GCS path, load from there:\n",
    "    # SAVED_MODEL_GCS_PATH = 'gs://your-image-dataset-bucket/models/YourSpecificRunName/'\n",
    "    # model = tf.keras.models.load_model(SAVED_MODEL_GCS_PATH)\n",
    "    # For now, we'll assume a fresh model or one trained in the previous cell's execution.\n",
    "    # If the custom_autoencoder object from the previous cell is still in memory, use it directly:\n",
    "    if 'custom_autoencoder' in locals():\n",
    "        model = custom_autoencoder\n",
    "        print(\"Using the 'custom_autoencoder' object from the previous cell.\")\n",
    "    else:\n",
    "        print(\"Warning: 'custom_autoencoder' not found. Model might not be loaded. Please ensure you ran previous training cells or load a saved model.\")\n",
    "        model = None # Set to None to prevent errors in subsequent cells\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "print(\"Setup and data/model loading complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c5d26-a699-470d-a1e6-7fec99434dc4",
   "metadata": {},
   "source": [
    "#### Quantitative Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642d234-266f-416c-bea2-67a374cd389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluation\n",
    "\n",
    "print(\"--- Quantitative Evaluation ---\")\n",
    "\n",
    "if model and test_ds_labeled:\n",
    "    # 1. Get predictions (reconstruction errors) and true labels\n",
    "    true_labels = []\n",
    "    reconstruction_errors = []\n",
    "    original_images_for_viz = [] # Store a few for qualitative analysis\n",
    "\n",
    "    print(\"Generating predictions and computing reconstruction errors...\")\n",
    "    for i, (images, labels) in enumerate(test_ds_labeled):\n",
    "        reconstructed_images = model.predict(images)\n",
    "        mse_batch = tf.reduce_mean(tf.square(images - reconstructed_images), axis=(1, 2, 3))\n",
    "        \n",
    "        reconstruction_errors.extend(mse_batch.numpy())\n",
    "        true_labels.extend(labels.numpy())\n",
    "\n",
    "        if i < 2: # Store a couple of batches for qualitative analysis later\n",
    "            original_images_for_viz.append((images.numpy(), labels.numpy(), reconstructed_images.numpy(), mse_batch.numpy()))\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    reconstruction_errors = np.array(reconstruction_errors)\n",
    "\n",
    "    if len(np.unique(true_labels)) < 2:\n",
    "        print(\"Not enough unique classes (normal/anomaly) in the test set to compute classification metrics. Please ensure your test set contains both types.\")\n",
    "    else:\n",
    "        # 2. Calculate AUC-ROC\n",
    "        # For anomaly detection, reconstruction error is the 'score', higher score = more anomalous\n",
    "        auc_roc = roc_auc_score(true_labels, reconstruction_errors)\n",
    "        print(f\"\\nArea Under the Receiver Operating Characteristic (AUC-ROC): {auc_roc:.4f}\")\n",
    "\n",
    "        # 3. Calculate Precision-Recall Curve and AUC-PR\n",
    "        precision, recall, thresholds = precision_recall_curve(true_labels, reconstruction_errors)\n",
    "        auc_pr = auc(recall, precision)\n",
    "        print(f\"Area Under the Precision-Recall Curve (AUC-PR): {auc_pr:.4f}\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = tf.keras.metrics.Roc(thresholds=np.linspace(0, 1, 100)).update_state(true_labels, reconstruction_errors).result()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot Precision-Recall curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(recall, precision, label=f'Precision-Recall Curve (AUC = {auc_pr:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 4. Determine optimal threshold and F1-score\n",
    "        # A common approach is to find the threshold that maximizes F1-score\n",
    "        f1_scores = []\n",
    "        for thresh in thresholds:\n",
    "            # Predictions: 1 if error > thresh, 0 otherwise\n",
    "            y_pred = (reconstruction_errors >= thresh).astype(int)\n",
    "            f1 = f1_score(true_labels, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        optimal_threshold_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "        optimal_f1_score = f1_scores[optimal_threshold_idx]\n",
    "\n",
    "        print(f\"\\nOptimal Threshold (maximizing F1-score): {optimal_threshold:.4f}\")\n",
    "        print(f\"F1-Score at Optimal Threshold: {optimal_f1_score:.4f}\")\n",
    "\n",
    "        # Confusion Matrix at optimal threshold\n",
    "        y_pred_optimal = (reconstruction_errors >= optimal_threshold).astype(int)\n",
    "        cm = confusion_matrix(true_labels, y_pred_optimal)\n",
    "        print(\"\\nConfusion Matrix at Optimal Threshold:\")\n",
    "        print(cm)\n",
    "        print(f\"  True Negatives (TN): {cm[0, 0]}\") # Correctly identified normal\n",
    "        print(f\"  False Positives (FP): {cm[0, 1]}\") # Normal incorrectly identified as anomaly\n",
    "        print(f\"  False Negatives (FN): {cm[1, 0]}\") # Anomaly incorrectly identified as normal\n",
    "        print(f\"  True Positives (TP): {cm[1, 1]}\") # Correctly identified anomaly\n",
    "\n",
    "        # Optional: IoU for Segmented Anomalies\n",
    "        # This metric is relevant if your anomaly detection model outputs a pixel-level mask\n",
    "        # indicating the anomalous regions.\n",
    "        # For a simple autoencoder, the \"anomaly\" is typically defined by high reconstruction error per pixel.\n",
    "        # To calculate IoU, you would need:\n",
    "        # 1. Ground truth anomaly masks (pixel-level annotations for anomalies).\n",
    "        # 2. A method to convert your model's output (e.g., pixel-wise reconstruction error)\n",
    "        #    into a binary segmentation mask (e.g., thresholding pixel errors).\n",
    "\n",
    "        # Example Placeholder for IoU calculation (conceptual)\n",
    "        # if your model was a U-Net that predicts an anomaly mask:\n",
    "        # def calculate_iou(y_true_mask, y_pred_mask):\n",
    "        #     intersection = np.logical_and(y_true_mask, y_pred_mask).sum()\n",
    "        #     union = np.logical_or(y_true_mask, y_pred_mask).sum()\n",
    "        #     if union == 0: return 1.0 # No pixels in either mask, perfect match\n",
    "        #     return intersection / union\n",
    "\n",
    "        # if anomaly_segmentation_available:\n",
    "        #     iou_scores = []\n",
    "        #     for original, true_mask, predicted_mask_logits in test_ds_segmentation:\n",
    "        #         predicted_mask = (tf.nn.sigmoid(predicted_mask_logits) > 0.5).numpy().astype(int)\n",
    "        #         iou_scores.append(calculate_iou(true_mask, predicted_mask))\n",
    "        #     mean_iou = np.mean(iou_scores)\n",
    "        #     print(f\"\\nMean Intersection over Union (IoU) for segmented anomalies: {mean_iou:.4f}\")\n",
    "        # else:\n",
    "        print(\"\\nIoU calculation is not applicable for a simple autoencoder anomaly detection setup (which outputs image-level reconstruction error). It applies to pixel-level anomaly segmentation.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping quantitative evaluation as model or test data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d7dc5-e771-4136-bd9e-c0b855853d37",
   "metadata": {},
   "source": [
    "#### Quantitative Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5069e6f-636f-453a-bfe0-02d740e56fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Analysis\n",
    "\n",
    "print(\"--- Qualitative Analysis ---\")\n",
    "\n",
    "if model and 'original_images_for_viz' in locals() and original_images_for_viz:\n",
    "    print(\"\\nVisualizing Original, Reconstructed, and Error Maps for Sample Images:\")\n",
    "\n",
    "    for batch_num, (originals, labels, reconstructions, errors) in enumerate(original_images_for_viz):\n",
    "        print(f\"\\n--- Batch {batch_num + 1} ---\")\n",
    "        num_display = min(5, len(originals)) # Display up to 5 images from this batch\n",
    "\n",
    "        plt.figure(figsize=(num_display * 3, 9))\n",
    "        for i in range(num_display):\n",
    "            is_anomaly = labels[i] == 1\n",
    "            title_prefix = \"Anomaly\" if is_anomaly else \"Normal\"\n",
    "\n",
    "            # Original Image\n",
    "            ax = plt.subplot(3, num_display, i + 1)\n",
    "            plt.imshow(originals[i])\n",
    "            plt.title(f\"{title_prefix}\\nOriginal (Error: {errors[i]:.4f})\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Reconstructed Image\n",
    "            ax = plt.subplot(3, num_display, i + 1 + num_display)\n",
    "            plt.imshow(reconstructions[i])\n",
    "            plt.title(f\"{title_prefix}\\nReconstructed\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Error Map (Absolute Difference)\n",
    "            ax = plt.subplot(3, num_display, i + 1 + 2 * num_display)\n",
    "            error_map = np.abs(originals[i] - reconstructions[i])\n",
    "            plt.imshow(np.mean(error_map, axis=-1), cmap='hot') # Mean across channels\n",
    "            plt.title(f\"{title_prefix}\\nError Map\")\n",
    "            plt.colorbar(ax=ax, fraction=0.046, pad=0.04) # Add colorbar\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nObserve: Anomalies should generally have higher reconstruction errors and more 'hot' regions in their error maps.\")\n",
    "    print(\"Failure modes might include: normal images with high error (false positives), or anomalies with low error (false negatives).\")\n",
    "\n",
    "\n",
    "    # --- Grad-CAM for Saliency Maps ---\n",
    "    # Grad-CAM helps understand which parts of the input image contributed most to the model's decision\n",
    "    # (in this case, reconstruction error). For an autoencoder, we're interested in regions\n",
    "    # that lead to high reconstruction loss.\n",
    "\n",
    "    print(\"\\n--- Grad-CAM for Saliency Maps on Autoencoder Errors ---\")\n",
    "    print(\"Note: Grad-CAM is typically applied to classification models by targeting a specific class logit.\")\n",
    "    print(\"For autoencoders, we can adapt it to highlight regions contributing to high reconstruction error.\")\n",
    "\n",
    "    def make_gradcam_heatmap(img_array, model, last_conv_layer_name=\"max_pooling2d_1\", target_layer=None):\n",
    "        \"\"\"\n",
    "        Generates a Grad-CAM heatmap for an autoencoder based on reconstruction error.\n",
    "        \n",
    "        Args:\n",
    "            img_array (tf.Tensor): The input image tensor.\n",
    "            model (tf.keras.Model): The autoencoder model.\n",
    "            last_conv_layer_name (str): Name of the last convolutional layer in the encoder part.\n",
    "                                        Adjust this based on your model's summary.\n",
    "        Returns:\n",
    "            np.ndarray: The heatmap.\n",
    "        \"\"\"\n",
    "        if target_layer is None:\n",
    "            # Try to infer the last conv layer if not provided\n",
    "            try:\n",
    "                for layer in reversed(model.layers):\n",
    "                    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Conv2DTranspose):\n",
    "                        last_conv_layer_name = layer.name\n",
    "                        print(f\"Using inferred last convolutional layer: {last_conv_layer_name}\")\n",
    "                        break\n",
    "                if not last_conv_layer_name:\n",
    "                    raise ValueError(\"Could not find a convolutional layer in the model.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not infer last conv layer: {e}. Please manually specify 'last_conv_layer_name'.\")\n",
    "                return None\n",
    "\n",
    "        # Create a model that outputs the feature maps from the last conv layer\n",
    "        # and the reconstructed output\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # The input image is float32 and needs to be watched by the tape for gradients\n",
    "            inputs = tf.cast(img_array, tf.float32)\n",
    "            tape.watch(inputs)\n",
    "\n",
    "            conv_outputs, predictions = grad_model(inputs)\n",
    "            \n",
    "            # The \"loss\" to compute gradients for is the reconstruction error\n",
    "            # We want to know what features cause high error\n",
    "            # For simplicity, we can take the mean reconstruction error over the image\n",
    "            # Or, for more localized maps, sum errors per pixel\n",
    "            loss = tf.reduce_mean(tf.square(inputs - predictions))\n",
    "            # Or, if you want a more class-discriminative map, and if you had anomaly scores\n",
    "            # loss = autoencoder_model.get_anomaly_score(inputs) # Requires a custom method for this.\n",
    "            # For a simple autoencoder, loss is reconstruction error.\n",
    "\n",
    "        # Get gradients of the loss with respect to the last conv layer's feature maps\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "        # Global average pooling of the gradients\n",
    "        # This gives a \"weight\" for each feature map channel\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "        # Multiply each channel's feature map by its importance weight\n",
    "        conv_outputs = conv_outputs[0] # Remove batch dimension\n",
    "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap) # Remove last dimension of 1\n",
    "\n",
    "        # ReLU on heatmap to only consider positive contributions\n",
    "        heatmap = tf.maximum(heatmap, 0)\n",
    "        \n",
    "        # Normalize heatmap to [0, 1] for visualization\n",
    "        max_val = tf.reduce_max(heatmap)\n",
    "        if max_val == 0:\n",
    "            return np.zeros_like(heatmap.numpy()) # Avoid division by zero\n",
    "        heatmap /= max_val\n",
    "        return heatmap.numpy()\n",
    "\n",
    "    # Select a few images (normal and anomaly) for Grad-CAM\n",
    "    grad_cam_sample_uris = []\n",
    "    if normal_image_uris:\n",
    "        grad_cam_sample_uris.extend(random.sample(normal_image_uris, min(2, len(normal_image_uris))))\n",
    "    if anomaly_image_uris:\n",
    "        grad_cam_sample_uris.extend(random.sample(anomaly_image_uris, min(2, len(anomaly_image_uris))))\n",
    "\n",
    "    if grad_cam_sample_uris and model:\n",
    "        for i, uri in enumerate(grad_cam_sample_uris):\n",
    "            print(f\"\\nProcessing image for Grad-CAM: {uri.split('/')[-1]}\")\n",
    "            original_image = load_and_preprocess_image(uri)\n",
    "            \n",
    "            # Get the input layer's name dynamically if available or assume 'input_1'\n",
    "            input_layer_name = model.input_names[0] if model.input_names else 'input_1'\n",
    "            \n",
    "            # Try to find a suitable last convolutional layer\n",
    "            last_conv_layer_to_use = None\n",
    "            for layer in reversed(model.layers):\n",
    "                if isinstance(layer, layers.Conv2D) and 'encoder' in layer.name: # Focus on encoder part\n",
    "                    last_conv_layer_to_use = layer.name\n",
    "                    break\n",
    "            if last_conv_layer_to_use is None:\n",
    "                print(\"Could not find a suitable last convolutional layer in the encoder. Please check your model architecture.\")\n",
    "                continue\n",
    "\n",
    "            heatmap = make_gradcam_heatmap(\n",
    "                tf.expand_dims(original_image, axis=0),\n",
    "                model,\n",
    "                last_conv_layer_name=last_conv_layer_to_use # Adjust this based on your model summary\n",
    "            )\n",
    "\n",
    "            if heatmap is not None:\n",
    "                # Resize heatmap to original image size\n",
    "                heatmap = np.uint8(255 * heatmap)\n",
    "                jet_colors = plt.cm.jet(np.arange(256))[:, :3]\n",
    "                jet_heatmap = jet_colors[heatmap]\n",
    "                jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "                jet_heatmap = jet_heatmap.resize((original_image.shape[1], original_image.shape[0]))\n",
    "                jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "                # Superimpose the heatmap on the original image\n",
    "                superimposed_img = jet_heatmap * 0.4 + original_image * 255.0 # Alpha blending\n",
    "                superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(original_image)\n",
    "                plt.title(\"Original Image\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(Image.fromarray(heatmap)) # Raw heatmap\n",
    "                plt.title(\"Grad-CAM Heatmap\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(superimposed_img)\n",
    "                plt.title(\"Superimposed Heatmap\")\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Skipping Grad-CAM for {uri} due to layer issue.\")\n",
    "    else:\n",
    "        print(\"Skipping Grad-CAM as no sample images or model are available.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping qualitative analysis as model or test data for visualization is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0820bf-4507-4423-a9a6-f465c28bcfb5",
   "metadata": {},
   "source": [
    "#### Adverarial Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c85c5e-b862-4fc2-8319-49af87f0bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Testing\n",
    "\n",
    "print(\"--- Adversarial Testing ---\")\n",
    "print(\"Exploring model robustness against minor perturbations/adversarial examples.\")\n",
    "\n",
    "if model and anomaly_image_uris:\n",
    "    # Adversarial attacks aim to make an anomaly look normal, or vice versa.\n",
    "    # For an autoencoder-based anomaly detector, an attack would try to reduce\n",
    "    # the reconstruction error of an anomalous image.\n",
    "\n",
    "    # --- 1. Select an example anomaly image ---\n",
    "    sample_anomaly_uri = random.choice(anomaly_image_uris)\n",
    "    original_anomaly_image = load_and_preprocess_image(sample_anomaly_uri)\n",
    "    original_anomaly_image_tensor = tf.expand_dims(original_anomaly_image, axis=0) # Add batch dim\n",
    "\n",
    "    # Calculate initial reconstruction error for the anomaly\n",
    "    initial_reconstruction = model.predict(original_anomaly_image_tensor)\n",
    "    initial_error = tf.reduce_mean(tf.square(original_anomaly_image_tensor - initial_reconstruction)).numpy()\n",
    "    print(f\"\\nOriginal Anomaly: {sample_anomaly_uri.split('/')[-1]}\")\n",
    "    print(f\"Initial Reconstruction Error: {initial_error:.6f}\")\n",
    "\n",
    "    # Display original anomaly image\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_anomaly_image.numpy())\n",
    "    plt.title(f\"Original Anomaly\\nError: {initial_error:.4f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(initial_reconstruction[0])\n",
    "    plt.title(\"Reconstructed Anomaly\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    error_map = np.abs(original_anomaly_image.numpy() - initial_reconstruction[0])\n",
    "    plt.imshow(np.mean(error_map, axis=-1), cmap='hot')\n",
    "    plt.title(\"Initial Error Map\")\n",
    "    plt.colorbar(ax=plt.gca(), fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. Implement a simple adversarial attack (FGSM - Fast Gradient Sign Method) ---\n",
    "    # Goal: Find a small perturbation that reduces the reconstruction error of an anomaly.\n",
    "    # This involves taking gradients of the error w.r.t. the input image.\n",
    "\n",
    "    loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def create_adversarial_example(model, input_image, epsilon):\n",
    "        # input_image must be a tensor and differentiable\n",
    "        input_image_tensor = tf.cast(input_image, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_image_tensor)\n",
    "            reconstructed_image = model(input_image_tensor)\n",
    "            # The loss here is what we want to minimize (reconstruction error)\n",
    "            loss = loss_object(input_image_tensor, reconstructed_image)\n",
    "\n",
    "        # Get the gradients of the loss with respect to the input image\n",
    "        gradient = tape.gradient(loss, input_image_tensor)\n",
    "        \n",
    "        # Take the sign of the gradients to create the perturbation\n",
    "        signed_grad = tf.sign(gradient)\n",
    "        \n",
    "        # Create the adversarial example by moving in the *negative* direction of the gradient\n",
    "        # to reduce the loss. Epsilon controls the magnitude of the perturbation.\n",
    "        adversarial_example = input_image_tensor - epsilon * signed_grad\n",
    "        \n",
    "        # Clip the perturbed image to be within valid pixel range [0, 1]\n",
    "        adversarial_example = tf.clip_by_value(adversarial_example, 0, 1)\n",
    "        \n",
    "        return adversarial_example\n",
    "\n",
    "    EPSILON = 0.05 # Magnitude of perturbation (adjust this!)\n",
    "\n",
    "    print(f\"\\nAttempting to generate adversarial example with epsilon = {EPSILON}...\")\n",
    "    \n",
    "    # Generate the adversarial example\n",
    "    adversarial_anomaly_image_tensor = create_adversarial_example(\n",
    "        model, \n",
    "        original_anomaly_image_tensor, \n",
    "        epsilon=EPSILON\n",
    "    )\n",
    "\n",
    "    # Calculate reconstruction error for the adversarial example\n",
    "    adversarial_reconstruction = model.predict(adversarial_anomaly_image_tensor)\n",
    "    adversarial_error = tf.reduce_mean(tf.square(adversarial_anomaly_image_tensor - adversarial_reconstruction)).numpy()\n",
    "\n",
    "    print(f\"Adversarial Reconstruction Error: {adversarial_error:.6f}\")\n",
    "    print(f\"Error Change: {initial_error - adversarial_error:.6f}\")\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(original_anomaly_image.numpy())\n",
    "    plt.title(f\"Original Anomaly\\nError: {initial_error:.4f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(adversarial_anomaly_image_tensor[0].numpy())\n",
    "    plt.title(f\"Adversarial Anomaly\\nError: {adversarial_error:.4f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(initial_reconstruction[0].numpy())\n",
    "    plt.title(\"Original Anomaly Rec.\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(adversarial_reconstruction[0].numpy())\n",
    "    plt.title(\"Adversarial Anomaly Rec.\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the perturbation itself\n",
    "    perturbation = adversarial_anomaly_image_tensor[0].numpy() - original_anomaly_image.numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow((perturbation + 1) / 2) # Normalize to [0,1] for display\n",
    "    plt.title(f\"Perturbation (Epsilon={EPSILON})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nObservation:\")\n",
    "    print(\"If the attack was successful, the 'Adversarial Anomaly' image should look very similar to the 'Original Anomaly',\")\n",
    "    print(\"but its reconstruction error should be significantly lower, potentially below your anomaly detection threshold.\")\n",
    "    print(\"This indicates a vulnerability where a slightly perturbed anomalous input could be misclassified as normal.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Adversarial Testing as model or anomaly data is not available.\")\n",
    "\n",
    "print(\"\\nModel evaluation and analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4fdf9-8cef-4726-a249-bb408d252c87",
   "metadata": {},
   "source": [
    "### **Containerization & Reproducibility for Model Training/Serving:**\n",
    "\n",
    "- **Dockerfile Creation:** Write Dockerfiles to package their custom TensorFlow training code and model serving logic into reproducible containers (templates/custom_training_job.py, templates/custom_prediction_routine.py).\n",
    "- **Local Docker Testing:** Build and test Docker images locally within the Workbench terminal before pushing to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718c654-87ae-4c69-a2ae-e962d16eb818",
   "metadata": {},
   "source": [
    "#### Dockerfile Creation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07202414-8e30-4a56-bbdc-2e6b224c2bd8",
   "metadata": {},
   "source": [
    "Containerization is a fundamental MLOps practice that packages your machine learning code, dependencies, and environment into isolated, portable units called containers. This ensures that your model trains and serves consistently across different environments, from local development to production on Google Cloud Platform (GCP) with Vertex AI.\n",
    "\n",
    "In these cells, we will:\n",
    "1.  **Create Dockerfiles:** Define the build process for custom TensorFlow training and prediction routines.\n",
    "2.  **Build Docker Images:** Transform your Dockerfiles into runnable images.\n",
    "3.  **Test Locally:** Verify the container's functionality within the Vertex Workbench environment before pushing to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ba5d2-f7c5-4749-b5a1-98d2875dc35d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Your Model Code and Dependencies\n",
    "\n",
    "Before we create the Dockerfiles, ensure your TensorFlow training and prediction code is organized and ready. We'll assume you have two Python scripts:\n",
    "- `templates/custom_training_job.py`: Contains your TensorFlow model training logic.\n",
    "- `templates/custom_prediction_routine.py`: Contains your custom model serving logic, including how to load your trained model and make predictions.\n",
    "\n",
    "**Directory Structure (Example):**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af926198-6f1d-486a-bc94-3de5728df2a9",
   "metadata": {},
   "source": [
    "your_project/\n",
    "├── notebooks/\n",
    "│   └── mlops_notebook.ipynb\n",
    "├── templates/\n",
    "│   ├── custom_training_job.py\n",
    "│   └── custom_prediction_routine.py\n",
    "├── data/\n",
    "│   └── (your dataset files)\n",
    "└── Dockerfiles/\n",
    "├── Dockerfile.train\n",
    "└── Dockerfile.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a3b30-53ea-4732-9135-d97446b36351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create dummy directories and files for demonstration if they don't exist\n",
    "os.makedirs('templates', exist_ok=True)\n",
    "os.makedirs('Dockerfiles', exist_ok=True)\n",
    "\n",
    "# Dummy training script\n",
    "training_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"Running custom training job...\")\n",
    "\n",
    "# Simulate model training\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a dummy model directory for saving\n",
    "model_dir = os.environ.get('AIP_MODEL_DIR', 'model_output')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Simulate saving the model\n",
    "model_save_path = os.path.join(model_dir, 'my_model')\n",
    "tf.saved_model.save(model, model_save_path)\n",
    "print(f\"Dummy model saved to: {model_save_path}\")\n",
    "\n",
    "print(\"Custom training job finished.\")\n",
    "\"\"\"\n",
    "\n",
    "with open('templates/custom_training_job.py', 'w') as f:\n",
    "    f.write(training_code)\n",
    "\n",
    "# Dummy prediction script\n",
    "prediction_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "class CustomPredictionRoutine(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        self._model = tf.saved_model.load(model_path)\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "    def predict(self, instances):\n",
    "        if self._model is None:\n",
    "            raise RuntimeError(\"Model not loaded. Call load() first.\")\n",
    "        \n",
    "        # Assume instances are already preprocessed numpy arrays\n",
    "        predictions = self._model(tf.constant(instances, dtype=tf.float32)).numpy().tolist()\n",
    "        return predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # For local testing purposes\n",
    "    # Set dummy AIP_MODEL_DIR to simulate Vertex AI environment\n",
    "    os.environ['AIP_MODEL_DIR'] = 'model_output/my_model' \n",
    "    \n",
    "    # Create dummy model artifacts for local testing\n",
    "    if not os.path.exists('model_output/my_model'):\n",
    "        print(\"Creating dummy model for local prediction routine testing...\")\n",
    "        dummy_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        dummy_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        tf.saved_model.save(dummy_model, 'model_output/my_model')\n",
    "        print(\"Dummy model created.\")\n",
    "\n",
    "    routine = CustomPredictionRoutine()\n",
    "    routine.load(os.environ['AIP_MODEL_DIR'])\n",
    "    \n",
    "    # Example prediction\n",
    "    dummy_input = np.random.rand(1, 784).tolist()\n",
    "    predictions = routine.predict(dummy_input)\n",
    "    print(\"Local prediction routine test completed. Sample prediction:\")\n",
    "    print(predictions[0][:5]) # Print first 5 prediction values\n",
    "\"\"\"\n",
    "\n",
    "with open('templates/custom_prediction_routine.py', 'w') as f:\n",
    "    f.write(prediction_code)\n",
    "\n",
    "print(\"Dummy `templates/custom_training_job.py` and `templates/custom_prediction_routine.py` created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b9344-87ff-488f-bcd1-44547e79ff4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dockerfile Creation\n",
    "\n",
    "We will create two separate Dockerfiles: one for the training job and one for the prediction routine. This separation adheres to best practices for MLOps, allowing independent scaling and deployment of each component.\n",
    "\n",
    "For base images, it's highly recommended to use Google Cloud's [Deep Learning Containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container). These images are pre-installed with popular ML frameworks (TensorFlow, PyTorch) and optimized for GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fa1c2-f969-477b-ac25-c586169941d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE VERSION\n",
    "# Base-cu124 CUDA 12.4 (Python 3.10) CUDA 12.4 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124.py310\n",
    "\n",
    "# TENSORFLOW VERSION\n",
    "# 2.17 (Python 3.10) 2.17.0 CPU only us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-17.py310\n",
    "# 2.17 (Python 3.10) 2.17.0 CUDA 12.3 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cu123.2-17.py310\n",
    "\n",
    "# PYTORCH\n",
    "# 2.4 (Python 3.10) 2.4.0 CUDA 12.4 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-cu124.2-4.py310\n",
    "\n",
    "# TEXT GENERATION INFERENCE CONTAINERS \n",
    "# TGI 2.4 2.4.0 CUDA 12.4 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\n",
    "\n",
    "# TEXT EMBEDDINGS INFERENCE CONTAINERS \n",
    "# TEI 1.5 1.5.1 CUDA 12.2 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-embeddings-inference-cu122.1-5.ubuntu2204\n",
    "# TEI 1.5 1.5.1 CPU us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-embeddings-inference-cpu.1-5\n",
    "\n",
    "# PYTORCH INFERENCE CONTAINERS\n",
    "# PyTorch 2.3 2.3.1 CUDA 12.1 4.46.1 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cu121.2-3.transformers.4-46.ubuntu2204.py311\n",
    "# PyTorch 2.3 2.3.1 CPU 4.46.1 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cpu.2-3.transformers.4-46.ubuntu2204.py311\n",
    "\n",
    "# PYTORCH TRAINING CONTAINERS\n",
    "# PyTorch 2.3 2.3.0 CUDA 12.1 4.42.3 us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-training-cu121.2-3.transformers.4-42.ubuntu2204.py310\n",
    "\n",
    "# vLLM INFERENCE CONTAINERS\n",
    "# PyTorch 2.4 2.4.0 CUDA 12.1 us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-5.ubuntu2204.py310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04194f-6f65-429d-83e1-d2a27fecfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfiles/Dockerfile.train\n",
    "\n",
    "# Use a Deep Learning Container image with TensorFlow for training\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-13.cpu:latest\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Install any additional Python dependencies for training\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy your training code into the container\n",
    "COPY templates/custom_training_job.py /app/custom_training_job.py\n",
    "\n",
    "# Set the entrypoint for the training job\n",
    "# Vertex AI Training expects your entrypoint to run your training script\n",
    "ENTRYPOINT [\"python\", \"custom_training_job.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c2772-46a6-4494-ae9c-a286ed9f32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfiles/Dockerfile.predict\n",
    "# Use a Deep Learning Container image with TensorFlow for prediction\n",
    "# A CPU image is often sufficient for serving, but you can choose GPU if needed.\n",
    "FROM us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Install any additional Python dependencies for serving\n",
    "# (e.g., if custom_prediction_routine.py has specific libraries)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy your custom prediction routine code into the container\n",
    "COPY templates/custom_prediction_routine.py /app/custom_prediction_routine.py\n",
    "\n",
    "# Vertex AI Custom Prediction expects an HTTP server for prediction.\n",
    "# The base images for prediction typically come with the necessary server (e.g., FastAPI or Flask)\n",
    "# and automatically discover your custom_prediction_routine.py if it implements the required methods (load, predict).\n",
    "# Ensure your custom_prediction_routine.py adheres to the Vertex AI custom prediction routine interface.\n",
    "# For TensorFlow, the tf2-cpu image handles this automatically.\n",
    "# No explicit ENTRYPOINT needed if using the built-in Vertex AI prediction server logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e254fcc-3ce5-49ed-a172-13e2859b47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow~=2.13.0\n",
    "numpy\n",
    "# Add any other libraries your training or prediction code needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b68bb5-1b87-40c6-84ed-b4489192026f",
   "metadata": {},
   "source": [
    "#### Local Docker Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9ea09-d9a1-4dde-98d3-4faff9d80751",
   "metadata": {},
   "source": [
    "Before pushing your Docker images to Artifact Registry and deploying them on Vertex AI, it's crucial to test them locally within your Vertex Workbench Jupyter Notebook environment. This helps catch issues early, saving time and compute costs.\n",
    "\n",
    "You'll use the Workbench terminal for these commands.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Build the Docker Image:** Use `docker build` to create your container image.\n",
    "2.  **Run the Docker Container:** Use `docker run` to execute your container and test its functionality.\n",
    "\n",
    "**Important Note:** The Vertex AI Workbench environment has Docker pre-installed and configured. You can directly run `docker` commands in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb28e3d-567e-4dde-82d4-e67c4f153e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the directory containing your Dockerfiles\n",
    "cd Dockerfiles\n",
    "\n",
    "# Define an image name and tag\n",
    "export TRAIN_IMAGE_NAME=\"my-tf-trainer\"\n",
    "export TRAIN_IMAGE_TAG=\"v1\"\n",
    "\n",
    "# Build the training Docker image\n",
    "# The '.' indicates the build context, which is the current directory (Dockerfiles/)\n",
    "# Adjust the build context if your templates directory is not relative to Dockerfiles\n",
    "docker build -f Dockerfile.train -t ${TRAIN_IMAGE_NAME}:${TRAIN_IMAGE_TAG} ../.\n",
    "\n",
    "# Go back to the notebook's root directory for next steps\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf2f39-018c-409e-949d-f486fda6cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training container locally\n",
    "# You might want to mount a local volume to simulate model output or data input\n",
    "# For this simple example, we'll just run it to see the print statements.\n",
    "# In a real scenario, you'd want to pass arguments or mount data.\n",
    "docker run --rm ${TRAIN_IMAGE_NAME}:${TRAIN_IMAGE_TAG}\n",
    "\n",
    "# The --rm flag automatically removes the container after it exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1769c4-c620-46a2-9503-b7cb558553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the directory containing your Dockerfiles\n",
    "cd Dockerfiles\n",
    "\n",
    "# Define an image name and tag\n",
    "export PREDICT_IMAGE_NAME=\"my-tf-predictor\"\n",
    "export PREDICT_IMAGE_TAG=\"v1\"\n",
    "\n",
    "# Build the prediction Docker image\n",
    "docker build -f Dockerfile.predict -t ${PREDICT_IMAGE_NAME}:${PREDICT_IMAGE_TAG} ../.\n",
    "\n",
    "# Go back to the notebook's root directory for next steps\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f5b42-0a04-4954-9070-fa2d5021bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local testing of prediction routines, you typically need to:\n",
    "# 1. Have a saved model available locally.\n",
    "# 2. Simulate the input data.\n",
    "# 3. Call the prediction endpoint/method.\n",
    "\n",
    "# We set a dummy AIP_MODEL_DIR in custom_prediction_routine.py for local testing.\n",
    "# When running in Vertex AI, this environment variable is automatically provided.\n",
    "\n",
    "# Run the prediction container locally.\n",
    "# The entrypoint for prediction images is often managed by Vertex AI's serving runtime.\n",
    "# To test locally, you'd typically run the script directly or simulate the HTTP server.\n",
    "# Our dummy custom_prediction_routine.py has a `if __name__ == '__main__'` block for this.\n",
    "docker run --rm -v $(pwd)/model_output:/app/model_output ${PREDICT_IMAGE_NAME}:${PREDICT_IMAGE_TAG} python /app/custom_prediction_routine.py\n",
    "\n",
    "# Explanation:\n",
    "# - `docker run --rm`: Runs the container and removes it after exit.\n",
    "# - `-v $(pwd)/model_output:/app/model_output`: Mounts your local 'model_output' directory\n",
    "#   (where the dummy model was saved by the training script) into the container's `/app/model_output`\n",
    "#   location. This makes the saved model available inside the container for prediction.\n",
    "# - `${PREDICT_IMAGE_NAME}:${PREDICT_IMAGE_TAG}`: Specifies the image to run.\n",
    "# - `python /app/custom_prediction_routine.py`: Overrides the default entrypoint (if any)\n",
    "#   to directly execute your prediction routine script for local testing.\n",
    "#   In a real Vertex AI deployment, the serving runtime would handle calling the `load` and `predict`\n",
    "#   methods of your `CustomPredictionRoutine` class automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173d5d5-898b-4af8-9765-5ac94dcb4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pushing to Artifact Registry\n",
    "\n",
    "Once you've successfully tested your Docker images locally, the next step in the MLOps pipeline is to push them to Google Cloud Artifact Registry. This makes your images accessible for Vertex AI Training and Vertex AI Prediction.\n",
    "\n",
    "**Commands you would run in the Workbench Terminal:**\n",
    "\n",
    "```bash\n",
    "# Set your GCP Project ID and region\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION=\"us-central1\" # Or your desired region\n",
    "\n",
    "# Define your Artifact Registry repository (create if it doesn't exist)\n",
    "export AR_REPO=\"vertex-ai-containers\"\n",
    "gcloud artifacts repositories create <span class=\"math-inline\">\\{AR\\_REPO\\} \\-\\-repository\\-format\\=docker \\-\\-location\\=</span>{REGION} --description=\"Docker images for Vertex AI\" --async --quiet\n",
    "\n",
    "# Authenticate Docker to Artifact Registry\n",
    "gcloud auth configure-docker ${REGION}-docker.pkg.dev\n",
    "\n",
    "# Tag the local images with the Artifact Registry path\n",
    "docker tag <span class=\"math-inline\">\\{TRAIN\\_IMAGE\\_NAME\\}\\:</span>{TRAIN_IMAGE_TAG} <span class=\"math-inline\">\\{REGION\\}\\-docker\\.pkg\\.dev/</span>{PROJECT_ID}/<span class=\"math-inline\">\\{AR\\_REPO\\}/</span>{TRAIN_IMAGE_NAME}:${TRAIN_IMAGE_TAG}\n",
    "docker tag <span class=\"math-inline\">\\{PREDICT\\_IMAGE\\_NAME\\}\\:</span>{PREDICT_IMAGE_TAG} <span class=\"math-inline\">\\{REGION\\}\\-docker\\.pkg\\.dev/</span>{PROJECT_ID}/<span class=\"math-inline\">\\{AR\\_REPO\\}/</span>{PREDICT_IMAGE_NAME}:${PREDICT_IMAGE_TAG}\n",
    "\n",
    "# Push the images to Artifact Registry\n",
    "docker push <span class=\"math-inline\">\\{REGION\\}\\-docker\\.pkg\\.dev/</span>{PROJECT_ID}/<span class=\"math-inline\">\\{AR\\_REPO\\}/</span>{TRAIN_IMAGE_NAME}:${TRAIN_IMAGE_TAG}\n",
    "docker push <span class=\"math-inline\">\\{REGION\\}\\-docker\\.pkg\\.dev/</span>{PROJECT_ID}/<span class=\"math-inline\">\\{AR\\_REPO\\}/</span>{PREDICT_IMAGE_NAME}:<span class=\"math-inline\">\\{PREDICT\\_IMAGE\\_TAG\\}\n",
    "echo \"Images pushed to Artifact Registry\\:\"\n",
    "echo \"</span>{REGION}-docker.pkg.dev/<span class=\"math-inline\">\\{PROJECT\\_ID\\}/</span>{AR_REPO}/<span class=\"math-inline\">\\{TRAIN\\_IMAGE\\_NAME\\}\\:</span>{TRAIN_IMAGE_TAG}\"\n",
    "echo \"<span class=\"math-inline\">\\{REGION\\}\\-docker\\.pkg\\.dev/</span>{PROJECT_ID}/<span class=\"math-inline\">\\{AR\\_REPO\\}/</span>{PREDICT_IMAGE_NAME}:${PREDICT_IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f98ecb-13ec-4fea-b31f-962122eb6f45",
   "metadata": {},
   "source": [
    "**NOTE:** This structured notebook will provide a clear and actionable guide for AI researchers, enabling them to leverage containerization effectively for their MLOps workflows on GCP. Remember to emphasize the importance of local testing to quickly iterate and debug their container images before deploying to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74c0a0-5a1f-4b16-95b7-2b6f4eda6eab",
   "metadata": {},
   "source": [
    "### **Basic Deployment & Frontend Integration:**\n",
    "\n",
    "- **Model Registration:** Register promising model versions in the Vertex AI Model Registry.\n",
    "- **Endpoint Deployment (Interactive):** Deploy models to Vertex AI Endpoints directly from a notebook for quick testing of the prediction routine.\n",
    "- **React Frontend Development:** Develop and iterate on the React UI for anomaly visualization and interaction, sending requests to the deployed Vertex AI Endpoint. This involves basic web development within the Workbench or a local setup.\n",
    "- **Inference Testing:** Perform test inferences from the notebook and the React frontend to ensure correct data flow and prediction outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f8a6e-1312-40ab-9cb8-fa092ef6980d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model Registration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4fc23-e16c-4183-bdc4-da3cf98d8d13",
   "metadata": {},
   "source": [
    "**Model Registration in Vertex AI Model Registry**\n",
    "\n",
    "The Vertex AI Model Registry is a centralized repository to manage the lifecycle of your ML models, including versioning, metadata, and lineage. After a successful training run (local or on Vertex AI), you can register your model.\n",
    "\n",
    "For this example, we'll assume you have a TensorFlow `SavedModel` artifact. If your training job saved the model to a GCS bucket, you'd point to that GCS path. For local testing, we'll simulate a GCS path to your `model_output` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc5035-daaf-4406-964e-5edcad5db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'your-gcp-project-id') # Replace with your project ID\n",
    "REGION = 'us-central1' # Or your desired region\n",
    "MODEL_DISPLAY_NAME = 'my-anomaly-detector-model'\n",
    "MODEL_DESCRIPTION = 'Anomaly detection model trained with TensorFlow.'\n",
    "TRAINING_IMAGE_URI = f'{REGION}-docker.pkg.dev/{PROJECT_ID}/vertex-ai-containers/my-tf-trainer:v1'\n",
    "PREDICTION_IMAGE_URI = f'{REGION}-docker.pkg.dev/{PROJECT_ID}/vertex-ai-containers/my-tf-predictor:v1'\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Vertex AI SDK initialized for Project: {PROJECT_ID}, Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2686ce-a58a-4ec7-9f93-aefd0862ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Assume `model_output/my_model` was generated by your training script (or copied here)\n",
    "# For local testing, we'll upload this to a GCS bucket.\n",
    "# In a real Vertex AI Training job, the model would automatically be saved to GCS.\n",
    "\n",
    "BUCKET_NAME = f'{PROJECT_ID}-vertex-ai-models'\n",
    "MODEL_SOURCE_DIR = 'model_output/my_model' # Local path to your saved model\n",
    "\n",
    "# Create a GCS bucket if it doesn't exist\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "if not bucket.exists():\n",
    "    print(f\"Creating GCS bucket: gs://{BUCKET_NAME}\")\n",
    "    bucket.create(location=REGION)\n",
    "else:\n",
    "    print(f\"GCS bucket gs://{BUCKET_NAME} already exists.\")\n",
    "\n",
    "# Upload the SavedModel directory to GCS\n",
    "GCS_MODEL_PATH = f'gs://{BUCKET_NAME}/models/{MODEL_DISPLAY_NAME}/'\n",
    "print(f\"Uploading model artifacts from {MODEL_SOURCE_DIR} to {GCS_MODEL_PATH}\")\n",
    "\n",
    "def upload_directory_to_gcs(local_dir, bucket_name, gcs_path_prefix):\n",
    "    \"\"\"Uploads a local directory recursively to GCS.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            # Construct the GCS blob name relative to the gcs_path_prefix\n",
    "            relative_path = os.path.relpath(local_file_path, local_dir)\n",
    "            blob_name = os.path.join(gcs_path_prefix, relative_path)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_filename(local_file_path)\n",
    "            print(f\"Uploaded {local_file_path} to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "upload_directory_to_gcs(MODEL_SOURCE_DIR, BUCKET_NAME, f'models/{MODEL_DISPLAY_NAME}/')\n",
    "\n",
    "# Register the model in Vertex AI Model Registry\n",
    "print(\"\\nRegistering model in Vertex AI Model Registry...\")\n",
    "registered_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=GCS_MODEL_PATH,\n",
    "    serving_container_image_uri=PREDICTION_IMAGE_URI,\n",
    "    description=MODEL_DESCRIPTION,\n",
    "    # Add optional parameters for custom prediction routine if needed\n",
    "    # serving_container_predict_route='/predict',\n",
    "    # serving_container_health_route='/health',\n",
    "    # serving_container_ports=[8080],\n",
    "    # serving_container_environment_variables={'MODEL_NAME': 'my_model'},\n",
    "    # serving_container_explanation_metadata=explain_metadata,\n",
    "    # serving_container_args=[]\n",
    ")\n",
    "\n",
    "print(f\"Model registered. Model ID: {registered_model.name}\")\n",
    "print(f\"Model Version ID: {registered_model.version_id}\")\n",
    "print(f\"Model Resource Name: {registered_model.resource_name}\")\n",
    "\n",
    "# Store the model object for later use\n",
    "model_obj = registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14a6b9-8b4e-433f-b175-20a09d432ae0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Endpoint Deployment (interactive):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6e10b-b762-411f-8226-49bc2e329e78",
   "metadata": {},
   "source": [
    "Deploying a model to a Vertex AI Endpoint makes it available for online predictions via a REST API. This section will guide you through interactively deploying your registered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0083d4-20f9-4198-98c0-36815ba80d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Endpoint\n",
    "print(\"Creating Vertex AI Endpoint...\")\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_DISPLAY_NAME}-endpoint\",\n",
    "    description=\"Endpoint for anomaly detection model.\"\n",
    ")\n",
    "print(f\"Endpoint created. Endpoint ID: {endpoint.name}\")\n",
    "print(f\"Endpoint Resource Name: {endpoint.resource_name}\")\n",
    "\n",
    "# Deploy the model to the Endpoint\n",
    "print(f\"\\nDeploying model '{model_obj.display_name}' to Endpoint '{endpoint.display_name}'...\")\n",
    "endpoint.deploy(\n",
    "    model=model_obj,\n",
    "    deployed_model_display_name=f\"{MODEL_DISPLAY_NAME}-deployed\",\n",
    "    machine_type=\"n1-standard-2\", # Choose an appropriate machine type\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=True # Wait for deployment to complete\n",
    ")\n",
    "\n",
    "print(f\"Model '{model_obj.display_name}' deployed to Endpoint '{endpoint.display_name}' successfully!\")\n",
    "print(f\"Endpoint Public DNS: {endpoint.public_endpoint_dns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99b631-a8c2-4d2d-b570-ae95eab9d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the deployed model ID\n",
    "deployed_model_id = endpoint.list_models()[0].id\n",
    "print(f\"Deployed Model ID: {deployed_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f02a4-1a80-4cc9-8446-23d9e9fb6bba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### React Frontend Development:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45284fa4-7f21-4d7c-a2b0-8a50e2355999",
   "metadata": {},
   "source": [
    "Developing a React frontend directly within the Vertex Workbench Jupyter environment is **not ideal for iterative development**. Jupyter Notebooks are best for data science and ML prototyping, not full-stack web development.\n",
    "\n",
    "**Recommended Approach for React Frontend:**\n",
    "\n",
    "1.  **Local Development:** Develop the React app on your local machine using tools like `create-react-app`.\n",
    "2.  **Separate Repository:** Maintain the frontend code in a separate Git repository.\n",
    "3.  **Deployment Options:**\n",
    "    * **Cloud Run:** For a simple, scalable serverless deployment of your React app.\n",
    "    * **Cloud Hosting (Firebase Hosting, GCS + Load Balancer):** For static site hosting.\n",
    "\n",
    "**However, for the purpose of demonstrating the integration *conceptually* within this notebook, we'll provide the core JavaScript logic that would interact with your Vertex AI Endpoint.**\n",
    "\n",
    "**Conceptual Steps for React App (Not runnable in notebook):**\n",
    "\n",
    "1.  **Project Setup:**\n",
    "    ```bash\n",
    "    npx create-react-app anomaly-detector-frontend\n",
    "    cd anomaly-detector-frontend\n",
    "    npm start\n",
    "    ```\n",
    "2.  **Modify `src/App.js`:**\n",
    "    * Import `useState`, `useEffect`.\n",
    "    * Create input fields for data.\n",
    "    * Implement an API call function to the Vertex AI Endpoint.\n",
    "    * Display prediction results.\n",
    "\n",
    "**Key Frontend Interaction Logic (JavaScript - For your `App.js` or similar):**\n",
    "```javascript\n",
    "import React, { useState } from 'react';\n",
    "\n",
    "function App() {\n",
    "  const [inputData, setInputData] = useState(Array(784).fill(0.5)); // Example: 784 features, initialized with 0.5\n",
    "  const [prediction, setPrediction] = useState(null);\n",
    "  const [loading, setLoading] = useState(false);\n",
    "  const [error, setError] = useState(null);\n",
    "\n",
    "  // REPLACE WITH YOUR ACTUAL VERTEX AI ENDPOINT DNS\n",
    "  const VERTEX_AI_ENDPOINT_DNS = \"YOUR_VERTEX_AI_ENDPOINT_DNS_HERE\"; \n",
    "  const PROJECT_ID = \"YOUR_GCP_PROJECT_ID\";\n",
    "  const ENDPOINT_ID = \"YOUR_VERTEX_AI_ENDPOINT_ID\"; // E.g., '1234567890123456789'\n",
    "\n",
    "  // Function to get an access token for authentication\n",
    "  // In a real frontend, you'd use a service account key or Firebase Authentication\n",
    "  // to get a short-lived token server-side or via a secure client-side flow.\n",
    "  // For simplicity here, we'll assume a way to get a token (e.g., from a backend proxy)\n",
    "  async function getAccessToken() {\n",
    "    // This is a placeholder. In production, you'd use a more secure method.\n",
    "    // E.g., a backend server that fetches a token from Google Metadata Service\n",
    "    // or uses a service account key.\n",
    "    // For local dev, you might use 'gcloud auth print-access-token' in a proxy.\n",
    "    try {\n",
    "        const response = await fetch('/api/get-access-token'); // Assuming a local proxy server\n",
    "        const data = await response.json();\n",
    "        return data.accessToken;\n",
    "    } catch (err) {\n",
    "        console.error(\"Error fetching access token:\", err);\n",
    "        setError(\"Failed to get authentication token.\");\n",
    "        return null;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  const handlePredict = async () => {\n",
    "    setLoading(true);\n",
    "    setError(null);\n",
    "    setPrediction(null);\n",
    "\n",
    "    try {\n",
    "      const accessToken = await getAccessToken();\n",
    "      if (!accessToken) {\n",
    "        return;\n",
    "      }\n",
    "\n",
    "      const response = await fetch(\n",
    "        `https://${VERTEX_AI_ENDPOINT_DNS}/v1/projects/${PROJECT_ID}/locations/${REGION}/endpoints/${ENDPOINT_ID}:predict`,\n",
    "        {\n",
    "          method: 'POST',\n",
    "          headers: {\n",
    "            'Authorization': `Bearer ${accessToken}`,\n",
    "            'Content-Type': 'application/json',\n",
    "          },\n",
    "          body: JSON.stringify({\n",
    "            instances: [inputData], // Send as an array of instances\n",
    "          }),\n",
    "        }\n",
    "      );\n",
    "\n",
    "      if (!response.ok) {\n",
    "        const errorText = await response.text();\n",
    "        throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);\n",
    "      }\n",
    "\n",
    "      const data = await response.json();\n",
    "      setPrediction(data.predictions[0]); // Assuming your model returns a list of predictions\n",
    "    } catch (e) {\n",
    "      console.error(\"Prediction error:\", e);\n",
    "      setError(`Failed to get prediction: ${e.message}`);\n",
    "    } finally {\n",
    "      setLoading(false);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  const handleInputChange = (index, value) => {\n",
    "    const newInputData = [...inputData];\n",
    "    newInputData[index] = parseFloat(value);\n",
    "    setInputData(newInputData);\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div style={{ padding: '20px', fontFamily: 'Arial, sans-serif' }}>\n",
    "      <h1>Anomaly Detector</h1>\n",
    "      <p>Enter data points (e.g., 784 values for an image, or fewer for tabular data):</p>\n",
    "      {/* Example: A simple input for a few values, or a more complex component for many */}\n",
    "      <div style={{ marginBottom: '20px' }}>\n",
    "        {inputData.slice(0, 10).map((value, index) => ( // Display first 10 for simplicity\n",
    "          <input\n",
    "            key={index}\n",
    "            type=\"number\"\n",
    "            value={value}\n",
    "            onChange={(e) => handleInputChange(index, e.target.value)}\n",
    "            style={{ width: '50px', marginRight: '5px' }}\n",
    "          />\n",
    "        ))}\n",
    "        {inputData.length > 10 && <span>...and {inputData.length - 10} more values.</span>}\n",
    "      </div>\n",
    "\n",
    "      <button onClick={handlePredict} disabled={loading}>\n",
    "        {loading ? 'Predicting...' : 'Get Anomaly Prediction'}\n",
    "      </button>\n",
    "\n",
    "      {error && <p style={{ color: 'red' }}>Error: {error}</p>}\n",
    "\n",
    "      {prediction !== null && (\n",
    "        <div style={{ marginTop: '20px', border: '1px solid #ccc', padding: '15px' }}>\n",
    "          <h2>Prediction Result:</h2>\n",
    "          <pre>{JSON.stringify(prediction, null, 2)}</pre>\n",
    "          {/* Interpret prediction here, e.g., if prediction[0] > threshold, it's an anomaly */}\n",
    "          {prediction[0] > 0.5 ? \n",
    "            <p style={{ color: 'red', fontWeight: 'bold' }}>Likely Anomaly Detected!</p> :\n",
    "            <p style={{ color: 'green', fontWeight: 'bold' }}>Normal behavior.</p>\n",
    "          }\n",
    "        </div>\n",
    "      )}\n",
    "      \n",
    "      <h3>Important Security Note for Frontend Authentication:</h3>\n",
    "      <p>Directly exposing `gcloud auth print-access-token` results or service account keys in a client-side React app is **highly insecure**. For production, use a secure backend proxy or server-side authentication (e.g., using Firebase Authentication or a custom Node.js/Python backend that fetches tokens on behalf of the client).</p>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96615a-18b5-4bf2-a6bd-33f37c1e91a7",
   "metadata": {},
   "source": [
    "#### Inference Testing: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c1301-5817-42f3-98a7-21336f37d895",
   "metadata": {},
   "source": [
    "Now that the model is deployed, let's perform some test inferences from both the notebook and conceptually from the React frontend to ensure everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4597951-a916-4cdc-9c59-d0cb2ea1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Generate some dummy input data (e.g., a single instance)\n",
    "# Ensure the shape and data type match what your model expects\n",
    "# For a 784-feature input, like flattened MNIST images:\n",
    "dummy_input_instance = np.random.rand(784).tolist() # Simulate one data point\n",
    "test_instances = [dummy_input_instance]\n",
    "\n",
    "print(\"Performing prediction from notebook...\")\n",
    "try:\n",
    "    # Use the deployed_model object directly for prediction if you have the endpoint object\n",
    "    # Or, if starting fresh, use aiplatform.Endpoint(endpoint_resource_name)\n",
    "    \n",
    "    predictions = endpoint.predict(instances=test_instances)\n",
    "    \n",
    "    print(\"\\nPrediction Response:\")\n",
    "    print(json.dumps(predictions.predictions, indent=2))\n",
    "    \n",
    "    # You can add logic here to interpret the prediction\n",
    "    # For example, if it's a binary classification for anomaly:\n",
    "    if predictions.predictions and len(predictions.predictions[0]) > 0:\n",
    "        if predictions.predictions[0][0] > 0.5: # Assuming output is probability of anomaly\n",
    "            print(\"Model predicted: Likely Anomaly\")\n",
    "        else:\n",
    "            print(\"Model predicted: Normal\")\n",
    "    else:\n",
    "        print(\"Unexpected prediction format.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b9af4-0693-464f-a519-a9fd7bc7c732",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conceptual Inference Testing from React Frontend\n",
    "\n",
    "As described in Section 3, the `handlePredict` function in the `App.js` example demonstrates how the React frontend would send prediction requests to your Vertex AI Endpoint.\n",
    "\n",
    "**To actually test this:**\n",
    "1.  Set up your React application locally (outside of this Workbench notebook).\n",
    "2.  Replace the placeholder `YOUR_VERTEX_AI_ENDPOINT_DNS_HERE`, `YOUR_GCP_PROJECT_ID`, and `YOUR_VERTEX_AI_ENDPOINT_ID` with the actual values obtained from this notebook (e.g., from Cell 18).\n",
    "3.  Implement a secure way to get an access token for authentication (e.g., a simple Node.js proxy server that uses `gcloud auth print-access-token` for local development, or a proper service account key in a backend for production).\n",
    "4.  Run your React application (`npm start`).\n",
    "5.  Interact with the UI, input data, and click the \"Get Anomaly Prediction\" button. Observe the network requests and the displayed prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb1d46-5701-4062-b6d0-4623de5960f8",
   "metadata": {},
   "source": [
    "## Clean Up (Optional but Recommended)\n",
    "\n",
    "To avoid incurring unnecessary costs, it's good practice to undeploy models and delete endpoints when they are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ae9ae-a06f-449b-b9b5-29c2aeaae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will undeploy your model and delete the endpoint.\n",
    "# Only run this cell when you are finished with testing.\n",
    "\n",
    "print(f\"Undeploying model from endpoint: {endpoint.display_name}...\")\n",
    "endpoint.undeploy_all()\n",
    "print(\"Model undeployed.\")\n",
    "\n",
    "print(f\"Deleting endpoint: {endpoint.display_name}...\")\n",
    "endpoint.delete()\n",
    "print(\"Endpoint deleted.\")\n",
    "\n",
    "# You can also delete the model from the registry if desired\n",
    "# print(f\"Deleting model from registry: {model_obj.display_name}...\")\n",
    "# model_obj.delete()\n",
    "# print(\"Model deleted from registry.\")\n",
    "\n",
    "# To delete the GCS bucket:\n",
    "# storage_client.delete_bucket(BUCKET_NAME)\n",
    "# print(f\"Deleted GCS bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95f81b-fdcd-4172-a9b8-c3ecf92c32a2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bca648-a5aa-49e3-b404-de5f5a227c23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
