{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini Best Practices: General Prompt Engineering\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **General Prompt Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "Prompt engineering is the art and science of crafting inputs to an LLM to elicit desired outputs. With Gemini's power, well-crafted prompts can unlock incredible potential.\n",
        "\n",
        "- **Be Clear, Specific, and Concise:**\n",
        "    - **State Your Intent:** Directly tell what you want it to do (`summarize`, `analyze`, `generate`, `classify`, `extract`, `explain`).\n",
        "    - **Provide Constraints:** Specify length (e.g., `in three bullet points`, `a 100-word summary`), format (e.g., `JSON`, `table`, `markdown`, `Python code`), tone (e.g., `professional`, `authentic` ,`friendly`, `technical`), and target audience.\n",
        "    - **Avoid Ambiguity:** Re-read your prompt to ensure it cannot be misinterpreted. If a concept is technical, define it.\n",
        "\n",
        "- **Provide Sufficient Context:**\n",
        "    - **Background Information:** Give all necessary information to understand the task, such as relevant data, previous turns in a conversation, or the specific problem you're trying to solve.\n",
        "    - `\"You are a...\"` **Role-Playing:** Assign a persona or role to Gemini (e.g., `\"You are an expert data scientist`,\" \"Act as a Python developer\"). This helps guide the model's tone and expertise.\n",
        "    - **\"Why\" Matters:** Explain the purpose of your request. Knowing why you need something (e.g., `\"I need to set up a secure Google Cloud site for hosting a blog\"` vs. `\"How to deploy a website?\"`) helps Gemini provide more relevant and detailed solutions.\n",
        "\n",
        "- **Use Examples (Few-Shot Prompting):**\n",
        "    - **Demonstrate Desired Output**: Providing a few input-output examples within your prompt (few-shot prompting) is incredibly effective for guiding Gemini, especially for:\n",
        "        - **Formatting:** Showing exactly how you want the response structured (e.g., specific JSON schema).\n",
        "        - **Style/Tone:** Demonstrating the writing style or tone you expect.\n",
        "        - **Complex Logic/Classification:** Illustrating how specific inputs should be categorized or processed.\n",
        "    - **Consistency:** Ensure your examples are consistent in format and quality.\n",
        "    - **Optimal Number:** Experiment. Sometimes one or two examples are enough; for more complex tasks, you might need several.\n",
        "\n",
        "- **Break Down Complex Tasks (Chain-of-Thought / Step-by-Step):**\n",
        "    - **Intermediate Steps:** For multi-step problems, ask Gemini to \"think step by step\" or explicitly guide it through the logical progression.\n",
        "    - **Chaining Prompts:** For very complex workflows, consider breaking them into a series of smaller prompts, where the output of one prompt becomes the input for the next. This is useful for agentic workflows.\n",
        "\n",
        "- **Iterate and Refine:**\n",
        "    - **Experiment:** Not every prompt will be perfect the first time. Modify your prompt based on Gemini's responses.\n",
        "    - **Add/Remove Details:** If a response is too vague, add more detail. If it's too long, specify a word limit. If it's off-topic, refine your constraints.\n",
        "    - **Parameter Tuning:** Experiment with model parameters like temperature (controls randomness; lower for more deterministic output, higher for creativity), top_p (nucleus sampling), and top_k (top-k sampling) to influence the output's diversity and quality.\n",
        "\n",
        "- **Specify Output Format (Especially for ML Tasks):**\n",
        "    - **Structured Output (JSON/Enums):** Gemini on Vertex AI supports generating structured outputs like JSON. This is critical for ML tasks where you need parseable data (e.g., entity extraction, intent classification).\n",
        "        - **Recommended:** Define a responseSchema directly in the API call. This is the most robust way to ensure structured output.\n",
        "        - **Alternative (less reliable):** Provide the schema in natural language within the prompt, but this doesn't guarantee adherence.\n",
        "    - **Other Formats:** Explicitly ask for bullet points, tables, code snippets (with language specified), XML, CSV, etc.\n",
        "\n",
        "- **Utilize Multimodal Capabilities (for Gemini Pro/1.5 Pro):**\n",
        "    - If your task involves images or video, include them directly in the prompt.\n",
        "    - **Image/Video First:** For single-image prompts, placing the image before the text prompt can sometimes improve performance. For interleaved content, use the most natural order.\n",
        "    - **Hinting:** If the model isn't drawing information from the relevant part of the image, explicitly hint at the aspects you want it to focus on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Managing Context in Gemini for ML Tasks\n",
        "\n",
        "Gemini 1.5 Pro's massive 1-million token context window (and even larger in preview) significantly simplifies context management compared to earlier LLMs. However, it's still a finite resource, and efficient use is key for performance, cost, and accuracy.\n",
        "\n",
        "- Why Context Management is Crucial:\n",
        "\n",
        "    - **Cost:** Input tokens are billed. Sending redundant information increases cost.\n",
        "    - **Performance/Latency:** Processing a larger context window takes more time.\n",
        "    - **\"Lost in the Middle\":** While larger context windows help, models can still sometimes lose focus on critical information buried deep within a very long prompt (\"needle in a haystack\" problem).\n",
        "    - **Relevance:** You want the model to focus on the most relevant information for the current query.\n",
        "\n",
        "- Strategies for Managing Context:\n",
        "\n",
        "    - Prioritize Relevant Information:\n",
        "        - **Curate Inputs:** Even with a large context, only include information that is genuinely relevant to the current task or conversation turn. Avoid dumping entire documents if only a small section is needed.\n",
        "        - **Summarize or Extract:** Before feeding long texts to Gemini, consider pre-summarizing irrelevant sections or extracting only the key facts/entities you need for the prompt.\n",
        "        - **Query at the End:** For long contexts, it's often a good practice to place your specific query or question at the end of the prompt, after all the contextual information. This helps the model prioritize.\n",
        "\n",
        "    - External Memory & Retrieval Augmented Generation (RAG):\n",
        "        - **Knowledge Bases:** For information that changes frequently or is too vast to fit in the context window, use an external knowledge base (e.g., Cloud SQL, BigQuery, Cloud Storage, a vector database like AlloyDB for PostgreSQL with vector embeddings).\n",
        "        - **Retrieval:** Use retrieval mechanisms (like vector search) to pull only the most relevant chunks of information from your external knowledge base based on the user's query. These retrieved chunks then augment the prompt sent to Gemini.\n",
        "        \n",
        "        - Benefits of RAG:\n",
        "            - **Up-to-date information:** Gemini uses the latest data from your knowledge base.\n",
        "            - **Reduced hallucinations:** Model is grounded in factual data.\n",
        "            - **Overcomes context limits:** Provides access to virtually unlimited information.\n",
        "            - **Reduced cost:** You're only sending relevant snippets to the LLM.\n",
        "\n",
        "    - Context Caching (Gemini 1.5 Pro Feature):\n",
        "        - This is a game-changer for cost and latency when working with the same large documents or conversations repeatedly.\n",
        "        - You can \"cache\" a long document or conversation into the model's context. Subsequent queries on that same cached context are significantly cheaper and faster because the model doesn't re-process the entire original input each time.\n",
        "        - **Use Cases:** Chatbots interacting with a specific document, code analysis where the codebase remains stable, long-form summarization of a static document.\n",
        "\n",
        "    - Iterative Prompting & Conversational State:\n",
        "        - **Session Management:** For long conversations, track the conversation history.\n",
        "        - **Summarization/Condensation:** When the conversation history approaches the context limit, dynamically summarize or condense earlier turns to fit. You can even use Gemini itself to summarize past turns into a concise representation of the conversation state.\n",
        "        - **Sliding Window:** Maintain a \"sliding window\" of the most recent and relevant turns, discarding older, less relevant ones.\n",
        "\n",
        "    - Leverage Gemini's Multi-Turn Capabilities:\n",
        "        - Gemini is designed for multi-turn conversations. Use the ChatSession objects in the API to automatically manage the conversation history for you (within the context window)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example for an ML Task: Generating a JSON Schema for Data Validation\n",
        "\n",
        "- **ML Task:** You need Gemini to generate a JSON schema that describes the structure of your customer data, including data types and descriptions, for use in a data validation pipeline (e.g., in Vertex AI Data Validation or for Pydantic models).\n",
        "\n",
        "Bad Prompt (Vague):\n",
        "\"Create a JSON schema for customer data.\"\n",
        "\n",
        "Better Prompt (Specific, Contextual, Format-Controlled):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "latex"
        }
      },
      "source": [
        "\"You are a meticulous data engineer responsible for defining data schemas.\n",
        "Your task is to create a JSON schema for a customer dataset.\n",
        "\n",
        "The customer data contains the following fields:\n",
        "- `customer_id`: Unique identifier for the customer (string).\n",
        "- `name`: Full name of the customer (string).\n",
        "- `email`: Customer's email address (string, must be a valid email format).\n",
        "- `age`: Customer's age (integer, must be between 18 and 99).\n",
        "- `registration_date`: Date the customer registered (string, must be in YYYY-MM-DD format).\n",
        "- `is_prime_member`: Boolean indicating if the customer is a prime member (boolean).\n",
        "- `last_purchase_amount`: The amount of their last purchase (float, optional).\n",
        "- `address`: An object containing:\n",
        "    - `street`: Street name (string).\n",
        "    - `city`: City (string).\n",
        "    - `zip_code`: Zip code (string, must be a 5-digit number).\n",
        "- `favorite_categories`: An array of strings, listing categories the customer frequently shops in. Each string must be one of: 'Electronics', 'Books', 'Clothing', 'Home Goods', 'Food'.\n",
        "\n",
        "Generate the JSON schema, ensuring all data types, format constraints, and required fields are correctly specified.\n",
        "Provide the output as a valid JSON object, without any additional explanatory text or markdown formatting.\n",
        "\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Managing Context for ML Tasks (Conceptual):\n",
        "\n",
        "Imagine you're building a feature store. You have a very long document describing various raw data sources, their nuances, and how certain features are derived.\n",
        "\n",
        "   - Initial Load (Cost/Latency):\n",
        "        When you first interact with this document, you pay the cost for Gemini to process its entire length.\n",
        "        response = model.generate_content(long_document_text + \"Based on this document, list all potential features related to customer engagement.\")\n",
        "\n",
        "   - Subsequent Queries with Context Caching (Cost/Latency Optimization):\n",
        "        If you're using Vertex AI and have context caching enabled for Gemini 1.5 Pro, you can \"cache\" this document.\n",
        "        cached_content = model.cache_content(long_document_text) (conceptual API)\n",
        "        Now, for subsequent queries related to the same document, you reference the cached content, significantly reducing token costs and latency:\n",
        "        response = model.generate_content(cached_content_id, \"From the cached document, how are 'daily active users' calculated?\")\n",
        "\n",
        "   - RAG for Dynamically Updated Information:\n",
        "        If your data source descriptions change frequently, or you have many such documents, you'd combine Gemini with RAG:\n",
        "        * **Step 1 (Offline/Pre-processing):** Chunk the long documents and create vector embeddings for each chunk. Store these in a vector database.\n",
        "        * **Step 2 (Runtime):** When a user asks a question, embed the user's query. Perform a vector search in your database to retrieve the top K most relevant chunks from your documents.\n",
        "        * **Step 3 (Prompt Augmentation):** Construct a prompt for Gemini: \"Context: [retrieved_chunk_1], [retrieved_chunk_2], ... User Query: [original_user_query]. Based on the provided context, answer the user's query.\"\n",
        "\n",
        "By combining precise prompt engineering with intelligent context management strategies, you can leverage Gemini's advanced capabilities effectively for a wide range of ML tasks, from data understanding and schema generation to complex reasoning and code assistance, while also keeping an eye on performance and cost."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
