{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Model Monitoring**\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "Model monitoring is a critical practice within the MLOps (Machine Learning Operations) lifecycle that focuses on continuously tracking the performance, behavior, and health of machine learning models after they have been deployed into a production environment \n",
        "\n",
        "Unlike the initial model evaluation during development (which happens on static dataset), model monitoring deals with the dynamic, unpredictable nature of real world data and user interactions.  Its essential because ML models, unlike traditional software, can degrade silently over time, leading to inaccurate predictions, negative business impacts, and even ethical concerns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Why is Model Monitoring So Important?**\n",
        "\n",
        "1. **Model Performance Degradation:** ML models are trained on historical data, but the real world is constantly changing.\n",
        "   - **Data Drift:** The statistical Properties of the input data change over time.  For example, user behavior patterns shift, economic conditions evolve, or sensor readings change due to environment.  If the model sees data significantly different from what it was trained on, its predictions will become less accurate.\n",
        "   - **Concept Drift:** The underlying relationship between the input features and the target variable changes.  For instance, what constituted \"fraudulent behavior\" a year ago might be different today due to new scam tactics.  The models learned rules become outdated.\n",
        "   - **Data Quality Issues:** Upstream data pipelines can break, leading to missing values, corrupted data, or schema changes that the model isn't prepared for.\n",
        "   - **Outliers and Anomalies:** Unexpected data points can throw off predictions or even cause the model to crash.\n",
        "\n",
        "2. **Silent Failures:** Unlike traditional software which might crash or throw obvious errors, an ML model can continue to run and produce predictions, but those predictions might be silently wrong or become less effective without any explicit error. This can lead to significant financial losses, poor customer experiences, or flawed decision making.\n",
        "\n",
        "3. **Business Impact:** ML models are deployed to achieve specific business goals (e.g., increase sales, reduce fraud, improve customer satisfaction). Monitoring ensures the model continues to contribute positively to these KPIs.\n",
        "\n",
        "4. **Resource Optimization:** Tracking resource consumption (CPU, GPU, memory, latency, throughput) of deployed models help optimize infrastructure costs and ensures the model scales efficiently under varying loads.\n",
        "\n",
        "5.  **Compliance and Ethics (Bias and Fairness):** In regulated industries or for models impacting sensitive decisions (e.g., loan applications, hiring), continuous monitoring is required to detect and mitigate biases that might emerge in production or ensure fairness across different demographic groups.\n",
        "\n",
        "6. **Trust and Explainability:** Monitoring can help reveal unexpected model behaviors or provide insights into why a models performance might be degrading, supporting debugging and building trust in AI systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **What to Monitor (Key Monitoring Signals):**\n",
        "\n",
        "1. Model Performance Metrics:\n",
        "   - **Direct Metrics:** If ground truth labels are available in a timely manner, you can calculate the actual performance metrics (e.g., accuracy, precision, recall, F1 score for classification, RMSE, MAE, R squared for regression). This is the gold standard.\n",
        "   - **Proxy Metrics (for delayed ground truth):** In many real world scenarios (e.g., fraud detection, loan default prediction), the true outcome is only known after a significant delay.  In these cases, you might monitor proxy metrics like:\n",
        "      - **Prediction Drift:** Changes in the distribution of the models outputs (e.g., if a fraud model suddenly predicts much less fraud).\n",
        "      - **Confidence Scores:** For classification models, monitoring the distribution of prediction confidence can indicate if the model is becoming less certain.\n",
        "\n",
        "2. Data Drift:\n",
        "   - **Input Feature Drift:** Monitor the statistical distributions of individual input features (e.g., mean, median, standard deviation, cardinality, missing value rates) and compare them to the training data or a defined baseline.\n",
        "   - **Covariate Shift:** More complex changes in the joint distribution of input features.\n",
        "   - **Feature Importance Drift:** For interpretable models, changes in which features are most influential for predictions can signal drift.\n",
        "\n",
        "3. Data Quality:\n",
        "   - **Completeness:** Rate of missing values.\n",
        "   - **Validty:** Data types, formats, and ranges (e.g., is age negative? Is a categorical variable outside its allowed values?).\n",
        "   - **Uniqueness:** Detection of unexpected duplicate entries.\n",
        "   - **Schema Changes:** Alerts if the incoming data schema deviates from the expected schema.\n",
        "\n",
        "4. System Health and Operational Metrics:\n",
        "   - **Latency:** Time taken to get a prediction.   \n",
        "   - **Throughput:** Number of request processed per second.\n",
        "   - **Error Rate:** Number of failed prediction  request.\n",
        "   - **Resources Utilization:** CPU, GPU, memory usage.\n",
        "   - **Uptime/Availability:** Is the model endpoint accessible and responding?\n",
        "\n",
        "5. Model Fairness and Bias:\n",
        "   - Monitoring performance metrics (e.g., accuracy, false positive rate, false negative rate) across different sensitive sub groups (e.g., gender, race, age) to detect emerging biases.\n",
        "\n",
        "6. Outlier and Anomaly Detection:\n",
        "   - Identify individual data points or predictions that are significantly different from the norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How Model Monitoring Works:**\n",
        "\n",
        "1. **Data Collection:** Capture incoming inference requests, the features used for prediction, the models output predictions, and ideally, the corresponding actual outcome (ground truth) when they become available.\n",
        "\n",
        "2. **Metric Calculation:** Compute the various monitoring signals (performance metrics, drift metrics, data quality checks) at regular intervals (e.g., hourly, daily, weekly).\n",
        "\n",
        "3. **Baseline Comparison:** Compare current metrics against a defined baseline (e.g., training data, a golden dataset, or historical production data from a period of good performance).\n",
        "\n",
        "4. **Thresholding and Alerting:** Define thresholds for each metric. If a metric crosses a threshold (e.g., accuracy drops below 90%, data drift exceeds a statistical significance level), an alert is triggered (email, Slack, PagerDuty).\n",
        "\n",
        "5. **Visualization and Dashboards:** Provide intuitive dashboards to visualize trends in performance, data characteristics, and system health over time.\n",
        "\n",
        "6. **Root Cause Analysis and Action:** When an alert fires, data scientists and ML engineers investigate the root cause, which often leads to:\n",
        "   - **Retraining:** The most common action for data/concept drift.\n",
        "   - **Feature Engineering:** Modifying or creating new features.\n",
        "   - **Data Pipeline Fixes:** Addressing upstream data quality issues.\n",
        "   - **Model Redeployment:** Deploying newly trained or optimized model.\n",
        "   - **Rollback:** Reverting to a previous, stable model version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Tools for Model Monitoring:**\n",
        "\n",
        "- **Cloud Native Solutions:** AWS SageMaker Model Monitor, Google Cloud Vertex AI Model Monitoring, Azure Machine Learning Managed Model Monitoring.\n",
        "- **Dedicated MLOps Platforms:** Databricks (with MLflow), Domino Data Lab, Seldon Core.\n",
        "- **Specialized Monitoring Tools:** Arize AI, Fiddler AI, WhyLabs, Evidently AI (open source), NannyML (Open source).\n",
        "- **General Observability Tools (with ML integration):** Prometheus, Grafana, Splunk, Datadog (often used for operational metrics, and increasingly integrate with ML monitoring)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model monitoring is not just about detecting problems, its about building a feedback loop that ensures the continuous relevance, accuracy, and reliability of machine learning models in production, thereby maximizing their business value over their entire lifecycle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
