{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hyperparameter Tuning**\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "Hyperparameter tuning, often simply called hyperparameter optimization, is the process of finding the optimal set of hyperparameters for a machine learning model to achieve the best possible performance on a given dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **What are Hyperparameters?**\n",
        "\n",
        "Before diving into tuning, its crucial to understand what hyperparameters are in contrast to model parameters:\n",
        "- **Model Parameters:** These are the internal variables or weights that a machine learning model learns from the training data. Examples include the coefficients in a linear regression model or the weights and biases in a neural network. These are learned during the training process.  \n",
        "- **Hyperparameters:** These are configuration settings that are external to the model and are not learned from the data. Instead, they are set before the training process begins and control how the model learns or its overall structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Example of Hyperparameters:**\n",
        "- Neural Networks:\n",
        "   - Learning rate\n",
        "   - Number of layers\n",
        "   - Number of neurons/units per layer\n",
        "   - Activation functions\n",
        "   - Optimizer (e.g., Adam, SGD, RMSprop)\n",
        "   - Batch size\n",
        "   - Number of epochs\n",
        "   - Dropout rate\n",
        "   - Regularization strength (L1, L2)\n",
        "\n",
        "- Support Vector Machines (SVMs):\n",
        "   - Kernel type (e.g., linear, RBF, polynomial)\n",
        "   - Regularization parameters (C)\n",
        "   - Gamma for RBF kernel\n",
        "\n",
        "- Decision Trees/Random Forests/Gradient Boosting:\n",
        "   - Max depth of trees:\n",
        "   - Number of estimators (trees) \n",
        "   - Minimum samples per leaf\n",
        "   - Learning rate for boosting\n",
        "   \n",
        "- K-Means Clustering:\n",
        "   - Number of clusters (k) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Why is Hyperparameter Tuning Important?**\n",
        "\n",
        "The choice of byperparameters can dramatically impact a models performance.\n",
        "- **Underfitting/Overfitting:** Incorrect hyperparameters can lead to a model that either underfits (too simple, can't capture patterns) or overfits (too complex, memorizes training data but performs poorly on new data).\n",
        "- **Convergence Speed:** The learning rate, for instance, dictates how quickly an optimization algorithm converges. Too high, and it might overshoot, too low and it might take too long to converge.\n",
        "- **Generalization:** Well-tuned hyperparameters help the model generalize better to unseen data.\n",
        "- **Resource Efficiency:** Optimal hyperparameters can lead to faster training times and more efficient use of computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How is Hyperparameter Tuning Done?**\n",
        "\n",
        "Hyperparameter tuning involves defining a search space for each hyperparameter and then using a strategy to explore that space to find the best combination. Evaluation is always done on a validation set (or via cross-validation) to avoid overfitting to the test set.\n",
        "\n",
        "Common strategies include:\n",
        "\n",
        "1. Manual Search (Trial and Error):\n",
        "   - **Description:** An experienced data scientist manually tweaks hyperparameters based on intuition, past experience, and observed model performance. \n",
        "   - **Pros:** Can be effective for simple models or when starting out, leverages human expertise.\n",
        "   - **Cons:** Time consuming, unscalable, prone to missing optimal combinations, subjective.\n",
        "2. Grid Search:\n",
        "   - **Description:** Defines a discrete set of values for each hyperparameter.  The algorithm then trains a model for every possible combination of these values.\n",
        "   - **Example:** if `learning_rate = [0.01, 0.001]` and `batch_size = [32, 64]`, Grid Search will test (0.01, 32), (0.01, 64), (0.001, 32), (0.001, 64).\n",
        "   - **Pros:** Simple to understand and implement, guarantees finding the best combination within the defined grid.\n",
        "   - **Cons:** Computationally expensive and time consuming as the number of hyperparameters and values increases (curse of dimensionality).  Not efficient if many hyperparameters are irrelevant or interact in complex ways.\n",
        "3. Random Search:\n",
        "   - **Description:** Instead of trying every combination, random search samples a fixed number of combinations from the defined search space. \n",
        "   - **Pros:** More efficient than Grid Search, especially for high dimensional search spaces, because its more likely to hit promising regions that Grid Search might miss if its grid is too coarse. Often finds good enough solutions much faster.\n",
        "   - **Cons:** Doesn't guarantee finding the absolute best combination, might miss the very best if the sampling budget is too small. \n",
        "4. Bayesian Optimization:\n",
        "   - **Description:** A more intelligent and efficient approach.  It builds a probabilistic model (surrogate model) of the objective function (e.g., validation accuracy) based on past evaluation results.  It then uses this model to intelligently choose the next set of hyperparameters to evaluate, aiming to maximize the objective function while minimizing the number of evaluations.  It balances exploration (trying new areas) and exploitation (refining promising areas). \n",
        "   - **Pros:** Significantly more efficient than Grid or Random Search, especially for expensive to evaluate models (e.g., deep neural networks), often finds better optima in fewer iterations.\n",
        "   - **Cons:** More complex to implement, can be slower initially due to overhead of building the probabilistic model, sequential in nature (can't easily parallelize all evaluations).\n",
        "   - **Popular Libraries:** Hyperopt, Optuna, Scikit-Optimize (skopt), Spearmint.\n",
        "5. Gradient-Based Optimization:\n",
        "   - **Description:** Treat hyperparameter tuning as an optimization problem where you want to minimize a validation loss function with respect to hyperparameters. This involves computing gradients of the validation loss with respect to hyperparameters (e.g., using implicit differentiation or meta learning approaches). \n",
        "   - **Pros:** Potentially very efficient, especially for models with many hyperparameters.\n",
        "   - **Cons:** Complex to implement, not applicable to all hyperparameters (e.g., discrete choices). More of a research area.\n",
        "6. Evolutionary Algorithms:\n",
        "   - **Description:** inspired by natural selection. A population of hyperparameter sets is randomly initialized.  The best performing sets are selected, mutated, and combined to create a new generation, iteratively searching for optimal solutions. \n",
        "   - **Pros:** Can explore complex, non convex search spaces effectively, highly parallelizable.\n",
        "   - **Cons:** Can be computationally intensive, might take many generations to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Tools for Hyperparameter Tuning:**\n",
        "\n",
        "Many ML frameworks and dedicated libraries offer built in or integrated hyperparameter tuning capabilities:\n",
        "- **scikit-learn:** `GridSearchCV`, `RandomizedSearchCV`\n",
        "- **Keras/TensorFlow:** `tf.keras.callbacks.TensorBoard` (for visualization during manual tuning) \n",
        "- **PyTorch Lighting:** Integrates with various loggers and tuners (e.g., Optuna, Weights & Biases)\n",
        "- Dedicated Libraries:\n",
        "   - **Hyperopt:** Bayesian Optimization\n",
        "   - **Optuna:** Flexible framework for hyperparameter optimization, supports various samplers.\n",
        "   - **Weights & Biases (W&B):** Powerful tool for hyperparameter optimization with excellent visualization and collaboration features.\n",
        "   - **Comet ML:** Similar to W&B, offers hyperparameter optimization along side experiment tracking. \n",
        "   - **Ray Tune:** Scalable hyperparameter tuning library for various ML frameworks.\n",
        "   - **Ax/BoTorch (Meta):** Flexible platform for adaptive experimentation and Bayesian optimization.\n",
        "- **Cloud ML Platforms:** AWS SagMaker, Google Cloud Vertex AI, Azure Machine Learning all offer managed hyperparameter tuning services that abstract away much of the infrastructure complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Best Practices for Hyperparameter Tuning:**\n",
        "\n",
        "- **Define a Search Space:** Clearly define the range or set of possible values for each hyperparameter.\n",
        "- **Start Broad, Then Narrow:** Begin with a wide search space using Random Search or a coarse Grid Search. Once promising regions are identified, narrow down the search space and use a finer-grained search (or Bayesian Optimization).\n",
        "- **Use a Validation Set/Cross Validation:** Never tune hyperparameters on the test set.  Always use a separate validation set or cross validation to get an unbiased estimate of performance during turning.\n",
        "- **Track Experiments:** Use an experiment tracking tool (like MLflow, W&B, Comet ML) to log eery hyperparameter configuration and its corresponding metrics.  This is crucial for reproducibility and comparison.\n",
        "- **Early Stopping:** Implement early stopping to halt training runs that are not performing well, saving computational resources.\n",
        "- **Parallelization:** Utilize parallel computing where possible to run multiple hyperparameter configurations simultaneously.\n",
        "- **Understand Your Model:** A good understanding of your models architecture and the role of different hyperparameters can guide your search and make it more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameter tuning often more art than science in its initial stages but with systematic approaches and the right tools, it becomes a scientific process that significantly contributes to building high performing and robust machine learning models.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
