{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Augmentation**\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "Data augmentation is powerful and widely used technique in machine learning especially in deep learning to artificially increase the size and diversity fo a training dataset.  It involves creating modification version of existing data samples rather than collecting new ones from scratch.  This process helps models learn more robust features, generalize better to unseen data, and combat common problems like overfitting and data scarcity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Why is Data Augmentation Important?**\n",
        "\n",
        "1. **Combating Data Scarcity:** Real world data collection and labeling can be incredibly expensive, time consuming or even impossible (e.g., rare medical conditions) Data augmentation allows you to stretch a smaller dataset to effectively train data hungry machine learning models, especially deep neural networks.\n",
        "\n",
        "2. **Preventing Overfitting** \n",
        "Overfitting occurs when a model learns the training data too well, including its noise and specific quirks, and fails to perform well on new, unseen data.  By providing varied versions of the same data, augmentation forces the model to learn more general and invariant features making it less likely to memorize specific training examples.  \n",
        "\n",
        "3. **Improving Generalization and Robustmess:**\n",
        "A diverse training set exposes the model to a wider range of variations it might encounter in the real world (e.g., different lighting angles, accents, phrasing).  This leads to models that are more robust and can generalize effectively to real world scenarios. \n",
        "\n",
        "4. **Addressing Class Imbalance:** In classification problems, one class might have significantly fewer samples than others (e.g. fraud detection, rare disease diagnosis).Data augmentation can be strategically applied to the minority class to increase its representation in the training data, helping the model learn from and correctly classify these underrepresented instances. \n",
        "\n",
        "5. **Cost Effectiveness:** Its much cheaper and faster to apply transformations to exsting data than to collect and annotate entirely new data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How does Data Augmentation work?**\n",
        "\n",
        "Data Augmentation typically involves applying a series of transformations to the original data while ensuring that the core meaning or label remains consistent. \n",
        "\n",
        "For example, if yo have an image of a cat and you rotate it, its still an image of a cat. If you change a word in a sentence to a synonym, the sentence's meaning usually remains the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Common Data Augmentation Techniques by Data Type**\n",
        "\n",
        "The specific augmentation techniques vary significantly depending on the type of data:\n",
        "\n",
        "1. **Image Data Augmentation (Computer Vision)**\n",
        "This is perhaps the most common application of data augmentation. Techniques include:\n",
        "\n",
        "**Geometric Transformations** \n",
        "- **Flipping:** Horizontal or vertical flips (e.g., flipping an image of a dog horizontally).\n",
        "- **Rotation:** Rotating the image by a certain degree (e.g., 5, 10, 15 degrees).\n",
        "- **Cropping:** Taking random crops of the image and resizing them to the original dimensions.\n",
        "- **Translation:** Shifting the image horizontally or vertically.\n",
        "- **Scaling/Zooming:** Zooming in or out the image. \n",
        "- **Shearing:** Tilting the image.\n",
        "\n",
        "**Color Space Transformations (Photometric Augmentations)**\n",
        "- **Brightness Adjustment:** Making the image brighter or darker.\n",
        "- **Contrast Adjustment:** Increasing or decreasing the contrast. \n",
        "- **Saturation Adjustment:** Changing the intensity of colors.\n",
        "- **Hue Adjustment:** Shifting the color tones. \n",
        "- **Grayscaling:** Converting the image to grayscale.\n",
        "\n",
        "Noise Injection: Adding random noise (e.g., Gaussian noise, salt and pepper noise) to the image to make the model more robust to noisy inputs.\n",
        "\n",
        "Random Erasing/Cutout:\n",
        "Randomly masking out a square region of the image with a constant color or random pixels.  This forces the model to learn more robust features from partial information.\n",
        "\n",
        "Mixing Images\n",
        "- **Mixup:** Linearly interpolating between two images and their labels.\n",
        "- **CutMix:** Cutting patches from one image and pasting the onto another, mixing their labels proportionally.\n",
        "\n",
        "Advanced Techniques (often generative):\n",
        "- **Generative Adversarial Networks (GANs):** Training a GAN to generate synthetic images that mimic the real data.\n",
        "- **Neural Style Transfer:** Applying the style of one image to the content of another. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Text Data Augmentation (Natural Language Processing -NLP)**\n",
        "\n",
        "Text data is more challenging to augment because simple modifications can easily change the meaning. Techniques often focus on preserving semantic meaning:\n",
        "- World-Level Transformations:\n",
        "   - Synonym Replacement: Replacing words with their synonyms (e.g., fast to quick)\n",
        "   - Random insertion: Inserting a random word (or synonym) at a random position.\n",
        "   - Random Deletion: Randomly deleting words.\n",
        "   - Random Swap: swapping the positions of two random words.\n",
        "- Sentence-level transformations\n",
        "   - Back Translation: Translating a sentence to another language and then translating it back to the original language. This often results in a rephrased but semantically similar sentence.  \n",
        "   - Paraphrasing: using rule based systems or neural networks to generate paraphrases of sentences.  \n",
        "   - Syntax-tree Manipulation: Reordering phrases or clauses within a sentence while maintaining grammatical correctness. \n",
        "- Document-Level Transformations: For longer texts, reordering paragraphs or sections. \n",
        "- Contextual Word Embeddings: Using models like BERT and GPT to generate new words or sentences based o context. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Audio Data Augmentation (Speech Recognition, Audio Classification)**\n",
        "\n",
        "- Time Domain Transformations\n",
        "   - Adding Noise: injecting background noise (e.g. white noise, street noise).\n",
        "   - Shifting: Shifting the audio forward or backward in time.\n",
        "   - Time Stretching: Changing the speed of the audio without changing the pitch.\n",
        "   - Pitch Shifting: Changing the pitch of the audio without changing the speed.\n",
        "   - Volume Adjustment: increasing or decreasing the amplitude.\n",
        "- Frequency-Domain Transformations (Spectrogram Augmentation):\n",
        "   - SpecAugment: Masking blocks of frequency channels of time steps in the spectrogram, forcing the model to rely on other parts of the input.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Tabular Data Augmentation**\n",
        "\n",
        "While less intuitive than image or text, tabular data can also be augmented, through with more caution:\n",
        "- **SMOTE (Synthetic Minority Over Sampling Techniques):** Creates synthetic samples for the minority class by interpolating between existing minority class instances and their nearest neighbors.\n",
        "- **Adding Noise:** Introducing small amounts of random noise to numerical features. \n",
        "- **Feature Perturbation:** Slightly altering feature values within realistic ranges.\n",
        "- **GANs/Variational Autoencoders (VAEs):** Generating synthetic tabular data using generative models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Considerations and Best Practices**\n",
        "- **Domain Knowledge:** The choice of augmentation techniques should be guided by domain knowledge. For example, flipping images for handwritten digits \"6\" and \"9\" would be incorrect, as it changes the label.  \n",
        "- **Realism:** Augmented data should remain realistic and representation of the true data distribution. Over augmenting or applying inappropriate transformations can introduce noise or misleading patterns, potentially harming model performance.  \n",
        "- **Balance:** Be mindful of class imbalance.  While augmentation can help, it should be used judiciously.  \n",
        "- **Augmentation Pipeline:** Data augmentation is often applied as part of the data loading pipeline, where transformations are applied on the fly during training, rather than crating a massive augmented dataset on disk (offline augmentation).  Online augmentation saves storage and provides more randomness.\n",
        "- **Validation:** Always evaluate the impact of data augmentation on you models performance on a separate, unaugmented validation set. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data augmentation is a powerful tool in a machine learning engineers toolkit, allowing them to make the most model robustness, and achieve better generalization performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
