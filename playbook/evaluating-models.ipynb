{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Evaluating Models**\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "Evaluating machine learning models is a critical step in the machine learning lifecycle, following model training.  Its how you assess how ell your model performs its intended task, whether its making accurate predictions, classifying data correctly, or generating relevant content.  Without proper evaluation, you can't be sure if your model is truly useful, if its overfitting or if its biased. \n",
        "\n",
        "The specific evaluation metrics and strategies depend heavily on the type of machine learning task your model is designed for.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **General Principles of Model Evaluation**\n",
        "\n",
        "1. **Use a Separate Test Set:** Always evaluate your model on data it has never seen before during training.  This is typically achieved by splitting your dataset into:\n",
        "   - **Training Set:** Used to train the model.\n",
        "   - **Validation Set:** Used during training to tune hyperparameters and prevent overfitting. \n",
        "   - **Test Set:** A completely held out dataset used only once at the very end to get an unbiased estimate of the models performance on new, unseen data.\n",
        "   - **Cross Validation:** For smaller datasets, cross validation techniques (e.g., k-fold cross validation) help ensure that the evaluation is robust and not overly dependent on a single train test split.\n",
        "2. **Understand Your Goal:** The best metric depends on the business problem your trying to solve.\n",
        "   - Is it more important to catch all positive cases (high recall), even if some are false alarms?\n",
        "   - Or is it more important that positive predictions are almost always correct (high precision) even if you miss some true positives? \n",
        "   - Is a balanced view of both recall and precision important (F1 score)?\n",
        "3. **Consider Baselines:** Compare your models performance against simple baselines (e.g., random guessing, predicting the majority class, a simple rule based system) to ensure your model is actually adding value.\n",
        "4. **Visualize Results:** Beyond numerical metrics, visualizations (e.g., confusion matrices, ROC curves, residual plots) provide deeper insights into model behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluation Metrics by Task Type**\n",
        "\n",
        "#### 1. **Classification Models**\n",
        "These models predict a categorical label.\n",
        "   - **Confusion Matrix:** A table that summarizes the performance of a classification model on a set of test data.  It breaks down predictions into:\n",
        "      - **True Positives (TP):** Correctly predicted positive class.\n",
        "      - **True Negative (TN):** Correctly predicted negative class.\n",
        "      - **False Positives (FP):** Predicted positive, but actually negative (type I error).\n",
        "      - **False Negative (FN):** Predicted negative, but actually positive (type II error).\n",
        "   \n",
        "   - **Accuracy:**\n",
        "      - **Formula:**\n",
        "         `(TP + TN)/(TP + TN + FP + FN)`\n",
        "      - **Interpretation:** The proportion of correctly classified instances.\n",
        "      - **Caveat:** Can be misleading on imbalanced datasets (e.g., 99% accuracy on a dataset where 99%, are negative even if you classify all as negative).\n",
        "\n",
        "   - **Precision (Positive Predictive Value):**\n",
        "      - **Formula:**\n",
        "        `TP/(TP + FP)`\n",
        "      - **Interpretation:** Of all instances predicted as positive, what proportion are actually positive?  Useful when the cost of False Positives is high (e.g, spam detection, recommending a stock).  \n",
        "\n",
        "   - **Recall (sensitivity, True Positive Rate):**\n",
        "      - **Formula:**\n",
        "        `TP/(TP + FN)`\n",
        "      - **Interpretation:** Of all actual positive instances, what proportion did the model correctly identify?  Useful when the cost of False Negatives is high (e.g, disease detection, fraud detection).  \n",
        "\n",
        "   - **F1 Score:**\n",
        "      - **Formula:**\n",
        "        `2 * (Precision * Recall)/(Precision + Recall)`\n",
        "      - **Interpretation:** The harmonic mean of precision and recall. Provides a balance between the two and is good for imbalanced datasets.\n",
        "\n",
        "   - **Specificity (True Negative Rate):**\n",
        "      - **Formula:**\n",
        "        `TN/(TN + FP)`\n",
        "      - **Interpretation:** Of all actual negative instances, what proportion did the model correctly identify?\n",
        "\n",
        "   - **ROC Curve (Receiver Operating Characteristic) & AUC (Area Under the Curve):**\n",
        "      - **ROC Curve:** Plots the True Positive Rate (Recall) against the False Positive Rate for various classification thresholds.\n",
        "      - **ACU:** The area under the ROC curve. A single scalar value (0 to 1) that summarizes classifier performance across all possible thresholds. A higher AUC indicates better overall performance.\n",
        "\n",
        "   - **Log Loss (Cross Entropy Loss):**\n",
        "      - **Interpretation:** Measures the performance of a classification model where the prediction is a probability value between 0 and 1. Penalizes false classification and rewards confident correct classification.  Lower is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "latex"
        }
      },
      "source": [
        "#### 2. **Regression Models**\n",
        "\n",
        "These models, predict a continuous numerical value.  \n",
        "\n",
        "**Mean Absolute Error (MAE):**\n",
        "   - **Formula:** \n",
        "$ \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $\n",
        "   - **Interpretation:** The average of the absolute differences between predictions and actual values.  Less sensitive to outliers than MSE. \n",
        "----\n",
        "**Mean Square Error (MSE):**\n",
        "   - **Formula:** \n",
        "$ \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $\n",
        "   - **Interpretation:** The average of the squared differences between predictions and actual values.  Penalizes larger errors more heavily.  Units are squared.  \n",
        "----\n",
        "**Root Mean Square Error (RMSE):**\n",
        "   - **Formula:** \n",
        "$ \\sqrt {\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $\n",
        "   - **Interpretation:** The square root of MSE.  Puts the error back into the same units as the target variable, making it more interpretable than MSE.\n",
        "----\n",
        "**R Squared (Coefficient of Determination):**\n",
        "   - **Formula:** \n",
        "$ 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} $\n",
        "   - **Interpretation:** Representation proportion of the variance in the dependent variable tha is predictable from the independent variables.  Ranges from 0 to 1, where 1 means the model perfectly explains the variance.  Can be negative in the model is worse than a simple mean.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. **Clustering Models**\n",
        "\n",
        "These models group data points without predefined labels.  Evaluation is often more challenging and can involve internal or external metrics.  \n",
        "\n",
        "- Internal Metrics (without ground truth):\n",
        "   - **Silhouette Score:** Measures how similar an object is to its own cluster compared to other clusters.  Ranges form -1 (bad clustering) to +1 (dense, well separated clusters). \n",
        "   - **Davies-Bouldin Index:** Lower value indicate better clustering (clusters are well separated and compact).\n",
        "   - **Dunn Index:** Higher values indicate better clustering (clusters are compact and well-separated) \n",
        "- External Metrics (with ground truth labels):\n",
        "   - **Adjusted Rand Index (ARI):** Measures the similarity between two clusterings (your models and the ground truth), accounting for chance.  Ranges from -1 to 1 with 1 being perfect agreement.\n",
        "   - **Normalized Mutual Information (NMI):** Measures the mutual information between the true labels and the clusters, normalized to be between 0 and 1. \n",
        "   - **Homogeneity, Completeness, V-measure:** Metrics that describe different aspects of a clustering solution relative to a ground truth. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. **Generative Models (GANs, VAEs, LLMs)**\n",
        "\n",
        "Evaluation is often more subjective and can involve human judgement.  \n",
        "\n",
        "- Image Generation:\n",
        "   - **Inception Score (IS):** Measures the quality and diversity of generated images (higher is better).\n",
        "   - **Frechet Inception Distance (FID):** Measures the similarity between real and generated images (lower is better).\n",
        "   - **Human Evaluation:** Surveys, A/B tests to assess realism, quality, and creativity.\n",
        "\n",
        "- Text Generation (LLMs):\n",
        "   - **Perplexity:** Measures how well a probability model predicts a sample.  Lower perplexity generally means a better language model.\n",
        "   - **BLEU (Bilingual Evaluation Understudy), ROUGE (Recall Orientated Understudy for Gisting Evaluation), METEOR:** used for machine translation, summarization, and text generation, comparing generated text to reference text.\n",
        "   - **Human Evaluation:** For coherence, fluency, relevance, harmfulness, bias, factual accuracy. This is paramount for LLMs.\n",
        "   - **Adversarial Testing/REd Teaming:** Specifically trying to make the model generate harmful or biased content.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Challenges in Model Evaluation**\n",
        "\n",
        "- **Data Drift:** Model Performance can degrade over time if the characteristics of the production data change form the training data.\n",
        "- **Concept Drift:** The relationship between input features and the target variable changes over time.\n",
        "- **Imbalanced Datasets:** Requires carful selection of metrics (accuracy can be misleading.)\n",
        "- **Subjectivity:** For generative models, quality can be subjective and hard to quantify.\n",
        "- **Cost of Errors:** Different types of errors (FP vs. FN) can have vastly different real world costs.\n",
        "- **Explainability:** Understating why a model makes certain predictions can be as important as the accuracy itself.  \n",
        "- **Bias and Fairness:** Ensuring the model performs equally well across different demographic groups or sensitive attributes.  This often requires specialized fairness metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Effective model evaluation is an iterative process that helps refine models, ensure they meet business objectives, and maintain their performance over time in real world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
