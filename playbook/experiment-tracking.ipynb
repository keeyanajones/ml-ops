{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Experiment Tracking**\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Keeyana Jones](https://github.com/keeyanajones/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "Experiment tracking s a cornerstone of effective machine learning (ML) development, especially in research, development, and MLOps.  Its the systematic process of recording, organizing, and analyzing all the relevant meta data, artifacts and results of each machine learning experiment or run.\n",
        "\n",
        "Think of ML experiment as a single attempt to train a model, with specific configurations, data, and code.  Because ML development is highly iterative and involves a lot of trail and error (trying different algorithms, hyperparameters, data preprocessing techniques, etc), keeping track of these experiments manually quickly becomes unmanageable.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Inputs/Configuration:\n",
        "   - **Code Version:** The Git commit hash or version of the training script, preprocessing scripts, and model definition code.\n",
        "   - **Data Version:** The specific version of the dataset used for training and validation (linking to a data versioning system like DVC or Git LFS).\n",
        "   - **Hyperparameters:** All the turnable parameters of your model and training process (e.g., learning rate batch size, number of layers, optimizer type, regularization strength).\n",
        "   - **Environment:** Details about the computing environment (e.g., Python version, library versions, GPU type, CPU count).\n",
        "   - **Random Seeds:** Any random seeds used to ensure reproducibility of stochastic processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Outputs/Results:\n",
        "   - **Metrics:** Key performance indicators (KPIs) measured during training and evaluation (e.g., accuracy, loss, precision, recall, F1 score for classification, MSE, RMSE, MAE, R-squared for regression).  These are often tracked over epochs.\n",
        "   - **Model Artifacts:** The trained model weights/checkpoints, architecture definitions, and any preprocessing components (e.g., fitted scalers, tokenizers).\n",
        "   - **Visualization:** Plots generated during training (e.g., loss curves, learning rate schedules, gradient norms), and evaluation visualizations (e.g., confusion matrices, ROC curves, example predictions).\n",
        "   - **Logs:** Standard output and error logs from the training process.\n",
        "   - **Hardware Usage:** CPU, GPU, and memory utilization during the run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Metadata:\n",
        "   - **Experiment ID/Name:** A unique identifier or descriptive name for each run.\n",
        "   - **Timestamp:** When the experiment was started and finished.\n",
        "   - **User/Author:** Who initiated the experiment.\n",
        "   - **Status:** Whether the run completed successfully, failed, or was interrupted.\n",
        "   - **Tags/Notes:** Custom tags or free form notes to categorize experiments and add context.         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Why is Experiment Tracking Essential?**\n",
        "\n",
        "1. **Reproducibility:** This is the most critical reason.  Without tracking, its virtually impossible to reproduce a specific models performance later.  If you can't reproduce results, you can't debug, verify, or build upon past work effectively. \n",
        "\n",
        "2. **Comparison and Analysis:**\n",
        "   - Easily Compare the performance of different model architectures, hyperparameter settings, or data preprocessing techniques side by side.\n",
        "   - Identify what changes led to improvement or regressions in model performance.\n",
        "   - Debug training processes by analyzing metric curves, resource usage, or specific outputs.\n",
        "\n",
        "3. **Collaboration:**\n",
        "   - Provides a centralized hub for teams to share, understand, and review each other's experiments.\n",
        "   - Reduces redundant work, as team members can see what configurations have already been tried.\n",
        "   - Facilitates knowledge transfer and onboarding for new team members. \n",
        "\n",
        "4. **Auditability and Compliance:** \n",
        "   - Creates a clear audit trail of model development, which is crucial for regulated industries that need to explain or justify how a model was built and what data it used. \n",
        "\n",
        "5. **Resource Optimization:**\n",
        "   - By tracking resource consumption (CPU/GPU usage, memory), you can identify inefficient experiments and optimize your computing costs.\n",
        "\n",
        "6. **Faster Iteration:** An organized overview of past experiments allows daa scientist to quickly identify promising directions and avoid repeating failed approaches, accelerating the entire development cycle.\n",
        "\n",
        "7. **Transition to MLOps:** Experiment tracking is a fundamental component of MLOps (Machine Learning Operations).  It bridges the gap between research/experimentation and production deployment by providing the necessary metadata for model registration, deployment, and monitoring.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How to Implement Experiment Tracking?**\n",
        "\n",
        "While you could use manual spreadsheets or ad-hoc file naming dedicated tools are highly recommended for any serious ML Project: \n",
        "\n",
        "1. Manual Tracking (Not Recommended for Scale):\n",
        "   - Spreadsheets (Google Sheets, Excel) or text files to log parameters and results.\n",
        "   - Systematic folder structures to save model artifacts and plots.\n",
        "   - **Pros:** Simple to start for very small, solo projects.\n",
        "   - **Cons:** Tedious, error prone, difficult to compare, scale or collaborate. \n",
        "\n",
        "2. Integrated with ML Frameworks (Basic Logging):\n",
        "   - **TensorBoard:** Built for TensorFlow (and now supports PyTorch), it allows visualizing metrics and graphs during training. Its a good basic logger but not a full experiment management system.\n",
        "   - **PyTorch Lighting Logger:** Integrates with various loggers (tensorBoard, MLflow, Weights & Biases) to simplify logging.\n",
        "\n",
        "3. Dedicated Experiment Tracking Platforms/Tools:\n",
        "   - These are the most common and powerful solutions, providing APIs for logging and a web UI for visualization and comparison.\n",
        "      - **MLFlow Tracking:** An open source platform thats part of the broader MLFlow ecosystem.  Its widely adopted for tracking metrics, parameters, and artifacts, and includes a UI.\n",
        "      - **Weights & Biases (W&B):** A popular commercial (with free tier) platform known for its excellent visualizations, comprehensive logging, hyperparameter weeps, and collaboration features.\n",
        "      - **Comet ML:** Another strong commercial (with free tier) contender offering robust experiment tracking, model production monitoring, and a rich UI.\n",
        "      - **Neptune.ai:** A lightweight, flexible experiment management tool designed to fit into an workflow, with a focus on ease of use.  \n",
        "      - **ClearML:** An open source MLOps platform that includes comprehensive experiment tracking, automation, and pipeline capabilities.   \n",
        "      - **DVC (Data Version Control) with DVC Studio:** While primarily for data and model versioning. DVC also has experiment tracking features that work with Git to track experiments, parameters, and metrics.\n",
        "      - **Guild AI:** Open source experiment racking pipeline automation and hyperparameter tuning.\n",
        "      - **Polyaxon:** An open source platform for managing and orchestrating ML experiments.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How they Work (Conceptual):**\n",
        "\n",
        "You typically integrate a few lines of code from the chosen tools client library into your training script.  For Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb # or mlflow, neptune, comet\n",
        "\n",
        "# Initialize a new experiment run\n",
        "wandb.init(project=\"my_image_classifier\", config={\"learning_rate\": 0.001, \"epochs\": 10})\n",
        "\n",
        "# Get hyperparameters from the config\n",
        "learning_rate = wandb.config.learning_rate\n",
        "epochs = wandb.config.epochs\n",
        "\n",
        "# Your training loop\n",
        "for epoch in range (epochs):\n",
        "    # ... train model ...\n",
        "    loss = calculate_loss()\n",
        "    accuracy = calculate_accuracy()\n",
        "\n",
        "# Log metrics to the experiment tracker\n",
        "wandb.log({\"loss\": loss, \"accuracy\": accuracy, \"epoch\": epoch})\n",
        "\n",
        "# After training, log the final model artifact\n",
        "wandb.save(\"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running your script, the logged data is sen to a centralized server (cloud hosted or self hosted) where you can then access an intuitive dashboard to view, filter, sort, compare, and analyze all your experiment runs.  \n",
        "\n",
        "By adopting a robust experiment tracking strategy, data science teams can significantly enhance their productivity, ensure reproducibility, and ultimately build better, more reliable machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_batch_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
